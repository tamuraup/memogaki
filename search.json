[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "about ページです\n\n\nCopyrightCopyright tamuraup. 2024. All Rights Reserved"
  },
  {
    "objectID": "archives/2024-05-01-chokudaisearch.html",
    "href": "archives/2024-05-01-chokudaisearch.html",
    "title": "chokudai search template",
    "section": "",
    "text": "chokudai search の練習をしたのでメモ。\n#[derive(PartialEq, Eq, Clone)]\nstruct State {\n    score: i64,\n}\nimpl State {\n    fn new() -&gt; Self {\n        todo!()\n    }\n}\n\nimpl Ord for State {\n    fn cmp(&self, other: &Self) -&gt; Ordering {\n        self.score.cmp(&other.score)\n    }\n}\n\nimpl PartialOrd for State {\n    fn partial_cmp(&self, other: &Self) -&gt; Option&lt;Ordering&gt; {\n        Some(self.cmp(other))\n    }\n}\n\nfn simple_chokudai_search(inp: &Input, timer: &Timer) {\n    let mut beams: Vec&lt;LimitedMaximumHeap&lt;State&gt;&gt; = vec![LimitedMaximumHeap::new(500); 51];\n\n    let mut iter = 0;\n    let mut best = 0;\n    beams[0].push(State::new());\n\n    'outer: loop {\n        iter += 1;\n        if timer.t() &gt;= 1.0 {\n            break;\n        }\n\n        for depth in 0..beams.len() {\n            if depth % 10 == 0 && timer.t() &gt;= 1.0 {\n                break 'outer;\n            }\n            if beams[depth].is_empty() {\n                continue;\n            }\n            // ここで複数回 pop すると 幅 &gt; 1 ってこと？\n            let state = beams[depth].pop().unwrap();\n            // TODO: なにか処理\n        }\n    }\n}\n\n\n\nCopyrightCopyright tamuraup. 2024. All Rights Reserved"
  },
  {
    "objectID": "posts/2024-05-03_ahc032memo.html",
    "href": "posts/2024-05-03_ahc032memo.html",
    "title": "AHC032 解説放送メモ",
    "section": "",
    "text": "解説放送みたメモ"
  },
  {
    "objectID": "posts/2024-05-03_ahc032memo.html#解説放送メモ",
    "href": "posts/2024-05-03_ahc032memo.html#解説放送メモ",
    "title": "AHC032 解説放送メモ",
    "section": "解説放送メモ",
    "text": "解説放送メモ\n\n解説放送: https://www.youtube.com/watch?v=9JS0wXXNiZk\neijirou さん解法: https://atcoder.jp/contests/ahc032/submissions?f.User=eijirou\nwata さん解法: https://atcoder.jp/contests/ahc032/submissions/52151408"
  },
  {
    "objectID": "posts/2024-05-03_ahc032memo.html#考察",
    "href": "posts/2024-05-03_ahc032memo.html#考察",
    "title": "AHC032 解説放送メモ",
    "section": "考察",
    "text": "考察\n\n3*3 のマスをすべて MOD*0.8 以上にするためには 5^9 通り候補がほしい\n\n(ランダムに与えられたスタンプで 1 マスを MOD*4/5 以上にできる確率 1/5)\n\n近傍がなめらかでないので、焼きなましなどの局所探索は不適\n同じ 9 マスを操作するなら、左上の点は分散させたほうが場合の数が増える"
  },
  {
    "objectID": "posts/2024-05-03_ahc032memo.html#ビームサーチ",
    "href": "posts/2024-05-03_ahc032memo.html#ビームサーチ",
    "title": "AHC032 解説放送メモ",
    "section": "ビームサーチ",
    "text": "ビームサーチ\n\n評価関数: \\(確定スコア + K * MOD * (1 - 進行度) * 残りの操作回数\\) (\\(K\\) はハイパーパラメータ)\n\n進行度が小さい場合は、後でより上手く揃えられるように操作回数を残したい\n\n\n\n「操作回数別でビームを分ける」とは\nwata さんの実装では操作回数別でビームを分けている。\nここでいうビームサーチは幅優先のビームサーチで「確定マス数」が深さに対応する。\n確定マス数を C とする。これを操作回数の基準として \\(\\pm W\\) 回の操作回数のズレを許容し、そのズレ幅ごとにビームをもつ。\n(例: コードの beam[W] はズレ幅 0 のビーム)\n\n\n揃えるマスの順\n35:30\n\n揃える順序は 3 マス揃えが連続しないほうが良い\nビームサーチの「確定スコア/確定マス数」を可視化することで確認している"
  },
  {
    "objectID": "posts/2024-05-03_ahc032memo.html#高速化",
    "href": "posts/2024-05-03_ahc032memo.html#高速化",
    "title": "AHC032 解説放送メモ",
    "section": "高速化",
    "text": "高速化\n\n不要なスタンプをできるだけ減らす工夫をしている\nスタンプを右上の値で分類し、右上マスを良い値にできるスタンプの候補を絞り込めるようにするなど\n\n\nK 分木の話\nwata さん実装の Searcher がそれ。\nやることは「3マス揃え」で話していることとほとんど同じ。\nMOD を K 個の区間に分けて、スタンプの 1 マス目の値で K 分割、2 マス目の値で K 分割 …\nのように分割する、木の深さが i マス目の分割となる K 分木を作り、良いスタンプの探索を高速にできるようにする。"
  },
  {
    "objectID": "posts/make_grid_example.html",
    "href": "posts/make_grid_example.html",
    "title": "画像グリッド描画サンプル",
    "section": "",
    "text": "複数枚の画像をグリッド描画するサンプルコードです。\ntorchvision の make_grid を使って描画します。 make_grid\n\nimport matplotlib.pyplot as plt\nimport random\nimport numpy as np\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torchvision.utils import make_grid\nimport torch\n\n\nclass CFG:\n    H=32\n    W=64\n    row=3\n\n\ndef get_tranform():\n    \"\"\"画像をアスペクト比を変えずに CFG.H * CFG.W サイズに変換する\"\"\"\n    t = A.Compose([\n        A.LongestMaxSize(max_size=max(CFG.H,CFG.W)),\n        A.PadIfNeeded(min_height=CFG.H, min_width=CFG.W, border_mode=0, mask_value=0),\n        ToTensorV2(),\n    ])\n    return t\n\n\n画像の用意\n数枚ランダムに単色画像を作成する\n\ndef create_img(color,size):\n    \"\"\"単色画像の作成\"\"\"\n    return np.array(color,dtype=np.uint8)*np.ones((*size,3), dtype=np.uint8)\n\n\nimages= [create_img([random.randint(0,255) for _ in range(3)],(20,50)) for _ in range(20)]\n\n\n\ngrid の作成\n\nt=get_tranform()\nimages=[t(image=img)['image'] for img in images]\n\n\ntmp=torch.stack(images)\n\n\nres=make_grid(tmp,3,padding=2)\n\n\n# channel を一番最後に\ngrid_image=res.permute(1,2,0).numpy()\n\n\nplt.imshow(grid_image)\n\n\n\n\n\n\n\n\n\n\nsave\n\nfrom PIL import Image\n\n\nimage=Image.fromarray(grid_image)\n\n\nimage.save(\"test.jpg\")\n\n\n\n\n\nCopyrightCopyright tamuraup. 2024. All Rights Reserved"
  },
  {
    "objectID": "posts/2024-08-16_kubo_3sho.html",
    "href": "posts/2024-08-16_kubo_3sho.html",
    "title": "データ解析のための統計モデリング入門 3章のポアソン回帰",
    "section": "",
    "text": "データ解析のための統計モデリング入門 3章のポアソン回帰をざっとやってみる"
  },
  {
    "objectID": "posts/2024-08-16_kubo_3sho.html#データ確認",
    "href": "posts/2024-08-16_kubo_3sho.html#データ確認",
    "title": "データ解析のための統計モデリング入門 3章のポアソン回帰",
    "section": "データ確認",
    "text": "データ確認\n\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nDATADIR=Path(\"/workdir/input/kubobook_2012/data/\")\nDATADIR\n\nPosixPath('/workdir/input/kubobook_2012/data')\n\n\n\ndf=pd.read_csv(DATADIR/\"data3a.csv\")\ndf.head()\n\n\n\n\n\n\n\n\ny\nx\nf\n\n\n\n\n0\n6\n8.31\nC\n\n\n1\n6\n9.44\nC\n\n\n2\n6\n9.50\nC\n\n\n3\n12\n9.07\nC\n\n\n4\n10\n10.16\nC\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\ny\nx\n\n\n\n\ncount\n100.000000\n100.000000\n\n\nmean\n7.830000\n10.089100\n\n\nstd\n2.624881\n1.008049\n\n\nmin\n2.000000\n7.190000\n\n\n25%\n6.000000\n9.427500\n\n\n50%\n8.000000\n10.155000\n\n\n75%\n10.000000\n10.685000\n\n\nmax\n15.000000\n12.400000\n\n\n\n\n\n\n\n\ncolor=['blue' if x=='C' else 'red' for x in df.f.to_list()]\nplt.scatter(df.x,df.y,c=color)"
  },
  {
    "objectID": "posts/2024-08-16_kubo_3sho.html#ポアソン回帰",
    "href": "posts/2024-08-16_kubo_3sho.html#ポアソン回帰",
    "title": "データ解析のための統計モデリング入門 3章のポアソン回帰",
    "section": "ポアソン回帰",
    "text": "ポアソン回帰\nsklearn のポアソン回帰を使って回帰してみる。\nポアソン回帰は \\(log (\\lambda) = \\beta X\\) の線形回帰になるらしい。(3章の回帰と同じで対数リンク関数が使われる)\nhttps://okumuralab.org/~okumura/stat/poisson_regression.html\n\nfrom sklearn import linear_model\nclf = linear_model.PoissonRegressor()\n\n\n# NOTE: 多変量の場合は、 df[[\"x1\",\"x2\"]] のようにする\nclf.fit(df[[\"x\"]],df[\"y\"])\n\nPoissonRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  PoissonRegressor?Documentation for PoissonRegressoriFittedPoissonRegressor() \n\n\n\nprint(\"beta_1:\",clf.intercept_) # bias \nprint(\"beta_2:\",clf.coef_) # 係数\n\nbeta_1: 1.378219625347999\nbeta_2: [0.06714908]\n\n\n\npreds=clf.predict(df[[\"x\"]])\npreds[:5]\n\narray([6.93255517, 7.47905849, 7.50925198, 7.29552944, 7.84953464])\n\n\n\n# lambda を手計算した結果と確認してみる\nnp.exp(df[:5][[\"x\"]]*clf.coef_[0]+clf.intercept_)\n\n\n\n\n\n\n\n\nx\n\n\n\n\n0\n6.932555\n\n\n1\n7.479058\n\n\n2\n7.509252\n\n\n3\n7.295529\n\n\n4\n7.849535\n\n\n\n\n\n\n\n\ndf[\"pred\"]=preds\n\n対数尤度計算してみる\n\n# 尤度計算してみる\nfrom scipy.stats import poisson\n\n\nloglike=df.apply(lambda x:np.log(poisson.pmf(x[\"y\"],x[\"pred\"])) ,axis=1).sum()\nprint(\"対数尤度:\",loglike)\n\n対数尤度: -235.41483299355457\n\n\nテキストの値とほぼ同じ値となっている。\nつぎに lambda の予測をプロットする。\n\ncolor=['blue' if x=='C' else 'red' for x in df.f.to_list()]\n\n# lambda の予測値を引く\nplt.scatter(df.x,df.y,c=color)\ndata_x = np.linspace(7,13,100).reshape(100,1)\nmu_=clf.predict(data_x)\nplt.plot(data_x,mu_)\n\nplt.show()\n\n/opt/conda/envs/foo/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but PoissonRegressor was fitted with feature names\n  warnings.warn("
  },
  {
    "objectID": "posts/2024-08-16_kubo_3sho.html#施肥効果の説明変数を追加-3.5",
    "href": "posts/2024-08-16_kubo_3sho.html#施肥効果の説明変数を追加-3.5",
    "title": "データ解析のための統計モデリング入門 3章のポアソン回帰",
    "section": "施肥効果の説明変数を追加 3.5〜",
    "text": "施肥効果の説明変数を追加 3.5〜\n\n# 施肥効果\ndf['d']=df.f.apply(lambda x:1 if x=='T' else 0)\n\n\nclf = linear_model.PoissonRegressor()\n# 多変量の場合は、 df[[\"x1\",\"x2\"]] のようにするだけで良い\nclf.fit(df[[\"x\",\"d\"]],df[\"y\"])\n\nPoissonRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  PoissonRegressor?Documentation for PoissonRegressoriFittedPoissonRegressor() \n\n\n\nprint(\"bias:\",clf.intercept_) # bias \nprint(\"beta:\",clf.coef_) # 係数\n\nbias: 1.3655409998858836\nbeta: [ 0.06925659 -0.01719852]\n\n\n対数尤度計算\n\npreds=clf.predict(df[[\"x\",\"d\"]])\nloglike=np.log(poisson.pmf(df[\"y\"],preds)).sum()\nprint(\"対数尤度:\",loglike)\n\n対数尤度: -235.34390910510987\n\n\n1変数だけのときの回帰よりも、尤度が少し増加していることが確認できた。"
  },
  {
    "objectID": "posts/2024-05-11-seed_everything.html",
    "href": "posts/2024-05-11-seed_everything.html",
    "title": "PyTorch 学習結果の再現性確保",
    "section": "",
    "text": "Pytorch Lightning で学習したとき、seed_everything で乱数固定すれば、同じ学習結果が得られると思っていたが実際にはそうでなかった。\n調べてみると、\nの設定も必要そうだった。Reproducibility — PyTorch 2.3 documentation\nこの 2 つを設定することで、内部で使用されるアルゴリズムを固定化できるとのこと。\n上記参考に以下の関数を呼ぶことで、同じ学習結果が得られるようになった。"
  },
  {
    "objectID": "posts/2024-05-11-seed_everything.html#その他参考",
    "href": "posts/2024-05-11-seed_everything.html#その他参考",
    "title": "PyTorch 学習結果の再現性確保",
    "section": "その他参考",
    "text": "その他参考\n\nhttps://qiita.com/north_redwing/items/1e153139125d37829d2d#cuda-convolution-benchmarking"
  },
  {
    "objectID": "posts/2024-08-14_gauss_sec3.html",
    "href": "posts/2024-08-14_gauss_sec3.html",
    "title": "ガウス過程と機械学習 3章 ガウス過程回帰",
    "section": "",
    "text": "『ガウス過程と機械学習』 (講談社) の3 章を読んだので、ガウス過程回帰の部分を実際にコードにしてみる。"
  },
  {
    "objectID": "posts/2024-08-14_gauss_sec3.html#図3.17-の基本アルゴリズムの実装",
    "href": "posts/2024-08-14_gauss_sec3.html#図3.17-の基本アルゴリズムの実装",
    "title": "ガウス過程と機械学習 3章 ガウス過程回帰",
    "section": "図3.17 の基本アルゴリズムの実装",
    "text": "図3.17 の基本アルゴリズムの実装\n以下の Qiita 記事の実装を借りて、一部変更した。\n『ガウス過程と機械学習』Pythonのnumpyだけで実装するガウス過程回帰 #機械学習プロフェッショナルシリーズ - Qiita\n注意)この記事のソースコードに「内積はドットで計算」とコメントされている部分があるが、内積ではなく行列積。\n\nimport numpy as np\nnp.random.seed(seed=9973)\n\n# 元データの作成\nn=100\ndata_x = np.linspace(0, 4*np.pi, n)\ndata_y = 2*np.sin(data_x) + 3*np.cos(2*data_x) + 5*np.sin(2/3*data_x) + np.random.randn(len(data_x))\n\n# 信号を欠損させて部分的なサンプル点を得る\nmissing_value_rate = 0.15\nsample_index = np.sort(np.random.choice(np.arange(n), int(n*missing_value_rate), replace=False))\n\n# データの定義\nxtrain = np.copy(data_x[sample_index])\nytrain = np.copy(data_y[sample_index])\n\nxtest = np.copy(data_x)\n\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize=(12, 5))\nplt.title('signal data', fontsize=20)\n\n# 元の信号\nplt.plot(data_x, data_y, 'x', color='green', label='correct signal')\n\n# 部分的なサンプル点\nplt.plot(data_x[sample_index], data_y[sample_index], 'o', color='red', label='sample dots')\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0, fontsize=12)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef RBF(x,x2,theta1,theta2,theta3,i_eq_j=False):\n    \"\"\"RBF kernel\"\"\"\n    if i_eq_j:\n        d=theta3\n    else:\n        d=0.\n    return theta1 * np.exp(-(x-x2)**2/theta2)+d\n    \n\n\ndef calc(xtrain,ytrain,xtest,kernel=RBF):\n    \"\"\"ガウス過程回帰の計算 図 3.17\"\"\"\n    # 平均\n    mu = []\n    # 分散\n    var = []\n\n    # 各パラメータ値\n    theta_1 = 16.\n    theta_2 = 2.\n    theta_3 = 0.34\n\n    N = len(xtrain)\n    K = np.zeros((N, N))\n    \n    for x in range(N):\n        for x_prime in range(N):\n            K[x, x_prime] = kernel(xtrain[x], xtrain[x_prime], theta_1, theta_2, theta_3,x==x_prime)\n            \n    inv_K=np.linalg.inv(K)\n    yy = np.dot(inv_K, ytrain)\n\n    M = len(xtest)\n    for test_i in range(M):\n        k = np.zeros((N,))\n        for x in range(N):\n            k[x] = kernel(xtrain[x], xtest[test_i], theta_1, theta_2, theta_3)\n\n        s = kernel(xtest[test_i], xtest[test_i], theta_1, theta_2, theta_3)\n        mu.append(np.dot(k, yy))\n        kK_ = np.dot(k, inv_K)\n        var.append(s - np.dot(kK_, k.T))\n    return mu,var\n\n\nmu,var=calc(xtrain,ytrain,xtest)\n\n\ndef visualize(data_x,data_y,xtest,sample_index,mu,var):\n    plt.figure(figsize=(12, 5))\n    plt.title('signal prediction by Gaussian process', fontsize=20)\n    \n    # 元の信号\n    plt.plot(data_x, data_y, 'x', color='green', label='correct signal')\n    # 部分的なサンプル点\n    plt.plot(data_x[sample_index], data_y[sample_index], 'o', color='red', label='sample dots')\n    \n    # 分散を標準偏差に変換\n    std = np.sqrt(var)\n    \n    # ガウス過程で求めた平均値を信号化\n    plt.plot(xtest, mu, color='blue', label='mean by Gaussian process')\n    # 分散\n    plt.fill_between(xtest, mu + 2*std, mu - 2*std, alpha=.2, color='blue', label= 'standard deviation by Gaussian process')\n    \n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0, fontsize=12)\n    plt.show()\n\n\nvisualize(data_x,data_y,xtest,sample_index,mu,var)"
  },
  {
    "objectID": "posts/2024-08-14_gauss_sec3.html#行列演算に変更",
    "href": "posts/2024-08-14_gauss_sec3.html#行列演算に変更",
    "title": "ガウス過程と機械学習 3章 ガウス過程回帰",
    "section": "行列演算に変更",
    "text": "行列演算に変更\nfor 文で処理している箇所を行列の演算に変更する。\nテキスト 図 3.18、公式 3.8 あたりを使う。\n\nimport numpy.matlib\ndef kgauss(X,theta_1,theta_2,theta_3):\n    \"\"\"\n    図 3.18 の実装\n    引数名は上の RBF 関数に合わせた\n    \"\"\"\n    N=len(X)\n    X=X.reshape(N,-1)\n    X=X.T # D * N 次元に\n    \n    z=(X**2).sum(axis=0).reshape(1,N)\n    K = numpy.matlib.repmat(z.T,1,N)+ numpy.matlib.repmat(z,N,1) -np.dot(X.T,X)*2.\n    return theta_1*np.exp(-K/theta_2)+theta_3*np.identity(N)\n\n\ndef calc2(xtrain,ytrain,xtest):\n    \"\"\"\n    図3.15 や公式 3.8 を使って for文を使わずに計算する\n    また、 train データの平均値が 0 でない場合に対応するため、 y の平均値を 0 に正規化してから計算している\n    \"\"\"\n    \n    # 平均0に正規化する。 y_mu は最後に mu に加算する\n    ytrain=ytrain.copy()\n    y_mu=ytrain.mean()\n    ytrain-=y_mu\n  \n    # 各パラメータ値\n    theta_1 = 16.\n    theta_2 = 2.\n    theta_3 = 0.34\n\n    N = len(xtrain)\n    M = len(xtest)\n    \n    # まとめてカーネル計算\n    xtrain=xtrain.reshape(N,-1)\n    xtest=xtest.reshape(M,-1)\n\n    # 図3.15 のとおり行列を分割\n    X=np.vstack([xtrain, xtest])\n    X = kgauss(X,theta_1,theta_2,theta_3)\n    \n    u, b=np.split(X,[N],0)\n    K, k_=np.split(u,[N],1)\n    k_t, k__=np.split(b,[N],1)\n    \n    assert k__.shape==(M,M)\n\n    inv_K=np.linalg.inv(K)\n    yy = np.dot(inv_K, ytrain)\n    \n    mu=np.dot(k_t,yy)\n    var=k__-np.dot(np.dot(k_t,inv_K),k_)\n    return mu+y_mu, var\n\n\nmu,var=calc2(xtrain,ytrain,xtest)\nvar=np.diag(var) # 対角成分の取得\nprint(mu[:5])\nprint(var[:5])\n\n[-0.55131748 -0.53446571 -0.48757389 -0.40223414 -0.26996228]\n[13.46226381 12.46070803 11.28034711  9.95007999  8.51800331]\n\n\n\nvisualize(data_x,data_y,xtest,sample_index,mu,var)\n\n\n\n\n\n\n\n\n上の結果とほぼ同じ結果が得られている。\n\ny の平均値が 0 でない場合の確認\nテキストに記載あるように、 y の平均値をあらかじめ 0 になるように正規化する処理を加えたのでその確認をする。\n\n# 元データの作成\nnp.random.seed(seed=9973)\nn=100\ndata_x = np.linspace(0, 4*np.pi, n)\n# 平均 +100 あたりになるように\ndata_y = 2*np.sin(data_x) + 3*np.cos(2*data_x) + 5*np.sin(2/3*data_x) + np.random.randn(len(data_x)) + 100\n\nmissing_value_rate = 0.15\nsample_index = np.sort(np.random.choice(np.arange(n), int(n*missing_value_rate), replace=False))\n\n# データの定義\nxtrain = np.copy(data_x[sample_index])\nytrain = np.copy(data_y[sample_index])\n\nxtest = np.copy(data_x)\n\n\nmu,var=calc2(xtrain,ytrain,xtest)\nvar=np.diag(var)\n\n\nvisualize(data_x,data_y,xtest,sample_index,mu,var)"
  },
  {
    "objectID": "posts/2024-08-14_gauss_sec3.html#サンプリングを確認",
    "href": "posts/2024-08-14_gauss_sec3.html#サンプリングを確認",
    "title": "ガウス過程と機械学習 3章 ガウス過程回帰",
    "section": "サンプリングを確認",
    "text": "サンプリングを確認\nテキスト 3.2.4\nテキストの順番と前後するが、ガウス過程回帰の結果を使ってサンプリング結果も確認しておく。\n求めた平均、分散からランダムにサンプリングして滑らかな関数となる様子を確認する。\nここで生成される曲線が「事後分布の雲」からランダムに出力された「関数 f」ということになる。\n\n\ndef get_cmap(name='tab10'):\n    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n    return plt.colormaps.get_cmap(name)\n\ndef visualize2(data_x,data_y,xtest,sample_index,mu,var,ys):\n    plt.figure(figsize=(12, 5))\n    plt.title('signal prediction by Gaussian process', fontsize=20)\n        \n    # ガウス過程で求めた平均値を信号化\n    cmap = get_cmap()\n    for i,y in enumerate(ys):\n        plt.plot(xtest, y, color=cmap(i/len(ys)), label=f'mean by Gaussian process {i}')   \n        \n    # 元の信号\n    plt.plot(data_x, data_y, 'x', color='green', label='correct signal')\n    plt.plot(data_x[sample_index], data_y[sample_index], 'o', color='red', label='sample dots')\n    std = np.sqrt(var) \n    plt.fill_between(xtest, mu + 2*std, mu - 2*std, alpha=.2, color='blue', label= 'standard deviation by Gaussian process')\n    \n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0, fontsize=12)\n    plt.show()\n\n\nrng = np.random.default_rng()\n# 5回のサンプリングを行った結果を表示\nys=[[rng.normal(a,np.sqrt(b)) for a,b in zip(mu,var)] for _ in range(5)]\nvisualize2(data_x,data_y,xtest,sample_index,mu,var,ys)"
  },
  {
    "objectID": "posts/2024-08-16_gauss_sec3_param.html",
    "href": "posts/2024-08-16_gauss_sec3_param.html",
    "title": "ガウス過程と機械学習 3章 ガウス過程回帰のパラメータ推定",
    "section": "",
    "text": "前回 からの続き\nテキスト 3.5 のパラメータ推定をやってみる。\ntrainデータを使って対数尤度が最大になるような \\(\\theta\\) を探索する。\n対数尤度の式は (3.92) の \\(-\\log |K_\\theta| - y^T K^{-1}_\\theta y\\) 。\n前回同様テストデータなどは以下の記事から借りている。\n『ガウス過程と機械学習』Pythonのnumpyだけで実装するガウス過程回帰 #機械学習プロフェッショナルシリーズ - Qiita\n# テストデータ生成\n\nimport numpy as np\nnp.random.seed(seed=9973)\n\n\n# 元データの作成\nn=100\ndata_x = np.linspace(0, 4*np.pi, n)\ndata_y = 2*np.sin(data_x) + 3*np.cos(2*data_x) + 5*np.sin(2/3*data_x) + np.random.randn(len(data_x))\n\n# 信号を欠損させて部分的なサンプル点を得る\nmissing_value_rate = 0.15\nsample_index = np.sort(np.random.choice(np.arange(n), int(n*missing_value_rate), replace=False))\n\n# データの定義\nxtrain = np.copy(data_x[sample_index])\nytrain = np.copy(data_y[sample_index])\n\nxtest = np.copy(data_x)\n# カーネル\n\nimport numpy.matlib\ndef kgauss(X,theta_1,theta_2,theta_3):\n    \"\"\"\n    RBF kernel\n    図 3.18 の実装\n    \"\"\"\n    N=len(X)\n    X=X.reshape(N,-1)\n    X=X.T # D * N 次元に\n    \n    z=(X**2).sum(axis=0).reshape(1,N)\n    K = numpy.matlib.repmat(z.T,1,N)+ numpy.matlib.repmat(z,N,1) -np.dot(X.T,X)*2.\n    return theta_1*np.exp(-K/theta_2)+theta_3*np.identity(N)"
  },
  {
    "objectID": "posts/2024-08-16_gauss_sec3_param.html#optuna-を使って-theta-を探索する",
    "href": "posts/2024-08-16_gauss_sec3_param.html#optuna-を使って-theta-を探索する",
    "title": "ガウス過程と機械学習 3章 ガウス過程回帰のパラメータ推定",
    "section": "optuna を使って \\(\\theta\\) を探索する",
    "text": "optuna を使って \\(\\theta\\) を探索する\n\ndef _objective(theta_1,theta_2,theta_3):\n    \"\"\" 対数尤度関数 \"\"\"\n    y=ytrain\n    x=xtrain\n    K=kgauss(xtrain,theta_1,theta_2,theta_3)\n    det_K=np.linalg.det(K)\n    inv_K=np.linalg.inv(K)\n    return -np.log(det_K)-np.dot(y.T,np.dot(inv_K,y))\n\n\n# optuna で探索\n\nimport optuna\noptuna.logging.disable_default_handler()\n \ndef objective(trial):\n    theta_1 = trial.suggest_float(\"theta_1\", 1e-10, 30)\n    theta_2 = trial.suggest_float(\"theta_2\", 1e-10, 30)\n    theta_3 = trial.suggest_float(\"theta_3\", 1e-10, 30)\n    return _objective(theta_1,theta_2,theta_3)\n\n \nstudy = optuna.create_study(direction=\"maximize\")\n\nstudy.optimize(objective, n_trials=300)\nprint(study.best_trial)\n\nFrozenTrial(number=185, state=1, values=[-39.02225822902864], datetime_start=datetime.datetime(2024, 8, 15, 20, 26, 10, 299349), datetime_complete=datetime.datetime(2024, 8, 15, 20, 26, 10, 312097), params={'theta_1': 15.162762907400607, 'theta_2': 1.9226918374007205, 'theta_3': 0.463028699778022}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'theta_1': FloatDistribution(high=30.0, log=False, low=1e-10, step=None), 'theta_2': FloatDistribution(high=30.0, log=False, low=1e-10, step=None), 'theta_3': FloatDistribution(high=30.0, log=False, low=1e-10, step=None)}, trial_id=185, value=None)\n\n\n\nprint(study.best_params)\n\n{'theta_1': 15.162762907400607, 'theta_2': 1.9226918374007205, 'theta_3': 0.463028699778022}"
  },
  {
    "objectID": "posts/2024-08-16_gauss_sec3_param.html#plot",
    "href": "posts/2024-08-16_gauss_sec3_param.html#plot",
    "title": "ガウス過程と機械学習 3章 ガウス過程回帰のパラメータ推定",
    "section": "plot",
    "text": "plot\n推定した \\(\\theta\\) を使って描画する。\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\ndef visualize(data_x,data_y,xtest,sample_index,mu,var,title):\n    plt.figure(figsize=(12, 5))\n    plt.title(title, fontsize=20)\n    \n    # 元の信号\n    plt.plot(data_x, data_y, 'x', color='green', label='correct signal')\n    # 部分的なサンプル点\n    plt.plot(data_x[sample_index], data_y[sample_index], 'o', color='red', label='sample dots')\n    \n    # 分散を標準偏差に変換\n    std = np.sqrt(var)\n    \n    # ガウス過程で求めた平均値を信号化\n    plt.plot(xtest, mu, color='blue', label='mean by Gaussian process')\n    # 分散\n    plt.fill_between(xtest, mu + 2*std, mu - 2*std, alpha=.2, color='blue', label= 'standard deviation by Gaussian process')\n    \n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0, fontsize=12)\n    plt.show()\n\n\ndef predict(xtrain,ytrain,xtest,theta_1,theta_2,theta_3):\n    \"\"\"\n    ガウス過程回帰\n    \"\"\"\n    ytrain=ytrain.copy()\n\n    N = len(xtrain)\n    M = len(xtest)\n    \n    # まとめてカーネル計算\n    xtrain=xtrain.reshape(N,-1)\n    xtest=xtest.reshape(M,-1)\n\n    # 図3.15 のとおり行列を分割\n    X=np.vstack([xtrain, xtest])\n    X = kgauss(X,theta_1,theta_2,theta_3)\n    \n    u, b=np.split(X,[N],0)\n    K, k_=np.split(u,[N],1)\n    k_t, k__=np.split(b,[N],1)\n    \n    assert k__.shape==(M,M)\n\n    inv_K=np.linalg.inv(K)\n    yy = np.dot(inv_K, ytrain)\n    \n    mu=np.dot(k_t,yy)\n    var=k__-np.dot(np.dot(k_t,inv_K),k_)\n    var=np.diag(var) # 対角成分の取得\n    return mu, var\n\n\nmu,var=predict(xtrain,ytrain,xtest,**study.best_params)\n\n\nvisualize(data_x,data_y,xtest,sample_index,mu,var,'推定したパラメータで回帰')\n\n\n\n\n\n\n\n\nテキストで使用されているパラメータを使って描画して比較してみる。\n\nmu,var=predict(xtrain,ytrain,xtest,1.0,0.4,0.1)\nvisualize(data_x,data_y,xtest,sample_index,mu,var,'テキストと同じパラメータの値で回帰')"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html",
    "href": "posts/2024-06-17_sqlcontest-01-10.html",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "",
    "text": "SQL の Window 関数の練習のため SQL コンテスト の問題4 を解いていく。\n注意"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第1回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第1回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第1回 問題4",
    "text": "第1回 問題4\nRANK を使う問題\nwith t1 as (select pf_code\n                 , NATION_CODE\n                 , amt\n                 , rank() over (\n        partition by pf_code\n        order by amt desc,NATION_CODE\n        ) as rnk\n            from FOREIGNER fo\n            where NATION_CODE &lt;&gt; '113')\n\nselect t1.pf_code as '都道府県コード'\n     , pf.PF_NAME as '都道府県名'\n     , max(case when t1.rnk == 1 then na.NATION_NAME else '' end) '1位 国名'\n     , max(case when t1.rnk == 1 then t1.amt else 0 end) '1位 人数'\n     , max(case when t1.rnk == 2 then na.NATION_NAME else '' end) '2位 国名'\n     , max(case when t1.rnk == 2 then t1.amt else 0 end) '2位 人数'\n     , max(case when t1.rnk == 3 then na.NATION_NAME else '' end) '3位 国名'\n     , max(case when t1.rnk == 3 then t1.amt else 0 end) '3位 人数'\n     , max(tot.amt) '合計人数'\nfrom t1\n     join PREFECTURE pf on t1.PF_CODE = pf.PF_CODE\n     join NATIONALITY na on t1.NATION_CODE = na.NATION_CODE\n     join (select pf_code, sum(amt) amt\n           from FOREIGNER\n           where NATION_CODE &lt;&gt; '113'\n           group by PF_CODE) tot on tot.PF_CODE = t1.pf_code\n\nwhere rnk &lt;= 3\n\ngroup by t1.PF_CODE\norder by 9 desc, 1"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第2回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第2回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第2回 問題4",
    "text": "第2回 問題4\n横持ちデータを縦持ちにする問題。\nWITH t1 AS (SELECT CASE no\n                       WHEN 0 THEN '小学校'\n                       WHEN 1 THEN '中学校'\n                       WHEN 2 THEN '高校'\n                       WHEN 3 THEN '短大'\n                       WHEN 4 THEN '大学'\n                       ELSE '大学院'\n    END KIND\n                 , CASE no\n                       WHEN 0 THEN ELEMENTARY\n                       WHEN 1 THEN MIDDLE\n                       WHEN 2 THEN HIGH\n                       WHEN 3 THEN JUNIOR_CLG\n                       WHEN 4 THEN COLLEGE\n                       ELSE GRADUATE\n        END AMT\n                 , no\n                 , SURVEY_YEAR\n                 , PF_CODE\n            FROM ENROLLMENT_STATUS\n               , (WITH SEQ AS (SELECT 0 AS no UNION ALL SELECT no + 1 AS no FROM SEQ WHERE no + 1 &lt; 6) SELECT * FROM seq) seq\n\n            WHERE SURVEY_YEAR = 2020)\nSELECT SURVEY_YEAR SV_YEAR\n     , PF.PF_NAME PREFECTURE\n     , KIND KIND\n     , SUM(AMT) AS AMT\nFROM t1\n     JOIN PREFECTURE PF ON PF.PF_CODE = t1.PF_CODE\nWHERE t1.amt IS NOT NULL\nGROUP BY SURVEY_YEAR, PF.PF_NAME, KIND, no\nORDER BY PF.PF_CODE, no"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第3回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第3回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第3回 問題4",
    "text": "第3回 問題4\nコンテストの得点と順位を求める問題。問題文長すぎ。\nテストケース 2 のテストデータの不備で、\nENTRIES.CONTEST_ID = 2 であるが SUBMISSIONS.CONTEST_ID が 2 でないデータが含まれていると思われる。\nWITH t1 AS (SELECT sub.*\n                 , ent.STARTED_AT\n            FROM ENTRIES ent\n                 JOIN SUBMISSIONS sub ON sub.ENTRY_ID = ent.ENTRY_ID\n            WHERE ent.CONTEST_ID = 2\n              AND sub.CONTEST_ID = 2)\n   , ac AS (SELECT entry_id\n                 , SUM(point) pt\n                 , MAX(STRFTIME('%s', SUBMITTED_AT)) - MAX(STRFTIME('%s', STARTED_AT)) tdiff\n            FROM t1\n            WHERE STATUS = 'AC'\n            GROUP BY ENTRY_ID)\n   , wa AS (SELECT t1.entry_id\n                 , COUNT(*) cnt\n            FROM t1\n                 JOIN (SELECT ENTRY_ID, PROBLEM_ID, SUBMITTED_AT\n                       FROM t1\n                       WHERE STATUS = 'AC') tac\n                      ON t1.ENTRY_ID = tac.ENTRY_ID\n                          AND t1.PROBLEM_ID = tac.PROBLEM_ID\n                          AND t1.SUBMITTED_AT &lt;= tac.SUBMITTED_AT\n            WHERE t1.STATUS &lt;&gt; 'AC'\n            GROUP BY t1.ENTRY_ID)\nSELECT RANK() OVER (\n    ORDER BY t.POINT DESC,t.EX_TIME\n    ) AS RANK\n     , t.*\nFROM (SELECT ent.USER_ID\n           , ac.pt AS POINT\n           , ac.tdiff + 300 * (IFNULL(wa.cnt, 0)) EX_TIME\n           , IFNULL(wa.cnt, 0) WRONG_ANS\n      FROM ac\n           LEFT JOIN wa ON ac.ENTRY_ID = wa.ENTRY_ID\n           JOIN ENTRIES ent ON ent.ENTRY_ID = ac.ENTRY_ID) t"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第4回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第4回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第4回 問題4",
    "text": "第4回 問題4\n存在しない日付をうまく作る問題。\n連番を作るコード(SEQ としている部分) を持っているとなにかと便利。\nWITH SEQ AS (SELECT 0 AS no UNION ALL SELECT no + 1 AS no FROM SEQ WHERE no + 1 &lt; 31)\n   , t1 AS (SELECT no, DATE('2022-08-01', '+' || no || ' day') AS dt FROM seq)\nSELECT dt AS REGIST_DATE\n     , SUBSTR('月火水木金土日', (no % 7) + 1, 1) AS WK\n     , IFNULL(COUNT(us.USER_CODE), 0) AS TOTAL\nFROM t1\n     LEFT JOIN\n     USERS AS us ON DATE(us.CONFIRMED_AT) == t1.dt\n         AND us.VALID_FLG = '1'\nGROUP BY dt, no;"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第5回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第5回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第5回 問題4",
    "text": "第5回 問題4\n与えられた式に従って累積和を取る問題。\nWITH t1 AS (SELECT MONTHLY\n                 , NEW_MRR + mrr.EXPANSION_MRR - mrr.DOWNGRADE_MRR - mrr.CHURN_MRR AS x\n            FROM MRR_DATA mrr)\n   , month AS (SELECT DISTINCT monthly\n               FROM MRR_DATA\n               UNION ALL\n               SELECT DATE(MAX(monthly), '+1 month')\n               FROM MRR_DATA)\nSELECT m.MONTHLY YM, IFNULL(SUM(t1.x), 0) MRR\nFROM month m\n     LEFT JOIN t1 ON t1.MONTHLY &lt; m.MONTHLY\nGROUP BY m.MONTHLY\nORDER BY m.MONTHLY;"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第6回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第6回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第6回 問題4",
    "text": "第6回 問題4\nやるだけの問題。\nSELECT di.DISTRICT_CODE CODE\n     , DISTRICT_NAME NAME\n     , LATITUDE LAT\n     , LONGITUDE LON\nFROM LOCATION_TBL lo\n     JOIN DISTRICT AS di ON di.DISTRICT_CODE = lo.DISTRICT_CODE\nWHERE di.DISTRICT_CODE &lt;&gt; '1101'\nORDER BY (SELECT (LATITUDE - lo.LATITUDE) * (LATITUDE - lo.LATITUDE) + (LONGITUDE - lo.LONGITUDE) * (LONGITUDE - lo.LONGITUDE) FROM LOCATION_TBL WHERE DISTRICT_CODE = '1101')\n        DESC, 1"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第7回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第7回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第7回 問題4",
    "text": "第7回 問題4\nこれもやるだけの問題。\nROUND, CAST の使い方を学んだ。\nWITH t1 AS (SELECT ITEM_CODE, SUM(UNITPRICE * SALES_QTY) AS amt\n            FROM sales\n                 INNER JOIN SALES_DTL de ON de.sales_no = sales.SALES_NO\n            WHERE SALES_DATE BETWEEN '2023-06-01' AND '2023-06-30'\n            GROUP BY ITEM_CODE)\n   , t2 AS (SELECT t1.ITEM_CODE\n                 , SUM(t2.amt) v0\n                 , ROUND(CAST(MAX(t1.amt) AS real) / (SELECT SUM(amt) FROM t1) * 100, 1) v1\n                 , ROUND(CAST(IFNULL(SUM(t2.amt), 0) AS real) / (SELECT SUM(amt) FROM t1) * 100, 1) v2\n            FROM t1\n                 LEFT JOIN t1 t2 ON t2.amt &gt;= t1.amt\n            GROUP BY t1.ITEM_CODE)\nSELECT t1.ITEM_CODE CODE\n     , ITEM_NAME NAME\n     , t1.amt SAL_AMT\n     , t2.v0 CML_AMT\n     , t2.v1 || '%' SAL_COMP\n     , t2.v2 || '%' TTL_COMP\n     , CASE\n           WHEN t2.v2 &lt;= 40 THEN 'A'\n           WHEN t2.v2 &lt;= 80 THEN 'B'\n           ELSE 'C' END AS RANK\nFROM t1\n     JOIN t2 ON t1.ITEM_CODE = t2.ITEM_CODE\n     JOIN ITEM ON ITEM.ITEM_CODE = t2.ITEM_CODE\n\nORDER BY 3 DESC, 1 DESC"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第8回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第8回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第8回 問題4",
    "text": "第8回 問題4\n問題文に”設定”とあって何をするのかわかりにくいが、更新クエリを問われている。 副問合せを使った UPDATE 忘れていたので勉強になった。\nUPDATE item SET ITEM_POPULAR_RANK=0;\nWITH t1 AS (SELECT D.ITEM_CODE\n                 , MAX(ORDER_DATE) DT\n                 , SUM(ORDER_QTY) qty\n            FROM ORDERS O\n                 JOIN ORDERS_DTL D ON D.ORDER_NO = O.ORDER_NO\n            WHERE O.ORDER_DATE BETWEEN '2023-04-01' AND '2023-06-30'\n            GROUP BY D.ITEM_CODE)\n   , t2 AS (SELECT t1.*\n                 , RANK() OVER (ORDER BY qty DESC, dt DESC,ITEM_CODE DESC\n        ) rnk\n            FROM t1)\nUPDATE item\nSET ITEM_POPULAR_RANK = t2.rnk\nFROM t2\nWHERE t2.ITEM_CODE = item.ITEM_CODE;"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第9回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第9回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第9回 問題4",
    "text": "第9回 問題4\nNTILE を使える問題。\nどのように集計するか悩んだ。 NTILE とったテーブルを前もって集計しておく(以下の t3 )と比較的楽そう。\nWITH t1 AS (SELECT PF_CODE, TOTAL_VALUE FROM CONVENIENCE WHERE SURVEY_YEAR = 2019 AND KIND_CODE = '100')\n   , t2 AS (SELECT PF_CODE, TOTAL_VALUE te FROM CONVENIENCE WHERE SURVEY_YEAR = 2019 AND KIND_CODE = '150')\n   , t3 AS (SELECT no, SUM(sal) sal, SUM(te) te\n            FROM (SELECT t1.TOTAL_VALUE sal, t2.te, NTILE(10) OVER ( ORDER BY t1.TOTAL_VALUE DESC, t2.te, t1.PF_CODE) NO\n                  FROM t1\n                       LEFT JOIN t2 ON t1.PF_CODE = t2.PF_CODE) t\n            GROUP BY t.no)\n   , t4 AS (SELECT t3.no, SUM(tmp.sal) s\n            FROM t3\n                 LEFT JOIN t3 AS tmp ON t3.no &gt;= tmp.no\n            GROUP BY t3.no)\nSELECT t3.no NO\n     , t3.sal TTL_SAL\n     , ROUND(CAST(t3.sal AS real) / tot.val * 100, 1) PER_SAL\n     , ROUND(CAST(t4.s AS real) / tot.val * 100, 1) CUM_SAL\n     , FLOOR(CAST(t3.sal AS real) / t3.te) AVG_SAL\nFROM t3\n     LEFT JOIN t4 ON t3.no = t4.no\n   , (SELECT SUM(sal) val FROM t3) tot\nGROUP BY t3.no\nORDER BY t3.no\n;"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第10回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第10回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第10回 問題4",
    "text": "第10回 問題4\nいろいろやり方はありそうな問題。今回は UNION ALL で入れ子にして1つ前のステップのデータを確認するようにした。\n他には RANK を使って timestamp 順にランクをとり、STEP {i} と一致しているところまでをカウントするなどが考えられる。\nSTEP5 のデータがない場合でも STEP5 | 0 と表示できるように SEQ テーブルを使うことに注意。\nWITH t1 AS (SELECT lo.SESSION_ID, lo.PROCESS_ID, lo.EX_TIMESTAMP, MAX(lo2.EX_TIMESTAMP) pre_t\n            FROM PROCESS_LOG lo\n                 LEFT JOIN PROCESS_LOG lo2 ON lo.SESSION_ID = lo2.SESSION_ID AND lo.EX_TIMESTAMP &gt; lo2.EX_TIMESTAMP\n            GROUP BY lo.SESSION_ID, lo.PROCESS_ID, lo.EX_TIMESTAMP)\n   , t2 AS (SELECT t1.*, 1 step\n            FROM t1\n            WHERE PROCESS_ID = 'STEP1'\n              AND pre_t IS NULL\n            UNION ALL\n            SELECT t1.*, t2.step + 1\n            FROM t1\n                 JOIN t2 ON t1.SESSION_ID = t2.SESSION_ID AND t1.PROCESS_ID = 'STEP' || (t2.step + 1) AND t1.pre_t = t2.EX_TIMESTAMP\n            WHERE step &lt; 5)\n   , seq AS (SELECT 1 AS no UNION ALL SELECT no + 1 AS no FROM SEQ WHERE no + 1 &lt;= 5)\nSELECT 'STEP' || seq.no PROCESS, COUNT(session_id) CNT\nFROM seq\n     LEFT JOIN t2 ON 'STEP' || seq.no = t2.PROCESS_ID\nGROUP BY seq.no\nORDER BY 1"
  },
  {
    "objectID": "posts/2025-03-25-image-rot/image_rot.html",
    "href": "posts/2025-03-25-image-rot/image_rot.html",
    "title": "Python でスキャン画像の回転補正をやってみる",
    "section": "",
    "text": "画像の回転補正をやってみる。\n\n準備\n安全運転BOOKの目次のスキャン画像を使います。\n(『安全運転BOOK』伊藤印刷株式会社)\n\nfrom pathlib import Path\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\n\nimg = cv2.imread(\"sample.jpg\", cv2.IMREAD_GRAYSCALE)\nplt.imshow(img, cmap='gray')\n\n\n\n\n\n\n\n\n\ndef rotate_image(img: np.ndarray, d: float, border_value: int = 1, resize=False):\n    \"\"\"\n    画像を d [度] 回転します。\n\n    Args:\n        img:\n        d: 回転角度\n        border_value: 余白を埋める値\n        resize: True のとき、回転後の画像が欠けないようにサイズ調整をします\n                False のときはもとのサイズと同じ画像が返ります\n    \"\"\"\n    height, width = img.shape[:2]\n    new_width, new_height = width, height\n    center = (width // 2, height // 2)\n    M = cv2.getRotationMatrix2D(center, d, 1)\n\n    if resize:\n        cos = np.abs(M[0, 0])\n        sin = np.abs(M[0, 1])\n        # 回転後のすべての領域が含まれるサイズを計算\n        new_width = int(width * cos + height * sin)\n        new_height = int(width * sin + height * cos)\n        # 中心ずらし\n        M[0, 2] += (new_width - width) / 2.0\n        M[1, 2] += (new_height - height) / 2.0\n\n    affine_img = cv2.warpAffine(img,\n                                M,\n                                (new_width, new_height),\n                                borderMode=cv2.BORDER_CONSTANT,\n                                borderValue=border_value)\n    return affine_img\n\n傾けた画像を用意する\n\nimg = rotate_image(img,5,255)\nplt.imshow(img, cmap='gray')\n\n\n\n\n\n\n\n\n\n\n回転補正\n以下のサイトの射影ヒストグラムをもとに最も良い「正面度」となる角度を探索し、画像を回転させる。\n射影ヒストグラムを用いた文書画像の回転補正 - 社会人研究者が色々頑張るブログ\n\ndef projection_histogram(bimg):\n    \"\"\"\n    射影ヒストグラムの作成\n    https://nsr-9.hatenablog.jp/entry/2021/08/09/200559\n    \"\"\"\n    bimg = np.ones_like(bimg) - bimg\n    out = np.sum(bimg, axis=1)\n    return out\n\n\ndef to_binary_adavtive_threshold(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"2値化処理 {0, 1} に変換\"\"\"\n    \n    bimg = cv2.adaptiveThreshold(\n        img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 7, 50\n    )\n    bimg[bimg == 255] = 1\n    return bimg\n\n\nbimg=to_binary_adavtive_threshold(img)\nhist = projection_histogram(bimg)\n\nヒストグラムの確認\n\nfig = plt.figure(figsize=(10, 6))\ngs = fig.add_gridspec(1, 2, width_ratios=[1, 1])\n\n# histgram\nax0 = fig.add_subplot(gs[0])\nval = hist[::-1]\nx = range(len(val))\nax0.barh(x, val)\nax0.set_ylim(0, len(val))\n\n# image\nax1 = fig.add_subplot(gs[1])\nax1.imshow(bimg, cmap=\"gray\")\nax1.axis(\"off\")\nplt.subplots_adjust(wspace=0, left=0, right=1, top=1, bottom=0)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef calc_cost(img:np.ndarray,d:float):\n    img=rotate_image(img,d,border_value=1,resize=True)\n    hist=projection_histogram(img)\n    hist=np.where(hist&lt;=1,0,1)\n    return np.sum(hist)\n\ncalc_cost(bimg,0.)\n\n1249\n\n\n角度を変えたときのコストの変化を確認する\n\nx=np.linspace(-8., 8, num=80)\nvals=[calc_cost(bimg,d) for d in x]\n\n\nmin_idx=0\nfor i in range(len(vals)):\n    if vals[i]&lt;vals[min_idx]:\n        min_idx=i\n\n\nprint(\"コスト最小値となる角度:\",x[min_idx])\n\nコスト最小値となる角度: -4.962025316455696\n\n\n\nplt.xlabel('angle')\nplt.ylabel('cost')\nplt.plot(x, vals, marker='o')  \nplt.show()\n\n\n\n\n\n\n\n\n探索した良い正面度の角度に補正する\n\ntmp=rotate_image(img,x[min_idx],border_value=255)\nfig = plt.figure(figsize=(10, 6))\nplt.imshow(tmp, cmap='gray')\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(10, 6))\ngs = fig.add_gridspec(1, 2, width_ratios=[1, 1])\n\nax0=fig.add_subplot(gs[0])\nax0.imshow(img, cmap='gray')\nax0.axis('off')\n\nax1=fig.add_subplot(gs[1])\nax1.imshow(tmp, cmap='gray')\nax1.axis('off')\nplt.subplots_adjust(wspace=0, left=0, right=1, top=1, bottom=0)\n\n# 縦線を追加\nax1.axvline(x=0, color='black', linewidth=2)   \nplt.show()\n\n\n\n\n\n\n\n\n\n\nおまけ\nぱっとわからなかった回転後の画像サイズの求め方\n\n\n\n\n\nCopyrightCopyright tamuraup. 2024. All Rights Reserved"
  },
  {
    "objectID": "posts/2025-02-05_make_lag_data.html",
    "href": "posts/2025-02-05_make_lag_data.html",
    "title": "時系列データの lag, step の作成操作",
    "section": "",
    "text": "ML わからんので手始めに時系列データの予測をやってみようとしたところ、 lag, step の作成に手こずったのでメモを残す。\nDataFrame の MultiIndex がまだまだわからん。"
  },
  {
    "objectID": "posts/2025-02-05_make_lag_data.html#データ",
    "href": "posts/2025-02-05_make_lag_data.html#データ",
    "title": "時系列データの lag, step の作成操作",
    "section": "データ",
    "text": "データ\nkaggle の時系列の練習コンペ Store Sales - Time Series Forecasting のものを使用\nこのデータを使って multi step データを作成する過程を残す"
  },
  {
    "objectID": "posts/2025-02-05_make_lag_data.html#コード",
    "href": "posts/2025-02-05_make_lag_data.html#コード",
    "title": "時系列データの lag, step の作成操作",
    "section": "コード",
    "text": "コード\n\n# https://www.kaggle.com/code/ekrembayar/store-sales-ts-forecasting-a-comprehensive-guide より拝借\n# BASE\n# ------------------------------------------------------\nimport numpy as n\nimport pandas as pd\nfrom pathlib import Path\nimport os\nimport gc\nimport warnings\n\n# DATA VISUALIZATION\n# ------------------------------------------------------\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n\n# CONFIGURATIONS\n# ------------------------------------------------------\npd.set_option('display.max_columns', None)\npd.options.display.float_format = '{:.2f}'.format\nwarnings.filterwarnings('ignore')\n\n\nDATA_DIR=Path(\"../input/data/\")\n\n\n# Import\ntrain = pd.read_csv(DATA_DIR/\"train.csv\")\ntest = pd.read_csv(DATA_DIR/\"test.csv\")\nstores = pd.read_csv(DATA_DIR/\"stores.csv\")\n#sub = pd.read_csv(DATA_DIR/\"sample_submission.csv\")   \ntransactions = pd.read_csv(DATA_DIR/\"transactions.csv\").sort_values([\"store_nbr\", \"date\"])\n\n# Datetime\ntrain[\"date\"] = pd.to_datetime(train.date)\ntest[\"date\"] = pd.to_datetime(test.date)\ntransactions[\"date\"] = pd.to_datetime(transactions.date)\n\n# Data types\ntrain.onpromotion = train.onpromotion.astype(\"float16\")\ntrain.sales = train.sales.astype(\"float32\")\nstores.cluster = stores.cluster.astype(\"int8\")\n\ntrain\n\n\n\n\n\n\n\n\nid\ndate\nstore_nbr\nfamily\nsales\nonpromotion\n\n\n\n\n0\n0\n2013-01-01\n1\nAUTOMOTIVE\n0.00\n0.00\n\n\n1\n1\n2013-01-01\n1\nBABY CARE\n0.00\n0.00\n\n\n2\n2\n2013-01-01\n1\nBEAUTY\n0.00\n0.00\n\n\n3\n3\n2013-01-01\n1\nBEVERAGES\n0.00\n0.00\n\n\n4\n4\n2013-01-01\n1\nBOOKS\n0.00\n0.00\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3000883\n3000883\n2017-08-15\n9\nPOULTRY\n438.13\n0.00\n\n\n3000884\n3000884\n2017-08-15\n9\nPREPARED FOODS\n154.55\n1.00\n\n\n3000885\n3000885\n2017-08-15\n9\nPRODUCE\n2419.73\n148.00\n\n\n3000886\n3000886\n2017-08-15\n9\nSCHOOL AND OFFICE SUPPLIES\n121.00\n8.00\n\n\n3000887\n3000887\n2017-08-15\n9\nSEAFOOD\n16.00\n0.00\n\n\n\n\n3000888 rows × 6 columns\n\n\n\n\nデータ準備\n\ntrain.sort_values(['date','family','store_nbr'],inplace=True)\ntrain.reset_index(drop=True,inplace=True)\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train.family)\n\nLabelEncoder()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LabelEncoder?Documentation for LabelEncoderiFittedLabelEncoder() \n\n\n\ntrain[\"family\"]=le.transform(train.family)\n\n\n# デバッグ用なのでデータを絞る\ntrain=train[train['store_nbr'].isin([1,2]) & train['family'].isin([1,2])]\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nid\ndate\nstore_nbr\nfamily\nsales\nonpromotion\n\n\n\n\n54\n1\n2013-01-01\n1\n1\n0.00\n0.00\n\n\n55\n364\n2013-01-01\n2\n1\n0.00\n0.00\n\n\n108\n2\n2013-01-01\n1\n2\n0.00\n0.00\n\n\n109\n365\n2013-01-01\n2\n2\n0.00\n0.00\n\n\n1836\n1783\n2013-01-02\n1\n1\n0.00\n0.00\n\n\n\n\n\n\n\n\nKEY=[\"date\",\"store_nbr\",\"family\"]\ntrain_step=train[KEY+[\"sales\"]]\n\n\n# https://github.com/Kaggle/learntools/blob/master/learntools/time_series/utils.py より\ndef make_multistep_target(ts, steps, reverse=False):\n\n    shifts = reversed(range(steps)) if reverse else range(steps)\n    return pd.concat({f'y_step_{i + 1}': ts.shift(-i) for i in shifts}, axis=1)\n\n\n# HACK: unstack せずに1発で columns を multiIndex にできるか？\ntrain_step=train_step.set_index(['family', 'store_nbr','date', ])\ntrain_step=train_step.unstack(['family', 'store_nbr'])\n\n下のように軸{family, store_nbr} ごとに sales が時系列順で縦に並ぶようにする。\nこれを shift することで各軸ごとの lag や multistep_target を作成できる。\n\ntrain_step.head()\n\n\n\n\n\n\n\n\nsales\n\n\nfamily\n1\n2\n\n\nstore_nbr\n1\n2\n1\n2\n\n\ndate\n\n\n\n\n\n\n\n\n2013-01-01\n0.00\n0.00\n0.00\n0.00\n\n\n2013-01-02\n0.00\n0.00\n2.00\n3.00\n\n\n2013-01-03\n0.00\n0.00\n0.00\n2.00\n\n\n2013-01-04\n0.00\n0.00\n3.00\n3.00\n\n\n2013-01-05\n0.00\n0.00\n3.00\n9.00\n\n\n\n\n\n\n\n\ntrain_step=make_multistep_target(train_step[\"sales\"], 3)\n\n\ntrain_step=train_step.stack(['family', 'store_nbr']) # family, store_nbr を index に戻す\n\n\ntrain_step=train_step.dropna()\n\n\ntrain_step.head()\n\n\n\n\n\n\n\n\n\n\ny_step_1\ny_step_2\ny_step_3\n\n\ndate\nfamily\nstore_nbr\n\n\n\n\n\n\n\n2013-01-01\n1\n1\n0.00\n0.00\n0.00\n\n\n2\n0.00\n0.00\n0.00\n\n\n2\n1\n0.00\n2.00\n0.00\n\n\n2\n0.00\n3.00\n2.00\n\n\n2013-01-02\n1\n1\n0.00\n0.00\n0.00\n\n\n\n\n\n\n\n\nFEATURES=[\"onpromotion\",]\n\ntrain_feature_df=train[KEY+FEATURES]\n\n\ntrain_feature_df=train_feature_df.set_index(['family', 'store_nbr','date', ])\n\n\ntrain_with_step_df=train_feature_df.join(train_step, how='inner')\n\n\nX=train_with_step_df[FEATURES]\ny=train_with_step_df[[c for c in train_with_step_df.columns if c.startswith(\"y_step\")]]\n\n\nX\n\n\n\n\n\n\n\n\n\n\nonpromotion\n\n\nfamily\nstore_nbr\ndate\n\n\n\n\n\n1\n1\n2013-01-01\n0.00\n\n\n2\n2013-01-01\n0.00\n\n\n2\n1\n2013-01-01\n0.00\n\n\n2\n2013-01-01\n0.00\n\n\n1\n1\n2013-01-02\n0.00\n\n\n...\n...\n...\n...\n\n\n2\n2\n2017-08-12\n1.00\n\n\n1\n1\n2017-08-13\n0.00\n\n\n2\n2017-08-13\n0.00\n\n\n2\n1\n2017-08-13\n0.00\n\n\n2\n2017-08-13\n1.00\n\n\n\n\n6728 rows × 1 columns\n\n\n\n\ny\n\n\n\n\n\n\n\n\n\n\ny_step_1\ny_step_2\ny_step_3\n\n\nfamily\nstore_nbr\ndate\n\n\n\n\n\n\n\n1\n1\n2013-01-01\n0.00\n0.00\n0.00\n\n\n2\n2013-01-01\n0.00\n0.00\n0.00\n\n\n2\n1\n2013-01-01\n0.00\n2.00\n0.00\n\n\n2\n2013-01-01\n0.00\n3.00\n2.00\n\n\n1\n1\n2013-01-02\n0.00\n0.00\n0.00\n\n\n...\n...\n...\n...\n...\n...\n\n\n2\n2\n2017-08-12\n7.00\n10.00\n7.00\n\n\n1\n1\n2017-08-13\n0.00\n0.00\n0.00\n\n\n2\n2017-08-13\n0.00\n0.00\n0.00\n\n\n2\n1\n2017-08-13\n1.00\n6.00\n4.00\n\n\n2\n2017-08-13\n10.00\n7.00\n9.00\n\n\n\n\n6728 rows × 3 columns\n\n\n\n\n\nモデルの学習\n上の情報で学習してみる\n\nfrom sklearn.multioutput import RegressorChain\nfrom sklearn.linear_model import LinearRegression\n# from xgboost import XGBRegressor\n\nmodel = RegressorChain(base_estimator=LinearRegression())\n\n\nmodel.fit(X, y)\n\nRegressorChain(base_estimator=LinearRegression())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RegressorChain?Documentation for RegressorChainiFittedRegressorChain(base_estimator=LinearRegression()) base_estimator: LinearRegressionLinearRegression() LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\npred=model.predict(X)\n\n\npred.shape\n\n(6728, 3)"
  },
  {
    "objectID": "posts/2024-09-17_gitlab-ci-wasm-page.html",
    "href": "posts/2024-09-17_gitlab-ci-wasm-page.html",
    "title": "Rust WebAssembly + React (Vite) で作成したサイトを GitLab Pages へデプロイ",
    "section": "",
    "text": "Rust のプログラムを WebAssembly にコンパイルしたものを使う Web サイトを GitLab Pages にデプロイしたので、その時のメモ。"
  },
  {
    "objectID": "posts/2024-09-17_gitlab-ci-wasm-page.html#構成",
    "href": "posts/2024-09-17_gitlab-ci-wasm-page.html#構成",
    "title": "Rust WebAssembly + React (Vite) で作成したサイトを GitLab Pages へデプロイ",
    "section": "構成",
    "text": "構成\nwasm ディレクトリに Rust のプロジェクト、web ディレクトリに vite で作成した web プロジェクトがある。\nWebAssembly にコンパイルしたファイルは web/public/wasm 配置している。\nproject_root\n├── wasm            # Rust Project root\n│   └── ...\n└── web             # Web Project root\n    ├── public/wasm\n    ├── ...\n    └── vite.config.ts"
  },
  {
    "objectID": "posts/2024-09-17_gitlab-ci-wasm-page.html#gitlab-ci",
    "href": "posts/2024-09-17_gitlab-ci-wasm-page.html#gitlab-ci",
    "title": "Rust WebAssembly + React (Vite) で作成したサイトを GitLab Pages へデプロイ",
    "section": "gitlab-ci",
    "text": "gitlab-ci\ngitlab-ci.yml を以下のように作成した\nimage: node:16.5.0\n\n# wasm build\nwasm-build-job:\n  stage: build\n  image: \"rust:latest\"\n  script:\n    - cargo install wasm-pack\n    - cd wasm\n    - wasm-pack build --target web --out-dir ../web/public/wasm\n\n  # ビルドしたものを後続 job に共有する\n  artifacts:\n    paths:\n      - web/public/wasm\n  rules:\n    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH\n\n# pages へデプロイ\npages:\n  stage: deploy\n  cache:\n    key:\n      files:\n        - package-lock.json\n      prefix: npm\n    paths:\n      - node_modules/\n  script:\n    - cd web\n    - npm install\n    - npm run build\n    - cp -a dist/. ../public/\n  artifacts:\n    paths:\n      - public\n  rules:\n    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH"
  },
  {
    "objectID": "posts/2024-09-17_gitlab-ci-wasm-page.html#参考サイト",
    "href": "posts/2024-09-17_gitlab-ci-wasm-page.html#参考サイト",
    "title": "Rust WebAssembly + React (Vite) で作成したサイトを GitLab Pages へデプロイ",
    "section": "参考サイト",
    "text": "参考サイト\n\nhttps://developer.mozilla.org/ja/docs/WebAssembly/Rust_to_Wasm\nhttps://gitlab-docs.creationline.com/ee/ci/yaml/"
  },
  {
    "objectID": "posts/2025-06-29_toke-nyumon-27-01.html",
    "href": "posts/2025-06-29_toke-nyumon-27-01.html",
    "title": "統計入門27. 時系列データ分析-1",
    "section": "",
    "text": "※このレポートは OpenAI の ChatGPT によって自動生成されました。"
  },
  {
    "objectID": "posts/2025-06-29_toke-nyumon-27-01.html#弱定常過程と強定常過程の違い",
    "href": "posts/2025-06-29_toke-nyumon-27-01.html#弱定常過程と強定常過程の違い",
    "title": "統計入門27. 時系列データ分析-1",
    "section": "1. 弱定常過程と強定常過程の違い",
    "text": "1. 弱定常過程と強定常過程の違い\n\n\n\n\n\n\n\n\n種類\n定義\n特徴\n\n\n\n\n弱定常\n平均・分散・共分散が時間によらず一定\n共分散が**ラグ（時間差）**のみに依存\n\n\n強定常\nあらゆる時点の同時分布が、時刻のシフトに対して不変\n分布の形が時間で変わらない必要あり\n\n\n\n\n✅ ノイズの合成の分布が変わると、強定常にならない ✅ 正規分布ノイズなら、線形結合しても形が変わらず → 強定常に近い"
  },
  {
    "objectID": "posts/2025-06-29_toke-nyumon-27-01.html#arma-と-arima-の違い",
    "href": "posts/2025-06-29_toke-nyumon-27-01.html#arma-と-arima-の違い",
    "title": "統計入門27. 時系列データ分析-1",
    "section": "2. ARMA と ARIMA の違い",
    "text": "2. ARMA と ARIMA の違い\n\n\n\n\n\n\n\n\nモデル\n内容\n対象データ\n\n\n\n\nARMA(p, q)\n自己回帰 + 移動平均\n定常データ\n\n\nARIMA(p, d, q)\n差分 + ARMA（非定常 → 定常化）\n非定常データ（トレンドなど含む）\n\n\n\nARIMA は「差分をとったデータに ARMA モデルを当てる」と理解するとわかりやすい。"
  },
  {
    "objectID": "posts/2025-06-29_toke-nyumon-27-01.html#ディッキーフラー検定と拡張-df-検定",
    "href": "posts/2025-06-29_toke-nyumon-27-01.html#ディッキーフラー検定と拡張-df-検定",
    "title": "統計入門27. 時系列データ分析-1",
    "section": "3. ディッキー・フラー検定と拡張 DF 検定",
    "text": "3. ディッキー・フラー検定と拡張 DF 検定\n\n\n\n\n\n\n\n\n検定\n説明\n適用範囲\n\n\n\n\nDF 検定\n単純な AR(1)構造で単位根を検定\n誤差が白色雑音のとき限定\n\n\nADF 検定\nラグ付き差分項を追加し、誤差の自己相関も考慮\n実務ではこちらが主流 ✅\n\n\n\n\n単位根がある＝非定常。ADF は statsmodels.adfuller() などで実行可能。"
  },
  {
    "objectID": "posts/2025-06-29_toke-nyumon-27-01.html#maqモデルの次数の決め方",
    "href": "posts/2025-06-29_toke-nyumon-27-01.html#maqモデルの次数の決め方",
    "title": "統計入門27. 時系列データ分析-1",
    "section": "4. MA(q)モデルの次数の決め方",
    "text": "4. MA(q)モデルの次数の決め方\n\n自己相関関数（ACF）を使う\nMA(q)の場合、**ACF はラグ q で急にゼロ（打ち切り）**になる\n\n例：ACFがラグ2でパタッとゼロ → MA(2)モデルの候補"
  },
  {
    "objectID": "posts/2025-06-29_toke-nyumon-27-01.html#自己相関係数は何に対して計算する",
    "href": "posts/2025-06-29_toke-nyumon-27-01.html#自己相関係数は何に対して計算する",
    "title": "統計入門27. 時系列データ分析-1",
    "section": "5. 自己相関係数は何に対して計算する？",
    "text": "5. 自己相関係数は何に対して計算する？\n\n✅ ACF は「実際の時系列データ」に対して計算 → 各ラグにおいて、データがどれだけ自分と似ているか（自己相関）を測る"
  },
  {
    "objectID": "posts/2025-06-29_toke-nyumon-27-01.html#ar-モデルの次数の決め方pacf-を使う",
    "href": "posts/2025-06-29_toke-nyumon-27-01.html#ar-モデルの次数の決め方pacf-を使う",
    "title": "統計入門27. 時系列データ分析-1",
    "section": "6. AR モデルの次数の決め方：PACF を使う",
    "text": "6. AR モデルの次数の決め方：PACF を使う\n\n\n\nモデル\n判断に使う\n打ち切れる箇所\n\n\n\n\nAR(p)\nPACF（偏自己相関）\nラグ p\n\n\nMA(q)\nACF（自己相関）\nラグ q\n\n\n\n\nPACF がラグ 2 まで非ゼロ → AR(2)の候補\nACF/PACF は モデル構造のヒントであり、最終的には AIC/BIC などで比較"
  },
  {
    "objectID": "posts/2025-06-29_toke-nyumon-27-01.html#ar-と-armaどちらを使うか",
    "href": "posts/2025-06-29_toke-nyumon-27-01.html#ar-と-armaどちらを使うか",
    "title": "統計入門27. 時系列データ分析-1",
    "section": "7. AR と ARMA、どちらを使うか？",
    "text": "7. AR と ARMA、どちらを使うか？\n\n\n\nACF\nPACF\n推奨モデル\n\n\n\n\n減衰\n打ち切り\nAR(p)\n\n\n打ち切り\n減衰\nMA(q)\n\n\n減衰\n減衰\nARMA(p, q)\n\n\n\n\n実務では AR だけで足りない場合も多く、ARMA がより柔軟で使われやすい\nモデルは ACF/PACF で仮説を立て、AIC/BIC で確定が王道"
  },
  {
    "objectID": "posts/cudacheck.html",
    "href": "posts/cudacheck.html",
    "title": "gpu check",
    "section": "",
    "text": "import torch\ntorch.cuda.is_available()\n\nTrue\n\n\n\n!nvidia-smi\n\nMon Apr  3 22:00:02 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n| 30%   28C    P8    26W / 250W |    349MiB / 11264MiB |      3%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n\n\n\n\n\nCopyrightCopyright tamuraup. 2024. All Rights Reserved"
  },
  {
    "objectID": "posts/2024-05-12_tasuketsu.html",
    "href": "posts/2024-05-12_tasuketsu.html",
    "title": "numpy, pandas.DataFrame で多数決をとる",
    "section": "",
    "text": "多数決を取ったメモ。 今回は行ごとの最頻値を求める。\nscipy.stats.mode を使うと楽。\nscipy.stats.mode — SciPy v1.13.0 Manual\nimport numpy as np\nimport scipy.stats as stats\na=np.array([[1,1,2],[3,4,5],[1,1,1]])\na\n\narray([[1, 1, 2],\n       [3, 4, 5],\n       [1, 1, 1]])\nb=np.array([[1,2,3],[3,4,1],[1,1,2]])\nb\n\narray([[1, 2, 3],\n       [3, 4, 1],\n       [1, 1, 2]])\nc=np.hstack((a,b))\nc\n\narray([[1, 1, 2, 1, 2, 3],\n       [3, 4, 5, 3, 4, 1],\n       [1, 1, 1, 1, 1, 2]])\nstats.mode(a,axis=1,keepdims=True)[0]\n\narray([[1],\n       [3],\n       [1]])\nstats.mode(b,axis=1,keepdims=True)[0]\n\narray([[1],\n       [1],\n       [1]])\nstats.mode(c,axis=1,keepdims=True)[0]\n\narray([[1],\n       [3],\n       [1]])"
  },
  {
    "objectID": "posts/2024-05-12_tasuketsu.html#pandas.dataframe",
    "href": "posts/2024-05-12_tasuketsu.html#pandas.dataframe",
    "title": "numpy, pandas.DataFrame で多数決をとる",
    "section": "pandas.DataFrame",
    "text": "pandas.DataFrame\nDataFrame にも mode 関数がある。\npandas.DataFrame.mode — pandas 2.2.2 ドキュメント\n\nimport pandas as pd\n\n\ndf=pd.DataFrame(a)\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n1\n1\n2\n\n\n1\n3\n4\n5\n\n\n2\n1\n1\n1\n\n\n\n\n\n\n\n\ndf.mode(axis=1)\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n1.0\nNaN\nNaN\n\n\n1\n3.0\n4.0\n5.0\n\n\n2\n1.0\nNaN\nNaN\n\n\n\n\n\n\n\n最頻値が複数あればその分の列が返るっぽい？ (ドキュメントに書いてなさそう)\n\ndf=pd.DataFrame(c)\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n1\n1\n2\n1\n2\n3\n\n\n1\n3\n4\n5\n3\n4\n1\n\n\n2\n1\n1\n1\n1\n1\n2\n\n\n\n\n\n\n\n\ndf.mode(axis=1)\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1.0\nNaN\n\n\n1\n3.0\n4.0\n\n\n2\n1.0\nNaN"
  },
  {
    "objectID": "posts/2024-06-07_ahc028memo.html",
    "href": "posts/2024-06-07_ahc028memo.html",
    "title": "AHC028 解説放送メモ",
    "section": "",
    "text": "解説放送みたメモ"
  },
  {
    "objectID": "posts/2024-06-07_ahc028memo.html#解説放送メモ",
    "href": "posts/2024-06-07_ahc028memo.html#解説放送メモ",
    "title": "AHC028 解説放送メモ",
    "section": "解説放送メモ",
    "text": "解説放送メモ\n\n解説放送: https://www.youtube.com/watch?v=5Znl6bqHyck"
  },
  {
    "objectID": "posts/2024-06-07_ahc028memo.html#dp",
    "href": "posts/2024-06-07_ahc028memo.html#dp",
    "title": "AHC028 解説放送メモ",
    "section": "DP",
    "text": "DP\n\n文字の並びが決まると、最適な経路が DP で求まる\n\n単語の並びを決めた後にこれを利用して経路を求めても良い"
  },
  {
    "objectID": "posts/2024-06-07_ahc028memo.html#貪欲の評価関数",
    "href": "posts/2024-06-07_ahc028memo.html#貪欲の評価関数",
    "title": "AHC028 解説放送メモ",
    "section": "貪欲の評価関数",
    "text": "貪欲の評価関数\n\nコスト: 問題分にある計算式でのコスト\nベースコスト: (単語間の移動コストを無視して)単語のみを考えた場合のコスト\n\nこれはコストの下界になっている\n\n\n評価関数を コスト - ベースコスト とすることで余分な移動量を評価できる。\n評価関数がこれだけだとベースコストの大きさが考慮されない？"
  },
  {
    "objectID": "posts/2024-06-07_ahc028memo.html#ビームサーチ",
    "href": "posts/2024-06-07_ahc028memo.html#ビームサーチ",
    "title": "AHC028 解説放送メモ",
    "section": "ビームサーチ",
    "text": "ビームサーチ\n\n評価値: DP の最小値 - 使った単語のベースコストの和\n\nDP の最小値 は、DPテーブルから遷移先の最小値を選ぶことを繰り返して M 個目まで選択したときの最小値？ (推定コストの総和に該当？)\n1 度のコスト計算に O(M) はかかるので、単語数を減らしたほうがビーム幅を増やせる。\n\n単語のマージ\nbase(s1) + base(s2) &gt;= base(s1s2) + X であればマージする"
  },
  {
    "objectID": "posts/2024-06-07_ahc028memo.html#焼きなまし",
    "href": "posts/2024-06-07_ahc028memo.html#焼きなまし",
    "title": "AHC028 解説放送メモ",
    "section": "焼きなまし",
    "text": "焼きなまし\n\n近傍\n\n単語の並びから 1 つとって、別の場所に挿入する\n有向グラフの TSP に一部分をまとめて移動させる近傍がある？ それを用いる\n\n反転操作が発生せず、向きが固定\n連続している部分は良い並びになっていることが多いはずだから、それを大きく崩さないような操作になっている"
  },
  {
    "objectID": "posts/2024-05-09_bcewithlogitsloss_check.html",
    "href": "posts/2024-05-09_bcewithlogitsloss_check.html",
    "title": "BCEWithLogitsLoss の確認",
    "section": "",
    "text": "BCEWithLogitsLoss はマルチラベル分類のロスに使えることを知った\nBCEWithLogitsLoss\n\nimport numpy as np\nimport torch\n\n\ntarget = torch.ones([1, 10], dtype=torch.float32)  # 10 classes, batch size = 1\n# A prediction (logit)\noutput = torch.cat([torch.full([1, 5], 1.5),torch.full([1,5],1.0)],dim=1)\ncriterion = torch.nn.BCEWithLogitsLoss(reduction='none')\ncriterion(output, target)  # -log(sigmoid(output value))\n\ntensor([[0.2014, 0.2014, 0.2014, 0.2014, 0.2014, 0.3133, 0.3133, 0.3133, 0.3133,\n         0.3133]])\n\n\n\nsigmoid=torch.sigmoid(torch.Tensor([1.0,1.5]))\ntorch.log(sigmoid)\n\ntensor([-0.3133, -0.2014])\n\n\npos_weight は 各クラスで Positive/Negative のデータ数が不均衡な場合に設定すると良いらしい\n\n\n\nCopyrightCopyright tamuraup. 2024. All Rights Reserved"
  },
  {
    "objectID": "archives/2024-05-20_compute_delta.html",
    "href": "archives/2024-05-20_compute_delta.html",
    "title": "音声信号の “delta” という特徴量",
    "section": "",
    "text": "音声信号に delta という特徴量があるらしい。 Practical Cryptography\nあまり良くわかっていませんが、各周波数帯での軌跡を特徴量にするイメージでしょうか？ 各周波数帯のデータと [1.0] * time をたたみ込み演算をしたもの特徴量とするようです。\nkaggle でも過去の上位解法に使用されているようです。[4-th place solution] Inference and Training tips\nまた torchaudio にも実装されています。torchaudio.functional.compute_deltas — Torchaudio 2.2.0.dev20240520 documentation\n実装例"
  },
  {
    "objectID": "archives/2024-05-20_compute_delta.html#一応演算結果確認しておく",
    "href": "archives/2024-05-20_compute_delta.html#一応演算結果確認しておく",
    "title": "音声信号の “delta” という特徴量",
    "section": "一応演算結果確認しておく",
    "text": "一応演算結果確認しておく\n\nimport numpy as np\nimport torch\n\n\n# kaggle の solution notebook より\ndef compute_deltas(\n        specgram: torch.Tensor, win_length: int = 5, mode: str = \"replicate\"\n) -&gt; torch.Tensor:\n    device = specgram.device\n    dtype = specgram.dtype\n\n    # pack batch\n    shape = specgram.size()\n    specgram = specgram.reshape(1, -1, shape[-1])\n\n    assert win_length &gt;= 3\n\n    n = (win_length - 1) // 2\n\n    # twice sum of integer squared\n    denom = n * (n + 1) * (2 * n + 1) / 3\n\n    specgram = torch.nn.functional.pad(specgram, (n, n), mode=mode)\n\n    kernel = torch.arange(-n, n + 1, 1, device=device, dtype=dtype).repeat(\n        specgram.shape[1], 1, 1\n    )\n    output = (\n            torch.nn.functional.conv1d(specgram, kernel, groups=specgram.shape[1]) / denom\n    )\n\n    # unpack batch\n    output = output.reshape(shape)\n\n    return output\n\n\nx=torch.rand([1, 10]) # dim (freq, time)\nprint(x.shape)\nprint(x)\n\ntorch.Size([1, 10])\ntensor([[0.6760, 0.4193, 0.8303, 0.1316, 0.1804, 0.4828, 0.7212, 0.0631, 0.1529,\n         0.5410]])\n\n\n\ndelta=compute_deltas(x)\nprint(delta.shape)\nprint(delta)\n\ntorch.Size([1, 10])\ntensor([[ 0.0052, -0.0934, -0.1279, -0.0523,  0.0133,  0.0404, -0.0475, -0.0452,\n          0.0117,  0.1344]])\n\n\n\nn=2\ntmp=torch.nn.functional.pad(x, (2, 2), mode='replicate')\ndenom = n * (n + 1) * (2 * n + 1) / 3\n\n\nfor p in range(n,n+10):\n    sm=0.0\n    for i in range(1,n+1):\n        sm+=(-tmp[0][p-i] +tmp[0][p+i])*i\n    print(sm/denom)\n\ntensor(0.0052)\ntensor(-0.0934)\ntensor(-0.1279)\ntensor(-0.0523)\ntensor(0.0133)\ntensor(0.0404)\ntensor(-0.0475)\ntensor(-0.0452)\ntensor(0.0117)\ntensor(0.1344)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "めもちょう",
    "section": "",
    "text": "統計入門27. 時系列データ分析-1\n\n\n\n\n\n\n\n\n2025 年 06 月 29 日\n\n\n\n\n\n\n\n\n\n\n\n\nPython でスキャン画像の回転補正をやってみる\n\n\n\nPython\n\nimage\n\n\n\n\n\n\n\n\n\n2025 年 03 月 25 日\n\n\n\n\n\n\n\n\n\n\n\n\n時系列データの lag, step の作成操作\n\n\n\nml\n\n\n\n\n\n\n\n\n\n2025 年 02 月 05 日\n\n\n\n\n\n\n\n\n\n\n\n\nRust WebAssembly + React (Vite) で作成したサイトを GitLab Pages へデプロイ\n\n\n\nGitLab\n\nRust\n\nwasm\n\n\n\n\n\n\n\n\n\n2024 年 09 月 17 日\n\n\n\n\n\n\n\n\n\n\n\n\nガウス過程と機械学習 3章 ガウス過程回帰のパラメータ推定\n\n\n\nml\n\n\n\n\n\n\n\n\n\n2024 年 08 月 16 日\n\n\n\n\n\n\n\n\n\n\n\n\nデータ解析のための統計モデリング入門 3章のポアソン回帰\n\n\n\nml\n\n\n\n\n\n\n\n\n\n2024 年 08 月 16 日\n\n\n\n\n\n\n\n\n\n\n\n\nガウス過程と機械学習 3章 ガウス過程回帰\n\n\n\nml\n\nnotebook\n\n\n\n\n\n\n\n\n\n2024 年 08 月 14 日\n\n\n\n\n\n\n\n\n\n\n\n\nSQLコンテストの問題 4 を解く(第 1〜10 回)\n\n\n\nSQL\n\n\n\n\n\n\n\n\n\n2024 年 06 月 17 日\n\n\n\n\n\n\n\n\n\n\n\n\nAHC028 解説放送メモ\n\n\n\nahc\n\n\n\n\n\n\n\n\n\n2024 年 06 月 07 日\n\n\n\n\n\n\n\n\n\n\n\n\nnumpy, pandas.DataFrame で多数決をとる\n\n\n\nml\n\n\n\n\n\n\n\n\n\n2024 年 05 月 12 日\n\n\n\n\n\n\n\n\n\n\n\n\nPyTorch 学習結果の再現性確保\n\n\n\nml\n\n\n\n\n\n\n\n\n\n2024 年 05 月 11 日\n\n\n\n\n\n\n\n\n\n\n\n\nBCEWithLogitsLoss の確認\n\n\n\nml\n\n\n\n\n\n\n\n\n\n2024 年 05 月 09 日\n\n\n\n\n\n\n\n\n\n\n\n\n画像グリッド描画サンプル\n\n\n\nnotebook\n\nimage\n\n\n\n\n\n\n\n\n\n2024 年 05 月 04 日\n\n\n\n\n\n\n\n\n\n\n\n\nAHC032 解説放送メモ\n\n\n\nahc\n\n\n\n\n\n\n\n\n\n2024 年 05 月 03 日\n\n\n\n\n\n\n\n\n\n\n\n\ngpu check\n\n\n\nnotebook\n\n\n\n\n\n\n\n\n\n2024 年 05 月 01 日\n\n\n\n\n\nNo matching items\nCopyrightCopyright tamuraup. 2024. All Rights Reserved"
  }
]