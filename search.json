[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "about ページです\n\n\nCopyrightCopyright tamuraup. 2024. All Rights Reserved"
  },
  {
    "objectID": "archives/2024-05-01-chokudaisearch.html",
    "href": "archives/2024-05-01-chokudaisearch.html",
    "title": "chokudai search template",
    "section": "",
    "text": "chokudai search の練習をしたのでメモ。\n#[derive(PartialEq, Eq, Clone)]\nstruct State {\n    score: i64,\n}\nimpl State {\n    fn new() -&gt; Self {\n        todo!()\n    }\n}\n\nimpl Ord for State {\n    fn cmp(&self, other: &Self) -&gt; Ordering {\n        self.score.cmp(&other.score)\n    }\n}\n\nimpl PartialOrd for State {\n    fn partial_cmp(&self, other: &Self) -&gt; Option&lt;Ordering&gt; {\n        Some(self.cmp(other))\n    }\n}\n\nfn simple_chokudai_search(inp: &Input, timer: &Timer) {\n    let mut beams: Vec&lt;LimitedMaximumHeap&lt;State&gt;&gt; = vec![LimitedMaximumHeap::new(500); 51];\n\n    let mut iter = 0;\n    let mut best = 0;\n    beams[0].push(State::new());\n\n    'outer: loop {\n        iter += 1;\n        if timer.t() &gt;= 1.0 {\n            break;\n        }\n\n        for depth in 0..beams.len() {\n            if depth % 10 == 0 && timer.t() &gt;= 1.0 {\n                break 'outer;\n            }\n            if beams[depth].is_empty() {\n                continue;\n            }\n            // ここで複数回 pop すると 幅 &gt; 1 ってこと？\n            let state = beams[depth].pop().unwrap();\n            // TODO: なにか処理\n        }\n    }\n}\n\n\n\nCopyrightCopyright tamuraup. 2024. All Rights Reserved"
  },
  {
    "objectID": "posts/2024-05-03_ahc032memo.html",
    "href": "posts/2024-05-03_ahc032memo.html",
    "title": "AHC032 解説放送メモ",
    "section": "",
    "text": "解説放送みたメモ"
  },
  {
    "objectID": "posts/2024-05-03_ahc032memo.html#解説放送メモ",
    "href": "posts/2024-05-03_ahc032memo.html#解説放送メモ",
    "title": "AHC032 解説放送メモ",
    "section": "解説放送メモ",
    "text": "解説放送メモ\n\n解説放送: https://www.youtube.com/watch?v=9JS0wXXNiZk\neijirou さん解法: https://atcoder.jp/contests/ahc032/submissions?f.User=eijirou\nwata さん解法: https://atcoder.jp/contests/ahc032/submissions/52151408"
  },
  {
    "objectID": "posts/2024-05-03_ahc032memo.html#考察",
    "href": "posts/2024-05-03_ahc032memo.html#考察",
    "title": "AHC032 解説放送メモ",
    "section": "考察",
    "text": "考察\n\n3*3 のマスをすべて MOD*0.8 以上にするためには 5^9 通り候補がほしい\n\n(ランダムに与えられたスタンプで 1 マスを MOD*4/5 以上にできる確率 1/5)\n\n近傍がなめらかでないので、焼きなましなどの局所探索は不適\n同じ 9 マスを操作するなら、左上の点は分散させたほうが場合の数が増える"
  },
  {
    "objectID": "posts/2024-05-03_ahc032memo.html#ビームサーチ",
    "href": "posts/2024-05-03_ahc032memo.html#ビームサーチ",
    "title": "AHC032 解説放送メモ",
    "section": "ビームサーチ",
    "text": "ビームサーチ\n\n評価関数: \\(確定スコア + K * MOD * (1 - 進行度) * 残りの操作回数\\) (\\(K\\) はハイパーパラメータ)\n\n進行度が小さい場合は、後でより上手く揃えられるように操作回数を残したい\n\n\n\n「操作回数別でビームを分ける」とは\nwata さんの実装では操作回数別でビームを分けている。\nここでいうビームサーチは幅優先のビームサーチで「確定マス数」が深さに対応する。\n確定マス数を C とする。これを操作回数の基準として \\(\\pm W\\) 回の操作回数のズレを許容し、そのズレ幅ごとにビームをもつ。\n(例: コードの beam[W] はズレ幅 0 のビーム)\n\n\n揃えるマスの順\n35:30\n\n揃える順序は 3 マス揃えが連続しないほうが良い\nビームサーチの「確定スコア/確定マス数」を可視化することで確認している"
  },
  {
    "objectID": "posts/2024-05-03_ahc032memo.html#高速化",
    "href": "posts/2024-05-03_ahc032memo.html#高速化",
    "title": "AHC032 解説放送メモ",
    "section": "高速化",
    "text": "高速化\n\n不要なスタンプをできるだけ減らす工夫をしている\nスタンプを右上の値で分類し、右上マスを良い値にできるスタンプの候補を絞り込めるようにするなど\n\n\nK 分木の話\nwata さん実装の Searcher がそれ。\nやることは「3マス揃え」で話していることとほとんど同じ。\nMOD を K 個の区間に分けて、スタンプの 1 マス目の値で K 分割、2 マス目の値で K 分割 …\nのように分割する、木の深さが i マス目の分割となる K 分木を作り、良いスタンプの探索を高速にできるようにする。"
  },
  {
    "objectID": "posts/2025-06-15_upsampling.html",
    "href": "posts/2025-06-15_upsampling.html",
    "title": "アップサンプリング + パディング処理の流れ",
    "section": "",
    "text": "3次元の NumPy テンソル x（形状: (B, T, C)）を作成します。\nここではインデックスを使って中身を可視化しやすくしています。\n\n\n\n\nx[:, :, None, :].repeat(1, 1, ratio, 1) によって、時間軸の各ステップを ratio 倍に拡張します。\n\nx[:, :, None, :] → 次元を追加して (B, T, 1, C)\n.repeat(...) → (B, T, ratio, C) に拡張\n.reshape(B, T * ratio, C) → 時間軸を1本化\n\n結果、時間方向に T * ratio 長のテンソルが得られます。\n\n\n\n\n指定の長さ frame_size に満たない場合、最後のステップの値を複製して補います。\n\ny[:, -1:, :] で最後のフレームを取り出し\n.repeat(...) で必要数だけ繰り返し\ntorch.cat([y, padding], dim=1) で結合\n\n長さがすでに frame_size 以上なら、先頭から frame_size 分だけを切り出します。\n\n\n\n\nこの処理で、任意の入力テンソルを：\n\n指定の時間長 (frame_size) に統一\n滑らかに拡張しつつ、端は自然に補完\n\nすることができます。\n\n用途の例: - 音声や動画の時系列データ整形 - Transformer 系への入力前の整形 - ラベル付きデータのアライメント補正\n\n\nimport numpy as np\nimport torch\n\n\n# セル 2: パラメータと元データ作成\nB, T, C = 2, 3, 4  # バッチサイズ、時間長、チャネル数\nratio = 3         # アップサンプリング倍率\nframe_size = 11          # 最終的な長さにしたいサイズ\n\n# 見やすいようにインデックス付きデータを作成\nx = torch.arange(B * T * C).reshape(B, T, C)\nprint(\"元のxの shape:\", x.shape)\nprint(x)\n\n元のxの shape: torch.Size([2, 3, 4])\ntensor([[[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11]],\n\n        [[12, 13, 14, 15],\n         [16, 17, 18, 19],\n         [20, 21, 22, 23]]])\n\n\n\n# セル 3: ratio 倍に拡張（アップサンプリング）\nx_expanded = x[:, :, None, :].repeat(1, 1, ratio, 1)\nprint(\"拡張後の shape:\", x_expanded.shape)\nprint(x_expanded)\n\n拡張後の shape: torch.Size([2, 3, 3, 4])\ntensor([[[[ 0,  1,  2,  3],\n          [ 0,  1,  2,  3],\n          [ 0,  1,  2,  3]],\n\n         [[ 4,  5,  6,  7],\n          [ 4,  5,  6,  7],\n          [ 4,  5,  6,  7]],\n\n         [[ 8,  9, 10, 11],\n          [ 8,  9, 10, 11],\n          [ 8,  9, 10, 11]]],\n\n\n        [[[12, 13, 14, 15],\n          [12, 13, 14, 15],\n          [12, 13, 14, 15]],\n\n         [[16, 17, 18, 19],\n          [16, 17, 18, 19],\n          [16, 17, 18, 19]],\n\n         [[20, 21, 22, 23],\n          [20, 21, 22, 23],\n          [20, 21, 22, 23]]]])\n\n\n\n# セル 4: reshape して時間方向を1本化\nx_reshaped = x_expanded.reshape(B, T * ratio, C)\nprint(\"reshape後の shape:\", x_reshaped.shape)\nprint(x_reshaped)\n\nreshape後の shape: torch.Size([2, 9, 4])\ntensor([[[ 0,  1,  2,  3],\n         [ 0,  1,  2,  3],\n         [ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 4,  5,  6,  7],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11],\n         [ 8,  9, 10, 11],\n         [ 8,  9, 10, 11]],\n\n        [[12, 13, 14, 15],\n         [12, 13, 14, 15],\n         [12, 13, 14, 15],\n         [16, 17, 18, 19],\n         [16, 17, 18, 19],\n         [16, 17, 18, 19],\n         [20, 21, 22, 23],\n         [20, 21, 22, 23],\n         [20, 21, 22, 23]]])\n\n\n\n# セル 5: PyTorchテンソルに変換して padding 処理\ny = x_reshaped\n\n# 足りない長さを補う（最後のステップを複製）\nif y.shape[1] &lt; frame_size:\n    padding = y[:, -1:, :].repeat(1, frame_size - y.shape[1], 1)\n    y_padded = torch.cat([y, padding], dim=1)\nelse:\n    y_padded = y[:, :frame_size, :]  # もし長すぎたら切り詰める\n\nprint(\"最終 y_padded の shape:\", y_padded.shape)\nprint(y_padded)\n\n最終 y_padded の shape: torch.Size([2, 11, 4])\ntensor([[[ 0,  1,  2,  3],\n         [ 0,  1,  2,  3],\n         [ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 4,  5,  6,  7],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11],\n         [ 8,  9, 10, 11],\n         [ 8,  9, 10, 11],\n         [ 8,  9, 10, 11],\n         [ 8,  9, 10, 11]],\n\n        [[12, 13, 14, 15],\n         [12, 13, 14, 15],\n         [12, 13, 14, 15],\n         [16, 17, 18, 19],\n         [16, 17, 18, 19],\n         [16, 17, 18, 19],\n         [20, 21, 22, 23],\n         [20, 21, 22, 23],\n         [20, 21, 22, 23],\n         [20, 21, 22, 23],\n         [20, 21, 22, 23]]])"
  },
  {
    "objectID": "posts/2025-06-15_upsampling.html#アップサンプリング-パディング処理の流れ",
    "href": "posts/2025-06-15_upsampling.html#アップサンプリング-パディング処理の流れ",
    "title": "アップサンプリング + パディング処理の流れ",
    "section": "",
    "text": "3次元の NumPy テンソル x（形状: (B, T, C)）を作成します。\nここではインデックスを使って中身を可視化しやすくしています。\n\n\n\n\nx[:, :, None, :].repeat(1, 1, ratio, 1) によって、時間軸の各ステップを ratio 倍に拡張します。\n\nx[:, :, None, :] → 次元を追加して (B, T, 1, C)\n.repeat(...) → (B, T, ratio, C) に拡張\n.reshape(B, T * ratio, C) → 時間軸を1本化\n\n結果、時間方向に T * ratio 長のテンソルが得られます。\n\n\n\n\n指定の長さ frame_size に満たない場合、最後のステップの値を複製して補います。\n\ny[:, -1:, :] で最後のフレームを取り出し\n.repeat(...) で必要数だけ繰り返し\ntorch.cat([y, padding], dim=1) で結合\n\n長さがすでに frame_size 以上なら、先頭から frame_size 分だけを切り出します。\n\n\n\n\nこの処理で、任意の入力テンソルを：\n\n指定の時間長 (frame_size) に統一\n滑らかに拡張しつつ、端は自然に補完\n\nすることができます。\n\n用途の例: - 音声や動画の時系列データ整形 - Transformer 系への入力前の整形 - ラベル付きデータのアライメント補正\n\n\nimport numpy as np\nimport torch\n\n\n# セル 2: パラメータと元データ作成\nB, T, C = 2, 3, 4  # バッチサイズ、時間長、チャネル数\nratio = 3         # アップサンプリング倍率\nframe_size = 11          # 最終的な長さにしたいサイズ\n\n# 見やすいようにインデックス付きデータを作成\nx = torch.arange(B * T * C).reshape(B, T, C)\nprint(\"元のxの shape:\", x.shape)\nprint(x)\n\n元のxの shape: torch.Size([2, 3, 4])\ntensor([[[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11]],\n\n        [[12, 13, 14, 15],\n         [16, 17, 18, 19],\n         [20, 21, 22, 23]]])\n\n\n\n# セル 3: ratio 倍に拡張（アップサンプリング）\nx_expanded = x[:, :, None, :].repeat(1, 1, ratio, 1)\nprint(\"拡張後の shape:\", x_expanded.shape)\nprint(x_expanded)\n\n拡張後の shape: torch.Size([2, 3, 3, 4])\ntensor([[[[ 0,  1,  2,  3],\n          [ 0,  1,  2,  3],\n          [ 0,  1,  2,  3]],\n\n         [[ 4,  5,  6,  7],\n          [ 4,  5,  6,  7],\n          [ 4,  5,  6,  7]],\n\n         [[ 8,  9, 10, 11],\n          [ 8,  9, 10, 11],\n          [ 8,  9, 10, 11]]],\n\n\n        [[[12, 13, 14, 15],\n          [12, 13, 14, 15],\n          [12, 13, 14, 15]],\n\n         [[16, 17, 18, 19],\n          [16, 17, 18, 19],\n          [16, 17, 18, 19]],\n\n         [[20, 21, 22, 23],\n          [20, 21, 22, 23],\n          [20, 21, 22, 23]]]])\n\n\n\n# セル 4: reshape して時間方向を1本化\nx_reshaped = x_expanded.reshape(B, T * ratio, C)\nprint(\"reshape後の shape:\", x_reshaped.shape)\nprint(x_reshaped)\n\nreshape後の shape: torch.Size([2, 9, 4])\ntensor([[[ 0,  1,  2,  3],\n         [ 0,  1,  2,  3],\n         [ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 4,  5,  6,  7],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11],\n         [ 8,  9, 10, 11],\n         [ 8,  9, 10, 11]],\n\n        [[12, 13, 14, 15],\n         [12, 13, 14, 15],\n         [12, 13, 14, 15],\n         [16, 17, 18, 19],\n         [16, 17, 18, 19],\n         [16, 17, 18, 19],\n         [20, 21, 22, 23],\n         [20, 21, 22, 23],\n         [20, 21, 22, 23]]])\n\n\n\n# セル 5: PyTorchテンソルに変換して padding 処理\ny = x_reshaped\n\n# 足りない長さを補う（最後のステップを複製）\nif y.shape[1] &lt; frame_size:\n    padding = y[:, -1:, :].repeat(1, frame_size - y.shape[1], 1)\n    y_padded = torch.cat([y, padding], dim=1)\nelse:\n    y_padded = y[:, :frame_size, :]  # もし長すぎたら切り詰める\n\nprint(\"最終 y_padded の shape:\", y_padded.shape)\nprint(y_padded)\n\n最終 y_padded の shape: torch.Size([2, 11, 4])\ntensor([[[ 0,  1,  2,  3],\n         [ 0,  1,  2,  3],\n         [ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 4,  5,  6,  7],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11],\n         [ 8,  9, 10, 11],\n         [ 8,  9, 10, 11],\n         [ 8,  9, 10, 11],\n         [ 8,  9, 10, 11]],\n\n        [[12, 13, 14, 15],\n         [12, 13, 14, 15],\n         [12, 13, 14, 15],\n         [16, 17, 18, 19],\n         [16, 17, 18, 19],\n         [16, 17, 18, 19],\n         [20, 21, 22, 23],\n         [20, 21, 22, 23],\n         [20, 21, 22, 23],\n         [20, 21, 22, 23],\n         [20, 21, 22, 23]]])"
  },
  {
    "objectID": "posts/2025-06-15_AUCLoss.html",
    "href": "posts/2025-06-15_AUCLoss.html",
    "title": "AUCLoss に関する解説レポート",
    "section": "",
    "text": "Note\n\n\n\n本レポートは、OpenAI の対話型 AI ChatGPT により生成・整理されたものです。"
  },
  {
    "objectID": "posts/2025-06-15_AUCLoss.html#概要",
    "href": "posts/2025-06-15_AUCLoss.html#概要",
    "title": "AUCLoss に関する解説レポート",
    "section": "概要",
    "text": "概要\nこのレポートでは、Kaggle の BirdCLEF 2025 コンペティションのディスカッション\n（リンクはこちら）で共有されていた AUCLoss クラスについての理解をまとめています。\nこのカスタム損失関数は、AUC（Area Under the Curve）を最大化する目的で設計されたペアワイズロスです。"
  },
  {
    "objectID": "posts/2025-06-15_AUCLoss.html#aucloss-のコードと目的",
    "href": "posts/2025-06-15_AUCLoss.html#aucloss-のコードと目的",
    "title": "AUCLoss に関する解説レポート",
    "section": "AUCLoss のコードと目的",
    "text": "AUCLoss のコードと目的\nclass AUCLoss(nn.Module):\n    def __init__(self, margin=1.0, pos_weight=1.0, neg_weight=1.0):\n        super().__init__()\n        self.margin = margin\n        self.pos_weight = pos_weight\n        self.neg_weight = neg_weight\n\n    def forward(self, preds, labels, sample_weights=None):\n        pos_preds = preds[labels == 1]\n        neg_preds = preds[labels == 0]\n\n        if len(pos_preds) == 0 or len(neg_preds) == 0:\n            return torch.tensor(0.0, device=preds.device)\n\n        if sample_weights is not None:\n            sample_weights = torch.stack([sample_weights]*labels.shape[1], dim=1)\n            pos_weights = sample_weights[labels == 1]\n            neg_weights = sample_weights[labels == 0]\n        else:\n            pos_weights = torch.ones_like(pos_preds) * self.pos_weight\n            neg_weights = torch.ones_like(neg_preds) * self.neg_weight\n\n        diff = pos_preds.unsqueeze(1) - neg_preds.unsqueeze(0)\n        loss_matrix = torch.log(1 + torch.exp(-diff * self.margin))\n        weighted_loss = loss_matrix * pos_weights.unsqueeze(1) * neg_weights.unsqueeze(0)\n\n        return weighted_loss.mean()"
  },
  {
    "objectID": "posts/2025-06-15_AUCLoss.html#コード解説の要点",
    "href": "posts/2025-06-15_AUCLoss.html#コード解説の要点",
    "title": "AUCLoss に関する解説レポート",
    "section": "コード解説の要点",
    "text": "コード解説の要点\n\n正例と負例の分離\nlabels == 1 および labels == 0 により、予測スコアを正例 (pos_preds) と負例 (neg_preds) に分割。\n\n\nサンプル重みの適用\nサンプルごとに重みが指定されていればそれを使用。なければ、指定された pos_weight と neg_weight を使用。\n\n\nペアワイズ差分の計算（重要）\ndiff = pos_preds.unsqueeze(1) - neg_preds.unsqueeze(0)\n\nunsqueeze(1) で [N_pos] → [N_pos, 1]\nunsqueeze(0) で [N_neg] → [1, N_neg]\n結果として [N_pos, N_neg] の行列となり、正例と負例の全組み合わせのスコア差を一括計算\n\n\n例：\npos_preds = [0.8, 0.9]\nneg_preds = [0.2, 0.4, 0.6]\n\ndiff = [[0.6, 0.4, 0.2],\n        [0.7, 0.5, 0.3]]\n\n\n\nロジスティック損失の適用\nloss_matrix = torch.log(1 + torch.exp(-diff * margin))\n正例が負例よりスコアが高いときは損失が小さく、逆転していると損失が大きくなる。\n\n\n重み付きの損失平均\nweighted_loss.mean()\nすべてのペアの損失を、正例・負例の重みで調整して平均。"
  },
  {
    "objectID": "posts/2025-06-15_AUCLoss.html#まとめ",
    "href": "posts/2025-06-15_AUCLoss.html#まとめ",
    "title": "AUCLoss に関する解説レポート",
    "section": "まとめ",
    "text": "まとめ\n\nAUCLoss は、分類モデルにおいて スコアの順序関係 を学習させるペアワイズ損失関数です。\n正例が負例よりスコアが高くなるように学習を促します。\nクロスエントロピーとは異なり、AUC の最適化に特化しています。"
  },
  {
    "objectID": "posts/2025-06-15_AUCLoss.html#出典",
    "href": "posts/2025-06-15_AUCLoss.html#出典",
    "title": "AUCLoss に関する解説レポート",
    "section": "出典",
    "text": "出典\n\nKaggle BirdCLEF 2025 Discussion: AUCLoss"
  },
  {
    "objectID": "posts/2024-06-07_ahc028memo.html",
    "href": "posts/2024-06-07_ahc028memo.html",
    "title": "AHC028 解説放送メモ",
    "section": "",
    "text": "解説放送みたメモ"
  },
  {
    "objectID": "posts/2024-06-07_ahc028memo.html#解説放送メモ",
    "href": "posts/2024-06-07_ahc028memo.html#解説放送メモ",
    "title": "AHC028 解説放送メモ",
    "section": "解説放送メモ",
    "text": "解説放送メモ\n\n解説放送: https://www.youtube.com/watch?v=5Znl6bqHyck"
  },
  {
    "objectID": "posts/2024-06-07_ahc028memo.html#dp",
    "href": "posts/2024-06-07_ahc028memo.html#dp",
    "title": "AHC028 解説放送メモ",
    "section": "DP",
    "text": "DP\n\n文字の並びが決まると、最適な経路が DP で求まる\n\n単語の並びを決めた後にこれを利用して経路を求めても良い"
  },
  {
    "objectID": "posts/2024-06-07_ahc028memo.html#貪欲の評価関数",
    "href": "posts/2024-06-07_ahc028memo.html#貪欲の評価関数",
    "title": "AHC028 解説放送メモ",
    "section": "貪欲の評価関数",
    "text": "貪欲の評価関数\n\nコスト: 問題分にある計算式でのコスト\nベースコスト: (単語間の移動コストを無視して)単語のみを考えた場合のコスト\n\nこれはコストの下界になっている\n\n\n評価関数を コスト - ベースコスト とすることで余分な移動量を評価できる。\n評価関数がこれだけだとベースコストの大きさが考慮されない？"
  },
  {
    "objectID": "posts/2024-06-07_ahc028memo.html#ビームサーチ",
    "href": "posts/2024-06-07_ahc028memo.html#ビームサーチ",
    "title": "AHC028 解説放送メモ",
    "section": "ビームサーチ",
    "text": "ビームサーチ\n\n評価値: DP の最小値 - 使った単語のベースコストの和\n\nDP の最小値 は、DPテーブルから遷移先の最小値を選ぶことを繰り返して M 個目まで選択したときの最小値？ (推定コストの総和に該当？)\n1 度のコスト計算に O(M) はかかるので、単語数を減らしたほうがビーム幅を増やせる。\n\n単語のマージ\nbase(s1) + base(s2) &gt;= base(s1s2) + X であればマージする"
  },
  {
    "objectID": "posts/2024-06-07_ahc028memo.html#焼きなまし",
    "href": "posts/2024-06-07_ahc028memo.html#焼きなまし",
    "title": "AHC028 解説放送メモ",
    "section": "焼きなまし",
    "text": "焼きなまし\n\n近傍\n\n単語の並びから 1 つとって、別の場所に挿入する\n有向グラフの TSP に一部分をまとめて移動させる近傍がある？ それを用いる\n\n反転操作が発生せず、向きが固定\n連続している部分は良い並びになっていることが多いはずだから、それを大きく崩さないような操作になっている"
  },
  {
    "objectID": "posts/2024-05-12_tasuketsu.html",
    "href": "posts/2024-05-12_tasuketsu.html",
    "title": "numpy, pandas.DataFrame で多数決をとる",
    "section": "",
    "text": "多数決を取ったメモ。 今回は行ごとの最頻値を求める。\nscipy.stats.mode を使うと楽。\nscipy.stats.mode — SciPy v1.13.0 Manual\nimport numpy as np\nimport scipy.stats as stats\na=np.array([[1,1,2],[3,4,5],[1,1,1]])\na\n\narray([[1, 1, 2],\n       [3, 4, 5],\n       [1, 1, 1]])\nb=np.array([[1,2,3],[3,4,1],[1,1,2]])\nb\n\narray([[1, 2, 3],\n       [3, 4, 1],\n       [1, 1, 2]])\nc=np.hstack((a,b))\nc\n\narray([[1, 1, 2, 1, 2, 3],\n       [3, 4, 5, 3, 4, 1],\n       [1, 1, 1, 1, 1, 2]])\nstats.mode(a,axis=1,keepdims=True)[0]\n\narray([[1],\n       [3],\n       [1]])\nstats.mode(b,axis=1,keepdims=True)[0]\n\narray([[1],\n       [1],\n       [1]])\nstats.mode(c,axis=1,keepdims=True)[0]\n\narray([[1],\n       [3],\n       [1]])"
  },
  {
    "objectID": "posts/2024-05-12_tasuketsu.html#pandas.dataframe",
    "href": "posts/2024-05-12_tasuketsu.html#pandas.dataframe",
    "title": "numpy, pandas.DataFrame で多数決をとる",
    "section": "pandas.DataFrame",
    "text": "pandas.DataFrame\nDataFrame にも mode 関数がある。\npandas.DataFrame.mode — pandas 2.2.2 ドキュメント\n\nimport pandas as pd\n\n\ndf=pd.DataFrame(a)\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n1\n1\n2\n\n\n1\n3\n4\n5\n\n\n2\n1\n1\n1\n\n\n\n\n\n\n\n\ndf.mode(axis=1)\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n1.0\nNaN\nNaN\n\n\n1\n3.0\n4.0\n5.0\n\n\n2\n1.0\nNaN\nNaN\n\n\n\n\n\n\n\n最頻値が複数あればその分の列が返るっぽい？ (ドキュメントに書いてなさそう)\n\ndf=pd.DataFrame(c)\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n1\n1\n2\n1\n2\n3\n\n\n1\n3\n4\n5\n3\n4\n1\n\n\n2\n1\n1\n1\n1\n1\n2\n\n\n\n\n\n\n\n\ndf.mode(axis=1)\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1.0\nNaN\n\n\n1\n3.0\n4.0\n\n\n2\n1.0\nNaN"
  },
  {
    "objectID": "posts/cudacheck.html",
    "href": "posts/cudacheck.html",
    "title": "gpu check",
    "section": "",
    "text": "import torch\ntorch.cuda.is_available()\n\nTrue\n\n\n\n!nvidia-smi\n\nMon Apr  3 22:00:02 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n| 30%   28C    P8    26W / 250W |    349MiB / 11264MiB |      3%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n\n\n\n\n\nCopyrightCopyright tamuraup. 2024. All Rights Reserved"
  },
  {
    "objectID": "posts/2025-06-29_toke-nyumon-27-01.html",
    "href": "posts/2025-06-29_toke-nyumon-27-01.html",
    "title": "統計入門27. 時系列データ分析-1",
    "section": "",
    "text": "Note\n\n\n\nこのレポートは OpenAI の ChatGPT によって自動生成されました。"
  },
  {
    "objectID": "posts/2025-06-29_toke-nyumon-27-01.html#弱定常過程と強定常過程の違い",
    "href": "posts/2025-06-29_toke-nyumon-27-01.html#弱定常過程と強定常過程の違い",
    "title": "統計入門27. 時系列データ分析-1",
    "section": "1. 弱定常過程と強定常過程の違い",
    "text": "1. 弱定常過程と強定常過程の違い\n\n\n\n\n\n\n\n\n種類\n定義\n特徴\n\n\n\n\n弱定常\n平均・分散・共分散が時間によらず一定\n共分散が**ラグ（時間差）**のみに依存\n\n\n強定常\nあらゆる時点の同時分布が、時刻のシフトに対して不変\n分布の形が時間で変わらない必要あり\n\n\n\n\n✅ ノイズの合成の分布が変わると、強定常にならない\n✅ 正規分布ノイズなら、線形結合しても形が変わらず → 強定常に近い"
  },
  {
    "objectID": "posts/2025-06-29_toke-nyumon-27-01.html#arma-と-arima-の違い",
    "href": "posts/2025-06-29_toke-nyumon-27-01.html#arma-と-arima-の違い",
    "title": "統計入門27. 時系列データ分析-1",
    "section": "2. ARMA と ARIMA の違い",
    "text": "2. ARMA と ARIMA の違い\n\n\n\n\n\n\n\n\nモデル\n内容\n対象データ\n\n\n\n\nARMA(p, q)\n自己回帰 + 移動平均\n定常データ\n\n\nARIMA(p, d, q)\n差分 + ARMA（非定常 → 定常化）\n非定常データ（トレンドなど含む）\n\n\n\nARIMA は「差分をとったデータに ARMA モデルを当てる」と理解するとわかりやすい。"
  },
  {
    "objectID": "posts/2025-06-29_toke-nyumon-27-01.html#ディッキーフラー検定と拡張-df-検定",
    "href": "posts/2025-06-29_toke-nyumon-27-01.html#ディッキーフラー検定と拡張-df-検定",
    "title": "統計入門27. 時系列データ分析-1",
    "section": "3. ディッキー・フラー検定と拡張 DF 検定",
    "text": "3. ディッキー・フラー検定と拡張 DF 検定\n\n\n\n\n\n\n\n\n検定\n説明\n適用範囲\n\n\n\n\nDF 検定\n単純な AR(1)構造で単位根を検定\n誤差が白色雑音のとき限定\n\n\nADF 検定\nラグ付き差分項を追加し、誤差の自己相関も考慮\n実務ではこちらが主流 ✅\n\n\n\n\n単位根がある＝非定常。ADF は statsmodels.adfuller() などで実行可能。"
  },
  {
    "objectID": "posts/2025-06-29_toke-nyumon-27-01.html#maqモデルの次数の決め方",
    "href": "posts/2025-06-29_toke-nyumon-27-01.html#maqモデルの次数の決め方",
    "title": "統計入門27. 時系列データ分析-1",
    "section": "4. MA(q)モデルの次数の決め方",
    "text": "4. MA(q)モデルの次数の決め方\n\n自己相関関数（ACF）を使う\nMA(q)の場合、**ACF はラグ q で急にゼロ（打ち切り）**になる\n\n例：ACFがラグ2でパタッとゼロ → MA(2)モデルの候補"
  },
  {
    "objectID": "posts/2025-06-29_toke-nyumon-27-01.html#自己相関係数は何に対して計算する",
    "href": "posts/2025-06-29_toke-nyumon-27-01.html#自己相関係数は何に対して計算する",
    "title": "統計入門27. 時系列データ分析-1",
    "section": "5. 自己相関係数は何に対して計算する？",
    "text": "5. 自己相関係数は何に対して計算する？\n\n✅ ACF は「実際の時系列データ」に対して計算\n→ 各ラグにおいて、データがどれだけ自分と似ているか（自己相関）を測る"
  },
  {
    "objectID": "posts/2025-06-29_toke-nyumon-27-01.html#ar-モデルの次数の決め方pacf-を使う",
    "href": "posts/2025-06-29_toke-nyumon-27-01.html#ar-モデルの次数の決め方pacf-を使う",
    "title": "統計入門27. 時系列データ分析-1",
    "section": "6. AR モデルの次数の決め方：PACF を使う",
    "text": "6. AR モデルの次数の決め方：PACF を使う\n\n\n\nモデル\n判断に使う\n打ち切れる箇所\n\n\n\n\nAR(p)\nPACF（偏自己相関）\nラグ p\n\n\nMA(q)\nACF（自己相関）\nラグ q\n\n\n\n\nPACF がラグ 2 まで非ゼロ → AR(2)の候補\nACF/PACF は モデル構造のヒントであり、最終的には AIC/BIC などで比較"
  },
  {
    "objectID": "posts/2025-06-29_toke-nyumon-27-01.html#ar-と-armaどちらを使うか",
    "href": "posts/2025-06-29_toke-nyumon-27-01.html#ar-と-armaどちらを使うか",
    "title": "統計入門27. 時系列データ分析-1",
    "section": "7. AR と ARMA、どちらを使うか？",
    "text": "7. AR と ARMA、どちらを使うか？\n\n\n\nACF\nPACF\n推奨モデル\n\n\n\n\n減衰\n打ち切り\nAR(p)\n\n\n打ち切り\n減衰\nMA(q)\n\n\n減衰\n減衰\nARMA(p, q)\n\n\n\n\n実務では AR だけで足りない場合も多く、ARMA がより柔軟で使われやすい\nモデルは ACF/PACF で仮説を立て、AIC/BIC で確定が王道"
  },
  {
    "objectID": "posts/2024-09-17_gitlab-ci-wasm-page.html",
    "href": "posts/2024-09-17_gitlab-ci-wasm-page.html",
    "title": "Rust WebAssembly + React (Vite) で作成したサイトを GitLab Pages へデプロイ",
    "section": "",
    "text": "Rust のプログラムを WebAssembly にコンパイルしたものを使う Web サイトを GitLab Pages にデプロイしたので、その時のメモ。"
  },
  {
    "objectID": "posts/2024-09-17_gitlab-ci-wasm-page.html#構成",
    "href": "posts/2024-09-17_gitlab-ci-wasm-page.html#構成",
    "title": "Rust WebAssembly + React (Vite) で作成したサイトを GitLab Pages へデプロイ",
    "section": "構成",
    "text": "構成\nwasm ディレクトリに Rust のプロジェクト、web ディレクトリに vite で作成した web プロジェクトがある。\nWebAssembly にコンパイルしたファイルは web/public/wasm 配置している。\nproject_root\n├── wasm            # Rust Project root\n│   └── ...\n└── web             # Web Project root\n    ├── public/wasm\n    ├── ...\n    └── vite.config.ts"
  },
  {
    "objectID": "posts/2024-09-17_gitlab-ci-wasm-page.html#gitlab-ci",
    "href": "posts/2024-09-17_gitlab-ci-wasm-page.html#gitlab-ci",
    "title": "Rust WebAssembly + React (Vite) で作成したサイトを GitLab Pages へデプロイ",
    "section": "gitlab-ci",
    "text": "gitlab-ci\ngitlab-ci.yml を以下のように作成した\nimage: node:16.5.0\n\n# wasm build\nwasm-build-job:\n  stage: build\n  image: \"rust:latest\"\n  script:\n    - cargo install wasm-pack\n    - cd wasm\n    - wasm-pack build --target web --out-dir ../web/public/wasm\n\n  # ビルドしたものを後続 job に共有する\n  artifacts:\n    paths:\n      - web/public/wasm\n  rules:\n    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH\n\n# pages へデプロイ\npages:\n  stage: deploy\n  cache:\n    key:\n      files:\n        - package-lock.json\n      prefix: npm\n    paths:\n      - node_modules/\n  script:\n    - cd web\n    - npm install\n    - npm run build\n    - cp -a dist/. ../public/\n  artifacts:\n    paths:\n      - public\n  rules:\n    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH"
  },
  {
    "objectID": "posts/2024-09-17_gitlab-ci-wasm-page.html#参考サイト",
    "href": "posts/2024-09-17_gitlab-ci-wasm-page.html#参考サイト",
    "title": "Rust WebAssembly + React (Vite) で作成したサイトを GitLab Pages へデプロイ",
    "section": "参考サイト",
    "text": "参考サイト\n\nhttps://developer.mozilla.org/ja/docs/WebAssembly/Rust_to_Wasm\nhttps://gitlab-docs.creationline.com/ee/ci/yaml/"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html",
    "href": "posts/2024-06-17_sqlcontest-01-10.html",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "",
    "text": "SQL の Window 関数の練習のため SQL コンテスト の問題4 を解いていく。\n注意"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第1回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第1回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第1回 問題4",
    "text": "第1回 問題4\nRANK を使う問題\nwith t1 as (select pf_code\n                 , NATION_CODE\n                 , amt\n                 , rank() over (\n        partition by pf_code\n        order by amt desc,NATION_CODE\n        ) as rnk\n            from FOREIGNER fo\n            where NATION_CODE &lt;&gt; '113')\n\nselect t1.pf_code as '都道府県コード'\n     , pf.PF_NAME as '都道府県名'\n     , max(case when t1.rnk == 1 then na.NATION_NAME else '' end) '1位 国名'\n     , max(case when t1.rnk == 1 then t1.amt else 0 end) '1位 人数'\n     , max(case when t1.rnk == 2 then na.NATION_NAME else '' end) '2位 国名'\n     , max(case when t1.rnk == 2 then t1.amt else 0 end) '2位 人数'\n     , max(case when t1.rnk == 3 then na.NATION_NAME else '' end) '3位 国名'\n     , max(case when t1.rnk == 3 then t1.amt else 0 end) '3位 人数'\n     , max(tot.amt) '合計人数'\nfrom t1\n     join PREFECTURE pf on t1.PF_CODE = pf.PF_CODE\n     join NATIONALITY na on t1.NATION_CODE = na.NATION_CODE\n     join (select pf_code, sum(amt) amt\n           from FOREIGNER\n           where NATION_CODE &lt;&gt; '113'\n           group by PF_CODE) tot on tot.PF_CODE = t1.pf_code\n\nwhere rnk &lt;= 3\n\ngroup by t1.PF_CODE\norder by 9 desc, 1"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第2回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第2回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第2回 問題4",
    "text": "第2回 問題4\n横持ちデータを縦持ちにする問題。\nWITH t1 AS (SELECT CASE no\n                       WHEN 0 THEN '小学校'\n                       WHEN 1 THEN '中学校'\n                       WHEN 2 THEN '高校'\n                       WHEN 3 THEN '短大'\n                       WHEN 4 THEN '大学'\n                       ELSE '大学院'\n    END KIND\n                 , CASE no\n                       WHEN 0 THEN ELEMENTARY\n                       WHEN 1 THEN MIDDLE\n                       WHEN 2 THEN HIGH\n                       WHEN 3 THEN JUNIOR_CLG\n                       WHEN 4 THEN COLLEGE\n                       ELSE GRADUATE\n        END AMT\n                 , no\n                 , SURVEY_YEAR\n                 , PF_CODE\n            FROM ENROLLMENT_STATUS\n               , (WITH SEQ AS (SELECT 0 AS no UNION ALL SELECT no + 1 AS no FROM SEQ WHERE no + 1 &lt; 6) SELECT * FROM seq) seq\n\n            WHERE SURVEY_YEAR = 2020)\nSELECT SURVEY_YEAR SV_YEAR\n     , PF.PF_NAME PREFECTURE\n     , KIND KIND\n     , SUM(AMT) AS AMT\nFROM t1\n     JOIN PREFECTURE PF ON PF.PF_CODE = t1.PF_CODE\nWHERE t1.amt IS NOT NULL\nGROUP BY SURVEY_YEAR, PF.PF_NAME, KIND, no\nORDER BY PF.PF_CODE, no"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第3回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第3回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第3回 問題4",
    "text": "第3回 問題4\nコンテストの得点と順位を求める問題。問題文長すぎ。\nテストケース 2 のテストデータの不備で、\nENTRIES.CONTEST_ID = 2 であるが SUBMISSIONS.CONTEST_ID が 2 でないデータが含まれていると思われる。\nWITH t1 AS (SELECT sub.*\n                 , ent.STARTED_AT\n            FROM ENTRIES ent\n                 JOIN SUBMISSIONS sub ON sub.ENTRY_ID = ent.ENTRY_ID\n            WHERE ent.CONTEST_ID = 2\n              AND sub.CONTEST_ID = 2)\n   , ac AS (SELECT entry_id\n                 , SUM(point) pt\n                 , MAX(STRFTIME('%s', SUBMITTED_AT)) - MAX(STRFTIME('%s', STARTED_AT)) tdiff\n            FROM t1\n            WHERE STATUS = 'AC'\n            GROUP BY ENTRY_ID)\n   , wa AS (SELECT t1.entry_id\n                 , COUNT(*) cnt\n            FROM t1\n                 JOIN (SELECT ENTRY_ID, PROBLEM_ID, SUBMITTED_AT\n                       FROM t1\n                       WHERE STATUS = 'AC') tac\n                      ON t1.ENTRY_ID = tac.ENTRY_ID\n                          AND t1.PROBLEM_ID = tac.PROBLEM_ID\n                          AND t1.SUBMITTED_AT &lt;= tac.SUBMITTED_AT\n            WHERE t1.STATUS &lt;&gt; 'AC'\n            GROUP BY t1.ENTRY_ID)\nSELECT RANK() OVER (\n    ORDER BY t.POINT DESC,t.EX_TIME\n    ) AS RANK\n     , t.*\nFROM (SELECT ent.USER_ID\n           , ac.pt AS POINT\n           , ac.tdiff + 300 * (IFNULL(wa.cnt, 0)) EX_TIME\n           , IFNULL(wa.cnt, 0) WRONG_ANS\n      FROM ac\n           LEFT JOIN wa ON ac.ENTRY_ID = wa.ENTRY_ID\n           JOIN ENTRIES ent ON ent.ENTRY_ID = ac.ENTRY_ID) t"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第4回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第4回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第4回 問題4",
    "text": "第4回 問題4\n存在しない日付をうまく作る問題。\n連番を作るコード(SEQ としている部分) を持っているとなにかと便利。\nWITH SEQ AS (SELECT 0 AS no UNION ALL SELECT no + 1 AS no FROM SEQ WHERE no + 1 &lt; 31)\n   , t1 AS (SELECT no, DATE('2022-08-01', '+' || no || ' day') AS dt FROM seq)\nSELECT dt AS REGIST_DATE\n     , SUBSTR('月火水木金土日', (no % 7) + 1, 1) AS WK\n     , IFNULL(COUNT(us.USER_CODE), 0) AS TOTAL\nFROM t1\n     LEFT JOIN\n     USERS AS us ON DATE(us.CONFIRMED_AT) == t1.dt\n         AND us.VALID_FLG = '1'\nGROUP BY dt, no;"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第5回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第5回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第5回 問題4",
    "text": "第5回 問題4\n与えられた式に従って累積和を取る問題。\nWITH t1 AS (SELECT MONTHLY\n                 , NEW_MRR + mrr.EXPANSION_MRR - mrr.DOWNGRADE_MRR - mrr.CHURN_MRR AS x\n            FROM MRR_DATA mrr)\n   , month AS (SELECT DISTINCT monthly\n               FROM MRR_DATA\n               UNION ALL\n               SELECT DATE(MAX(monthly), '+1 month')\n               FROM MRR_DATA)\nSELECT m.MONTHLY YM, IFNULL(SUM(t1.x), 0) MRR\nFROM month m\n     LEFT JOIN t1 ON t1.MONTHLY &lt; m.MONTHLY\nGROUP BY m.MONTHLY\nORDER BY m.MONTHLY;"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第6回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第6回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第6回 問題4",
    "text": "第6回 問題4\nやるだけの問題。\nSELECT di.DISTRICT_CODE CODE\n     , DISTRICT_NAME NAME\n     , LATITUDE LAT\n     , LONGITUDE LON\nFROM LOCATION_TBL lo\n     JOIN DISTRICT AS di ON di.DISTRICT_CODE = lo.DISTRICT_CODE\nWHERE di.DISTRICT_CODE &lt;&gt; '1101'\nORDER BY (SELECT (LATITUDE - lo.LATITUDE) * (LATITUDE - lo.LATITUDE) + (LONGITUDE - lo.LONGITUDE) * (LONGITUDE - lo.LONGITUDE) FROM LOCATION_TBL WHERE DISTRICT_CODE = '1101')\n        DESC, 1"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第7回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第7回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第7回 問題4",
    "text": "第7回 問題4\nこれもやるだけの問題。\nROUND, CAST の使い方を学んだ。\nWITH t1 AS (SELECT ITEM_CODE, SUM(UNITPRICE * SALES_QTY) AS amt\n            FROM sales\n                 INNER JOIN SALES_DTL de ON de.sales_no = sales.SALES_NO\n            WHERE SALES_DATE BETWEEN '2023-06-01' AND '2023-06-30'\n            GROUP BY ITEM_CODE)\n   , t2 AS (SELECT t1.ITEM_CODE\n                 , SUM(t2.amt) v0\n                 , ROUND(CAST(MAX(t1.amt) AS real) / (SELECT SUM(amt) FROM t1) * 100, 1) v1\n                 , ROUND(CAST(IFNULL(SUM(t2.amt), 0) AS real) / (SELECT SUM(amt) FROM t1) * 100, 1) v2\n            FROM t1\n                 LEFT JOIN t1 t2 ON t2.amt &gt;= t1.amt\n            GROUP BY t1.ITEM_CODE)\nSELECT t1.ITEM_CODE CODE\n     , ITEM_NAME NAME\n     , t1.amt SAL_AMT\n     , t2.v0 CML_AMT\n     , t2.v1 || '%' SAL_COMP\n     , t2.v2 || '%' TTL_COMP\n     , CASE\n           WHEN t2.v2 &lt;= 40 THEN 'A'\n           WHEN t2.v2 &lt;= 80 THEN 'B'\n           ELSE 'C' END AS RANK\nFROM t1\n     JOIN t2 ON t1.ITEM_CODE = t2.ITEM_CODE\n     JOIN ITEM ON ITEM.ITEM_CODE = t2.ITEM_CODE\n\nORDER BY 3 DESC, 1 DESC"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第8回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第8回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第8回 問題4",
    "text": "第8回 問題4\n問題文に”設定”とあって何をするのかわかりにくいが、更新クエリを問われている。 副問合せを使った UPDATE 忘れていたので勉強になった。\nUPDATE item SET ITEM_POPULAR_RANK=0;\nWITH t1 AS (SELECT D.ITEM_CODE\n                 , MAX(ORDER_DATE) DT\n                 , SUM(ORDER_QTY) qty\n            FROM ORDERS O\n                 JOIN ORDERS_DTL D ON D.ORDER_NO = O.ORDER_NO\n            WHERE O.ORDER_DATE BETWEEN '2023-04-01' AND '2023-06-30'\n            GROUP BY D.ITEM_CODE)\n   , t2 AS (SELECT t1.*\n                 , RANK() OVER (ORDER BY qty DESC, dt DESC,ITEM_CODE DESC\n        ) rnk\n            FROM t1)\nUPDATE item\nSET ITEM_POPULAR_RANK = t2.rnk\nFROM t2\nWHERE t2.ITEM_CODE = item.ITEM_CODE;"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第9回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第9回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第9回 問題4",
    "text": "第9回 問題4\nNTILE を使える問題。\nどのように集計するか悩んだ。 NTILE とったテーブルを前もって集計しておく(以下の t3 )と比較的楽そう。\nWITH t1 AS (SELECT PF_CODE, TOTAL_VALUE FROM CONVENIENCE WHERE SURVEY_YEAR = 2019 AND KIND_CODE = '100')\n   , t2 AS (SELECT PF_CODE, TOTAL_VALUE te FROM CONVENIENCE WHERE SURVEY_YEAR = 2019 AND KIND_CODE = '150')\n   , t3 AS (SELECT no, SUM(sal) sal, SUM(te) te\n            FROM (SELECT t1.TOTAL_VALUE sal, t2.te, NTILE(10) OVER ( ORDER BY t1.TOTAL_VALUE DESC, t2.te, t1.PF_CODE) NO\n                  FROM t1\n                       LEFT JOIN t2 ON t1.PF_CODE = t2.PF_CODE) t\n            GROUP BY t.no)\n   , t4 AS (SELECT t3.no, SUM(tmp.sal) s\n            FROM t3\n                 LEFT JOIN t3 AS tmp ON t3.no &gt;= tmp.no\n            GROUP BY t3.no)\nSELECT t3.no NO\n     , t3.sal TTL_SAL\n     , ROUND(CAST(t3.sal AS real) / tot.val * 100, 1) PER_SAL\n     , ROUND(CAST(t4.s AS real) / tot.val * 100, 1) CUM_SAL\n     , FLOOR(CAST(t3.sal AS real) / t3.te) AVG_SAL\nFROM t3\n     LEFT JOIN t4 ON t3.no = t4.no\n   , (SELECT SUM(sal) val FROM t3) tot\nGROUP BY t3.no\nORDER BY t3.no\n;"
  },
  {
    "objectID": "posts/2024-06-17_sqlcontest-01-10.html#第10回-問題4",
    "href": "posts/2024-06-17_sqlcontest-01-10.html#第10回-問題4",
    "title": "SQLコンテストの問題 4 を解く(第 1〜10 回)",
    "section": "第10回 問題4",
    "text": "第10回 問題4\nいろいろやり方はありそうな問題。今回は UNION ALL で入れ子にして1つ前のステップのデータを確認するようにした。\n他には RANK を使って timestamp 順にランクをとり、STEP {i} と一致しているところまでをカウントするなどが考えられる。\nSTEP5 のデータがない場合でも STEP5 | 0 と表示できるように SEQ テーブルを使うことに注意。\nWITH t1 AS (SELECT lo.SESSION_ID, lo.PROCESS_ID, lo.EX_TIMESTAMP, MAX(lo2.EX_TIMESTAMP) pre_t\n            FROM PROCESS_LOG lo\n                 LEFT JOIN PROCESS_LOG lo2 ON lo.SESSION_ID = lo2.SESSION_ID AND lo.EX_TIMESTAMP &gt; lo2.EX_TIMESTAMP\n            GROUP BY lo.SESSION_ID, lo.PROCESS_ID, lo.EX_TIMESTAMP)\n   , t2 AS (SELECT t1.*, 1 step\n            FROM t1\n            WHERE PROCESS_ID = 'STEP1'\n              AND pre_t IS NULL\n            UNION ALL\n            SELECT t1.*, t2.step + 1\n            FROM t1\n                 JOIN t2 ON t1.SESSION_ID = t2.SESSION_ID AND t1.PROCESS_ID = 'STEP' || (t2.step + 1) AND t1.pre_t = t2.EX_TIMESTAMP\n            WHERE step &lt; 5)\n   , seq AS (SELECT 1 AS no UNION ALL SELECT no + 1 AS no FROM SEQ WHERE no + 1 &lt;= 5)\nSELECT 'STEP' || seq.no PROCESS, COUNT(session_id) CNT\nFROM seq\n     LEFT JOIN t2 ON 'STEP' || seq.no = t2.PROCESS_ID\nGROUP BY seq.no\nORDER BY 1"
  },
  {
    "objectID": "posts/2025-02-05_make_lag_data.html",
    "href": "posts/2025-02-05_make_lag_data.html",
    "title": "時系列データの lag, step の作成操作",
    "section": "",
    "text": "ML わからんので手始めに時系列データの予測をやってみようとしたところ、 lag, step の作成に手こずったのでメモを残す。\nDataFrame の MultiIndex がまだまだわからん。"
  },
  {
    "objectID": "posts/2025-02-05_make_lag_data.html#データ",
    "href": "posts/2025-02-05_make_lag_data.html#データ",
    "title": "時系列データの lag, step の作成操作",
    "section": "データ",
    "text": "データ\nkaggle の時系列の練習コンペ Store Sales - Time Series Forecasting のものを使用\nこのデータを使って multi step データを作成する過程を残す"
  },
  {
    "objectID": "posts/2025-02-05_make_lag_data.html#コード",
    "href": "posts/2025-02-05_make_lag_data.html#コード",
    "title": "時系列データの lag, step の作成操作",
    "section": "コード",
    "text": "コード\n\n# https://www.kaggle.com/code/ekrembayar/store-sales-ts-forecasting-a-comprehensive-guide より拝借\n# BASE\n# ------------------------------------------------------\nimport numpy as n\nimport pandas as pd\nfrom pathlib import Path\nimport os\nimport gc\nimport warnings\n\n# DATA VISUALIZATION\n# ------------------------------------------------------\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n\n# CONFIGURATIONS\n# ------------------------------------------------------\npd.set_option('display.max_columns', None)\npd.options.display.float_format = '{:.2f}'.format\nwarnings.filterwarnings('ignore')\n\n\nDATA_DIR=Path(\"../input/data/\")\n\n\n# Import\ntrain = pd.read_csv(DATA_DIR/\"train.csv\")\ntest = pd.read_csv(DATA_DIR/\"test.csv\")\nstores = pd.read_csv(DATA_DIR/\"stores.csv\")\n#sub = pd.read_csv(DATA_DIR/\"sample_submission.csv\")   \ntransactions = pd.read_csv(DATA_DIR/\"transactions.csv\").sort_values([\"store_nbr\", \"date\"])\n\n# Datetime\ntrain[\"date\"] = pd.to_datetime(train.date)\ntest[\"date\"] = pd.to_datetime(test.date)\ntransactions[\"date\"] = pd.to_datetime(transactions.date)\n\n# Data types\ntrain.onpromotion = train.onpromotion.astype(\"float16\")\ntrain.sales = train.sales.astype(\"float32\")\nstores.cluster = stores.cluster.astype(\"int8\")\n\ntrain\n\n\n\n\n\n\n\n\nid\ndate\nstore_nbr\nfamily\nsales\nonpromotion\n\n\n\n\n0\n0\n2013-01-01\n1\nAUTOMOTIVE\n0.00\n0.00\n\n\n1\n1\n2013-01-01\n1\nBABY CARE\n0.00\n0.00\n\n\n2\n2\n2013-01-01\n1\nBEAUTY\n0.00\n0.00\n\n\n3\n3\n2013-01-01\n1\nBEVERAGES\n0.00\n0.00\n\n\n4\n4\n2013-01-01\n1\nBOOKS\n0.00\n0.00\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3000883\n3000883\n2017-08-15\n9\nPOULTRY\n438.13\n0.00\n\n\n3000884\n3000884\n2017-08-15\n9\nPREPARED FOODS\n154.55\n1.00\n\n\n3000885\n3000885\n2017-08-15\n9\nPRODUCE\n2419.73\n148.00\n\n\n3000886\n3000886\n2017-08-15\n9\nSCHOOL AND OFFICE SUPPLIES\n121.00\n8.00\n\n\n3000887\n3000887\n2017-08-15\n9\nSEAFOOD\n16.00\n0.00\n\n\n\n\n3000888 rows × 6 columns\n\n\n\n\nデータ準備\n\ntrain.sort_values(['date','family','store_nbr'],inplace=True)\ntrain.reset_index(drop=True,inplace=True)\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train.family)\n\nLabelEncoder()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LabelEncoder?Documentation for LabelEncoderiFittedLabelEncoder() \n\n\n\ntrain[\"family\"]=le.transform(train.family)\n\n\n# デバッグ用なのでデータを絞る\ntrain=train[train['store_nbr'].isin([1,2]) & train['family'].isin([1,2])]\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nid\ndate\nstore_nbr\nfamily\nsales\nonpromotion\n\n\n\n\n54\n1\n2013-01-01\n1\n1\n0.00\n0.00\n\n\n55\n364\n2013-01-01\n2\n1\n0.00\n0.00\n\n\n108\n2\n2013-01-01\n1\n2\n0.00\n0.00\n\n\n109\n365\n2013-01-01\n2\n2\n0.00\n0.00\n\n\n1836\n1783\n2013-01-02\n1\n1\n0.00\n0.00\n\n\n\n\n\n\n\n\nKEY=[\"date\",\"store_nbr\",\"family\"]\ntrain_step=train[KEY+[\"sales\"]]\n\n\n# https://github.com/Kaggle/learntools/blob/master/learntools/time_series/utils.py より\ndef make_multistep_target(ts, steps, reverse=False):\n\n    shifts = reversed(range(steps)) if reverse else range(steps)\n    return pd.concat({f'y_step_{i + 1}': ts.shift(-i) for i in shifts}, axis=1)\n\n\n# HACK: unstack せずに1発で columns を multiIndex にできるか？\ntrain_step=train_step.set_index(['family', 'store_nbr','date', ])\ntrain_step=train_step.unstack(['family', 'store_nbr'])\n\n下のように軸{family, store_nbr} ごとに sales が時系列順で縦に並ぶようにする。\nこれを shift することで各軸ごとの lag や multistep_target を作成できる。\n\ntrain_step.head()\n\n\n\n\n\n\n\n\nsales\n\n\nfamily\n1\n2\n\n\nstore_nbr\n1\n2\n1\n2\n\n\ndate\n\n\n\n\n\n\n\n\n2013-01-01\n0.00\n0.00\n0.00\n0.00\n\n\n2013-01-02\n0.00\n0.00\n2.00\n3.00\n\n\n2013-01-03\n0.00\n0.00\n0.00\n2.00\n\n\n2013-01-04\n0.00\n0.00\n3.00\n3.00\n\n\n2013-01-05\n0.00\n0.00\n3.00\n9.00\n\n\n\n\n\n\n\n\ntrain_step=make_multistep_target(train_step[\"sales\"], 3)\n\n\ntrain_step=train_step.stack(['family', 'store_nbr']) # family, store_nbr を index に戻す\n\n\ntrain_step=train_step.dropna()\n\n\ntrain_step.head()\n\n\n\n\n\n\n\n\n\n\ny_step_1\ny_step_2\ny_step_3\n\n\ndate\nfamily\nstore_nbr\n\n\n\n\n\n\n\n2013-01-01\n1\n1\n0.00\n0.00\n0.00\n\n\n2\n0.00\n0.00\n0.00\n\n\n2\n1\n0.00\n2.00\n0.00\n\n\n2\n0.00\n3.00\n2.00\n\n\n2013-01-02\n1\n1\n0.00\n0.00\n0.00\n\n\n\n\n\n\n\n\nFEATURES=[\"onpromotion\",]\n\ntrain_feature_df=train[KEY+FEATURES]\n\n\ntrain_feature_df=train_feature_df.set_index(['family', 'store_nbr','date', ])\n\n\ntrain_with_step_df=train_feature_df.join(train_step, how='inner')\n\n\nX=train_with_step_df[FEATURES]\ny=train_with_step_df[[c for c in train_with_step_df.columns if c.startswith(\"y_step\")]]\n\n\nX\n\n\n\n\n\n\n\n\n\n\nonpromotion\n\n\nfamily\nstore_nbr\ndate\n\n\n\n\n\n1\n1\n2013-01-01\n0.00\n\n\n2\n2013-01-01\n0.00\n\n\n2\n1\n2013-01-01\n0.00\n\n\n2\n2013-01-01\n0.00\n\n\n1\n1\n2013-01-02\n0.00\n\n\n...\n...\n...\n...\n\n\n2\n2\n2017-08-12\n1.00\n\n\n1\n1\n2017-08-13\n0.00\n\n\n2\n2017-08-13\n0.00\n\n\n2\n1\n2017-08-13\n0.00\n\n\n2\n2017-08-13\n1.00\n\n\n\n\n6728 rows × 1 columns\n\n\n\n\ny\n\n\n\n\n\n\n\n\n\n\ny_step_1\ny_step_2\ny_step_3\n\n\nfamily\nstore_nbr\ndate\n\n\n\n\n\n\n\n1\n1\n2013-01-01\n0.00\n0.00\n0.00\n\n\n2\n2013-01-01\n0.00\n0.00\n0.00\n\n\n2\n1\n2013-01-01\n0.00\n2.00\n0.00\n\n\n2\n2013-01-01\n0.00\n3.00\n2.00\n\n\n1\n1\n2013-01-02\n0.00\n0.00\n0.00\n\n\n...\n...\n...\n...\n...\n...\n\n\n2\n2\n2017-08-12\n7.00\n10.00\n7.00\n\n\n1\n1\n2017-08-13\n0.00\n0.00\n0.00\n\n\n2\n2017-08-13\n0.00\n0.00\n0.00\n\n\n2\n1\n2017-08-13\n1.00\n6.00\n4.00\n\n\n2\n2017-08-13\n10.00\n7.00\n9.00\n\n\n\n\n6728 rows × 3 columns\n\n\n\n\n\nモデルの学習\n上の情報で学習してみる\n\nfrom sklearn.multioutput import RegressorChain\nfrom sklearn.linear_model import LinearRegression\n# from xgboost import XGBRegressor\n\nmodel = RegressorChain(base_estimator=LinearRegression())\n\n\nmodel.fit(X, y)\n\nRegressorChain(base_estimator=LinearRegression())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RegressorChain?Documentation for RegressorChainiFittedRegressorChain(base_estimator=LinearRegression()) base_estimator: LinearRegressionLinearRegression() LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\npred=model.predict(X)\n\n\npred.shape\n\n(6728, 3)"
  },
  {
    "objectID": "posts/2025-03-25-image-rot/image_rot.html",
    "href": "posts/2025-03-25-image-rot/image_rot.html",
    "title": "Python でスキャン画像の回転補正をやってみる",
    "section": "",
    "text": "画像の回転補正をやってみる。\n\n準備\n安全運転BOOKの目次のスキャン画像を使います。\n(『安全運転BOOK』伊藤印刷株式会社)\n\nfrom pathlib import Path\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\n\nimg = cv2.imread(\"sample.jpg\", cv2.IMREAD_GRAYSCALE)\nplt.imshow(img, cmap='gray')\n\n\n\n\n\n\n\n\n\ndef rotate_image(img: np.ndarray, d: float, border_value: int = 1, resize=False):\n    \"\"\"\n    画像を d [度] 回転します。\n\n    Args:\n        img:\n        d: 回転角度\n        border_value: 余白を埋める値\n        resize: True のとき、回転後の画像が欠けないようにサイズ調整をします\n                False のときはもとのサイズと同じ画像が返ります\n    \"\"\"\n    height, width = img.shape[:2]\n    new_width, new_height = width, height\n    center = (width // 2, height // 2)\n    M = cv2.getRotationMatrix2D(center, d, 1)\n\n    if resize:\n        cos = np.abs(M[0, 0])\n        sin = np.abs(M[0, 1])\n        # 回転後のすべての領域が含まれるサイズを計算\n        new_width = int(width * cos + height * sin)\n        new_height = int(width * sin + height * cos)\n        # 中心ずらし\n        M[0, 2] += (new_width - width) / 2.0\n        M[1, 2] += (new_height - height) / 2.0\n\n    affine_img = cv2.warpAffine(img,\n                                M,\n                                (new_width, new_height),\n                                borderMode=cv2.BORDER_CONSTANT,\n                                borderValue=border_value)\n    return affine_img\n\n傾けた画像を用意する\n\nimg = rotate_image(img,5,255)\nplt.imshow(img, cmap='gray')\n\n\n\n\n\n\n\n\n\n\n回転補正\n以下のサイトの射影ヒストグラムをもとに最も良い「正面度」となる角度を探索し、画像を回転させる。\n射影ヒストグラムを用いた文書画像の回転補正 - 社会人研究者が色々頑張るブログ\n\ndef projection_histogram(bimg):\n    \"\"\"\n    射影ヒストグラムの作成\n    https://nsr-9.hatenablog.jp/entry/2021/08/09/200559\n    \"\"\"\n    bimg = np.ones_like(bimg) - bimg\n    out = np.sum(bimg, axis=1)\n    return out\n\n\ndef to_binary_adavtive_threshold(img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"2値化処理 {0, 1} に変換\"\"\"\n    \n    bimg = cv2.adaptiveThreshold(\n        img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 7, 50\n    )\n    bimg[bimg == 255] = 1\n    return bimg\n\n\nbimg=to_binary_adavtive_threshold(img)\nhist = projection_histogram(bimg)\n\nヒストグラムの確認\n\nfig = plt.figure(figsize=(10, 6))\ngs = fig.add_gridspec(1, 2, width_ratios=[1, 1])\n\n# histgram\nax0 = fig.add_subplot(gs[0])\nval = hist[::-1]\nx = range(len(val))\nax0.barh(x, val)\nax0.set_ylim(0, len(val))\n\n# image\nax1 = fig.add_subplot(gs[1])\nax1.imshow(bimg, cmap=\"gray\")\nax1.axis(\"off\")\nplt.subplots_adjust(wspace=0, left=0, right=1, top=1, bottom=0)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef calc_cost(img:np.ndarray,d:float):\n    img=rotate_image(img,d,border_value=1,resize=True)\n    hist=projection_histogram(img)\n    hist=np.where(hist&lt;=1,0,1)\n    return np.sum(hist)\n\ncalc_cost(bimg,0.)\n\n1249\n\n\n角度を変えたときのコストの変化を確認する\n\nx=np.linspace(-8., 8, num=80)\nvals=[calc_cost(bimg,d) for d in x]\n\n\nmin_idx=0\nfor i in range(len(vals)):\n    if vals[i]&lt;vals[min_idx]:\n        min_idx=i\n\n\nprint(\"コスト最小値となる角度:\",x[min_idx])\n\nコスト最小値となる角度: -4.962025316455696\n\n\n\nplt.xlabel('angle')\nplt.ylabel('cost')\nplt.plot(x, vals, marker='o')  \nplt.show()\n\n\n\n\n\n\n\n\n探索した良い正面度の角度に補正する\n\ntmp=rotate_image(img,x[min_idx],border_value=255)\nfig = plt.figure(figsize=(10, 6))\nplt.imshow(tmp, cmap='gray')\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(10, 6))\ngs = fig.add_gridspec(1, 2, width_ratios=[1, 1])\n\nax0=fig.add_subplot(gs[0])\nax0.imshow(img, cmap='gray')\nax0.axis('off')\n\nax1=fig.add_subplot(gs[1])\nax1.imshow(tmp, cmap='gray')\nax1.axis('off')\nplt.subplots_adjust(wspace=0, left=0, right=1, top=1, bottom=0)\n\n# 縦線を追加\nax1.axvline(x=0, color='black', linewidth=2)   \nplt.show()\n\n\n\n\n\n\n\n\n\n\nおまけ\nぱっとわからなかった回転後の画像サイズの求め方\n\n\n\n\n\nCopyrightCopyright tamuraup. 2024. All Rights Reserved"
  },
  {
    "objectID": "posts/2025-06-30_toke-nyumon-27-02.html",
    "href": "posts/2025-06-30_toke-nyumon-27-02.html",
    "title": "統計入門27. 時系列データ分析-2",
    "section": "",
    "text": "Note\n\n\n\n本レポートは、OpenAI の対話型 AI ChatGPT により生成・整理されたものです。"
  },
  {
    "objectID": "posts/2025-06-30_toke-nyumon-27-02.html#スペクトル密度関数とペリオドグラム",
    "href": "posts/2025-06-30_toke-nyumon-27-02.html#スペクトル密度関数とペリオドグラム",
    "title": "統計入門27. 時系列データ分析-2",
    "section": "1. スペクトル密度関数とペリオドグラム",
    "text": "1. スペクトル密度関数とペリオドグラム\n\nスペクトル密度関数は、時系列データの各周波数成分の強さ（エネルギー）を表す関数。定常な過程に対して定義される。\n角周波数は通常 \\(\\omega \\in [-\\pi, \\pi]\\) で表す。\nペリオドグラムは、スペクトル密度の推定量の一つ：\n\n観測データのフーリエ変換の絶対値平方で定義。\n一致推定量ではない（分散が消えず収束しない）ため、そのままでは不安定。\n対応策として Welch 法、AR モデル推定、カーネル平滑などがある。"
  },
  {
    "objectID": "posts/2025-06-30_toke-nyumon-27-02.html#状態空間モデルの構造",
    "href": "posts/2025-06-30_toke-nyumon-27-02.html#状態空間モデルの構造",
    "title": "統計入門27. 時系列データ分析-2",
    "section": "2. 状態空間モデルの構造",
    "text": "2. 状態空間モデルの構造\n\n状態空間モデル（State Space Model）は観測されない状態変数 \\(x_t\\) を導入し、観測データ \\(y_t\\) をその関数として記述する。\n2 本の方程式で構成される：\n\n\n状態方程式（state equation）\n\\[\nx_t = F x_{t-1} + w_t, \\quad w_t \\sim \\mathcal{N}(0, Q)\n\\]\n\n\n観測方程式（observation equation）\n\\[\ny_t = H x_t + v_t, \\quad v_t \\sim \\mathcal{N}(0, R)\n\\]\n\n\\(x_t\\) は観測されないが、時系列的に推移する。\n\\(y_t\\) は \\(x_t\\) を通じて得られる観測量。"
  },
  {
    "objectID": "posts/2025-06-30_toke-nyumon-27-02.html#カルマンフィルタの仕組み",
    "href": "posts/2025-06-30_toke-nyumon-27-02.html#カルマンフィルタの仕組み",
    "title": "統計入門27. 時系列データ分析-2",
    "section": "3. カルマンフィルタの仕組み",
    "text": "3. カルマンフィルタの仕組み\n\nカルマンフィルタは、状態空間モデルの中で隠れた状態 \\(x_t\\) を逐次的に推定するアルゴリズム。\n処理は 2 段階：\n\n\n予測（predict）\n\\[\n\\hat{x}_{t|t-1} = F \\hat{x}_{t-1|t-1}, \\quad\nP_{t|t-1} = F P_{t-1|t-1} F^\\top + Q\n\\]\n\n\n更新（update）\n\\[\nK_t = P_{t|t-1} H^\\top (H P_{t|t-1} H^\\top + R)^{-1}\n\\]\n\\[\n\\hat{x}_{t|t} = \\hat{x}_{t|t-1} + K_t (y_t - H \\hat{x}_{t|t-1})\n\\]\n\\[\nP_{t|t} = (I - K_t H) P_{t|t-1}\n\\]\n\n状態 \\(x_t\\) は常に観測されないため、予測 → 観測値で補正、という手順を踏む。\nこのモデルの実用的な目的の一つは、未来の観測値 \\(y_{t+1}\\) の予測。"
  },
  {
    "objectID": "posts/2025-06-30_toke-nyumon-27-02.html#ダービーワトソン検定",
    "href": "posts/2025-06-30_toke-nyumon-27-02.html#ダービーワトソン検定",
    "title": "統計入門27. 時系列データ分析-2",
    "section": "4. ダービー・ワトソン検定",
    "text": "4. ダービー・ワトソン検定\n\n回帰分析の残差に自己相関（特に 1 次）があるかどうかを調べる検定。\n検定統計量：\n\n\\[\nDW = \\frac{\\sum_{t=2}^n (e_t - e_{t-1})^2}{\\sum_{t=1}^n e_t^2}\n\\]\n\n解釈：\n\n\\(DW \\approx 2\\)：自己相関なし\n\\(DW &lt; 2\\)：正の自己相関\n\\(DW &gt; 2\\)：負の自己相関\n\n回帰モデルの誤差項がホワイトノイズ（独立同分布）かどうかをチェックする一手段。"
  },
  {
    "objectID": "posts/2025-06-30_toke-nyumon-27-02.html#aic赤池情報量規準",
    "href": "posts/2025-06-30_toke-nyumon-27-02.html#aic赤池情報量規準",
    "title": "統計入門27. 時系列データ分析-2",
    "section": "5. AIC（赤池情報量規準）",
    "text": "5. AIC（赤池情報量規準）\n\n式\n\\[\n\\mathrm{AIC} = -2 \\log L + 2k\n\\]\n\n\\(L\\)：最尤推定による最大尤度\n\\(k\\)：推定パラメータ数\n\n\n\n意味\n\nモデルの「当てはまりの良さ（尤度）」と「複雑さ（自由度）」のバランスをとる指標。\nAIC が小さいほど良いモデル（汎化性能が高いとみなされる）。"
  },
  {
    "objectID": "posts/2025-06-30_toke-nyumon-27-02.html#chatgpt-による振り返りと感想",
    "href": "posts/2025-06-30_toke-nyumon-27-02.html#chatgpt-による振り返りと感想",
    "title": "統計入門27. 時系列データ分析-2",
    "section": "💬 ChatGPT による振り返りと感想",
    "text": "💬 ChatGPT による振り返りと感想\n今回の対話では、時系列解析の基礎的なトピック（スペクトル密度・ペリオドグラム）から、状態空間モデル・カルマンフィルタの構造、そしてモデル選択指標である AIC やダービー・ワトソン検定まで、理論の核となる内容を段階的に丁寧に掘り下げていただきました。"
  },
  {
    "objectID": "posts/2025-07-03_fisher_kentei.html",
    "href": "posts/2025-07-03_fisher_kentei.html",
    "title": "統計入門28. フィッシャーの正確検定と超幾何分布の理解",
    "section": "",
    "text": "Note\n\n\n\nこのレポートは OpenAI の ChatGPT によって自動生成されました。"
  },
  {
    "objectID": "posts/2025-07-03_fisher_kentei.html#概要",
    "href": "posts/2025-07-03_fisher_kentei.html#概要",
    "title": "統計入門28. フィッシャーの正確検定と超幾何分布の理解",
    "section": "概要",
    "text": "概要\nこのドキュメントでは、フィッシャーの正確検定（Fisher’s exact test）について、その数式的背景、超幾何分布との関係、およびオッズ比の片側検定への応用までを丁寧に解説しています。特に小標本におけるカテゴリカルデータの解析に適しており、正確な \\(p\\) 値を求められる点が特徴です。"
  },
  {
    "objectID": "posts/2025-07-03_fisher_kentei.html#フィッシャーの正確検定とは",
    "href": "posts/2025-07-03_fisher_kentei.html#フィッシャーの正確検定とは",
    "title": "統計入門28. フィッシャーの正確検定と超幾何分布の理解",
    "section": "フィッシャーの正確検定とは",
    "text": "フィッシャーの正確検定とは\nフィッシャーの正確検定は、2×2 のクロス表において、行と列の間に有意な関係があるかを評価するための検定方法です。サンプル数が小さい場合でも正確な結果が得られるため、カイ二乗検定の代替手段として利用されます。\n\n2×2 表の一般形\n\\[\n\\begin{array}{c|cc|c}\n& \\text{列1} & \\text{列2} & \\text{行合計} \\\\\n\\hline\n\\text{行1} & a & b & a + b \\\\\n\\text{行2} & c & d & c + d \\\\\n\\hline\n\\text{列合計} & a + c & b + d & n = a + b + c + d\n\\end{array}\n\\]"
  },
  {
    "objectID": "posts/2025-07-03_fisher_kentei.html#数式の成り立ち",
    "href": "posts/2025-07-03_fisher_kentei.html#数式の成り立ち",
    "title": "統計入門28. フィッシャーの正確検定と超幾何分布の理解",
    "section": "数式の成り立ち",
    "text": "数式の成り立ち\nフィッシャーの正確検定で用いる確率は、周辺度数（行と列の合計）を固定した上で、この表が観測される確率を超幾何分布に基づいて計算します。\n\n確率の計算式\n\\[\nP(a) = \\frac{(a + b)! (c + d)! (a + c)! (b + d)!}{a! b! c! d! n!}\n\\]\nまたは、超幾何分布としての形：\n\\[\nP(a) = \\frac{\n\\binom{a + c}{a} \\binom{b + d}{b}\n}{\n\\binom{n}{a + b}\n}\n\\]\nこれは、「改善者があらかじめ \\(K = a + c\\) 人いて、無作為に投薬群（サンプル） \\(n = a + b\\) 人を選んだとき、その中にちょうど \\(a\\) 人の改善者が含まれる確率」を求めている形です。"
  },
  {
    "objectID": "posts/2025-07-03_fisher_kentei.html#超幾何分布との対応",
    "href": "posts/2025-07-03_fisher_kentei.html#超幾何分布との対応",
    "title": "統計入門28. フィッシャーの正確検定と超幾何分布の理解",
    "section": "超幾何分布との対応",
    "text": "超幾何分布との対応\nフィッシャーの正確検定は、以下の超幾何分布のパラメータ設定と一致します：\n\n\n\n超幾何分布の記号\n解釈\n\n\n\n\n\\(N\\)\n全体の人数 \\(= n\\)\n\n\n\\(K\\)\n成功（改善者）の数 \\(= a + c\\)\n\n\n\\(n\\)\nサンプル（投薬群）に割り当てられた人数 \\(= a + b\\)\n\n\n\\(k\\)\n成功かつサンプル内の人数 \\(= a\\)\n\n\n\n帰無仮説 \\(H_0\\) のもとでは、改善と投薬は無関係なので、改善者は投薬群・非投薬群に無作為に割り当てられる。よって、投薬群に含まれる改善者数は超幾何分布に従います。"
  },
  {
    "objectID": "posts/2025-07-03_fisher_kentei.html#オッズ比の片側検定への応用",
    "href": "posts/2025-07-03_fisher_kentei.html#オッズ比の片側検定への応用",
    "title": "統計入門28. フィッシャーの正確検定と超幾何分布の理解",
    "section": "オッズ比の片側検定への応用",
    "text": "オッズ比の片側検定への応用\nフィッシャーの正確検定は、オッズ比に関する片側検定としても使用可能です。\n\n帰無仮説：オッズ比 \\(\\theta = 1\\)\n対立仮説（右側検定）：\\(\\theta &gt; 1\\)（薬の効果がある）\n観測された改善者数 \\(a\\) に対し、それ以上に「オッズ比が大きくなるような」表の確率をすべて合計して片側 \\(p\\) 値を算出"
  },
  {
    "objectID": "posts/2025-07-03_fisher_kentei.html#手計算にやさしい例題",
    "href": "posts/2025-07-03_fisher_kentei.html#手計算にやさしい例題",
    "title": "統計入門28. フィッシャーの正確検定と超幾何分布の理解",
    "section": "手計算にやさしい例題",
    "text": "手計算にやさしい例題\n以下の簡単な 2×2 表を用いて、手計算で片側 \\(p\\) 値を求めました。\n\n\n\n\n改善した\n改善しなかった\n合計\n\n\n\n\n新薬群（投薬あり）\n3\n1\n4\n\n\nプラセボ群\n1\n3\n4\n\n\n合計\n4\n4\n8\n\n\n\nこのとき、観測値 \\(a = 3\\) に対し：\n\\[\nP(a = 3) = \\frac{\\binom{4}{3} \\binom{4}{1}}{\\binom{8}{4}} = \\frac{16}{70} \\approx 0.2286\n\\]\n\\[\nP(a = 4) = \\frac{\\binom{4}{4} \\binom{4}{0}}{\\binom{8}{4}} = \\frac{1}{70} \\approx 0.0143\n\\]\n\\[\nP(a \\geq 3) = \\frac{17}{70} \\approx 0.243\n\\]\nよって、片側検定（右側）の \\(p\\) 値は約 0.243。これは有意水準 5% では有意ではないと判断されます。"
  },
  {
    "objectID": "posts/2024-08-16_gauss_sec3_param.html",
    "href": "posts/2024-08-16_gauss_sec3_param.html",
    "title": "ガウス過程と機械学習 3章 ガウス過程回帰のパラメータ推定",
    "section": "",
    "text": "前回 からの続き\nテキスト 3.5 のパラメータ推定をやってみる。\ntrainデータを使って対数尤度が最大になるような \\(\\theta\\) を探索する。\n対数尤度の式は (3.92) の \\(-\\log |K_\\theta| - y^T K^{-1}_\\theta y\\) 。\n前回同様テストデータなどは以下の記事から借りている。\n『ガウス過程と機械学習』Pythonのnumpyだけで実装するガウス過程回帰 #機械学習プロフェッショナルシリーズ - Qiita\n# テストデータ生成\n\nimport numpy as np\nnp.random.seed(seed=9973)\n\n\n# 元データの作成\nn=100\ndata_x = np.linspace(0, 4*np.pi, n)\ndata_y = 2*np.sin(data_x) + 3*np.cos(2*data_x) + 5*np.sin(2/3*data_x) + np.random.randn(len(data_x))\n\n# 信号を欠損させて部分的なサンプル点を得る\nmissing_value_rate = 0.15\nsample_index = np.sort(np.random.choice(np.arange(n), int(n*missing_value_rate), replace=False))\n\n# データの定義\nxtrain = np.copy(data_x[sample_index])\nytrain = np.copy(data_y[sample_index])\n\nxtest = np.copy(data_x)\n# カーネル\n\nimport numpy.matlib\ndef kgauss(X,theta_1,theta_2,theta_3):\n    \"\"\"\n    RBF kernel\n    図 3.18 の実装\n    \"\"\"\n    N=len(X)\n    X=X.reshape(N,-1)\n    X=X.T # D * N 次元に\n    \n    z=(X**2).sum(axis=0).reshape(1,N)\n    K = numpy.matlib.repmat(z.T,1,N)+ numpy.matlib.repmat(z,N,1) -np.dot(X.T,X)*2.\n    return theta_1*np.exp(-K/theta_2)+theta_3*np.identity(N)"
  },
  {
    "objectID": "posts/2024-08-16_gauss_sec3_param.html#optuna-を使って-theta-を探索する",
    "href": "posts/2024-08-16_gauss_sec3_param.html#optuna-を使って-theta-を探索する",
    "title": "ガウス過程と機械学習 3章 ガウス過程回帰のパラメータ推定",
    "section": "optuna を使って \\(\\theta\\) を探索する",
    "text": "optuna を使って \\(\\theta\\) を探索する\n\ndef _objective(theta_1,theta_2,theta_3):\n    \"\"\" 対数尤度関数 \"\"\"\n    y=ytrain\n    x=xtrain\n    K=kgauss(xtrain,theta_1,theta_2,theta_3)\n    det_K=np.linalg.det(K)\n    inv_K=np.linalg.inv(K)\n    return -np.log(det_K)-np.dot(y.T,np.dot(inv_K,y))\n\n\n# optuna で探索\n\nimport optuna\noptuna.logging.disable_default_handler()\n \ndef objective(trial):\n    theta_1 = trial.suggest_float(\"theta_1\", 1e-10, 30)\n    theta_2 = trial.suggest_float(\"theta_2\", 1e-10, 30)\n    theta_3 = trial.suggest_float(\"theta_3\", 1e-10, 30)\n    return _objective(theta_1,theta_2,theta_3)\n\n \nstudy = optuna.create_study(direction=\"maximize\")\n\nstudy.optimize(objective, n_trials=300)\nprint(study.best_trial)\n\nFrozenTrial(number=185, state=1, values=[-39.02225822902864], datetime_start=datetime.datetime(2024, 8, 15, 20, 26, 10, 299349), datetime_complete=datetime.datetime(2024, 8, 15, 20, 26, 10, 312097), params={'theta_1': 15.162762907400607, 'theta_2': 1.9226918374007205, 'theta_3': 0.463028699778022}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'theta_1': FloatDistribution(high=30.0, log=False, low=1e-10, step=None), 'theta_2': FloatDistribution(high=30.0, log=False, low=1e-10, step=None), 'theta_3': FloatDistribution(high=30.0, log=False, low=1e-10, step=None)}, trial_id=185, value=None)\n\n\n\nprint(study.best_params)\n\n{'theta_1': 15.162762907400607, 'theta_2': 1.9226918374007205, 'theta_3': 0.463028699778022}"
  },
  {
    "objectID": "posts/2024-08-16_gauss_sec3_param.html#plot",
    "href": "posts/2024-08-16_gauss_sec3_param.html#plot",
    "title": "ガウス過程と機械学習 3章 ガウス過程回帰のパラメータ推定",
    "section": "plot",
    "text": "plot\n推定した \\(\\theta\\) を使って描画する。\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\ndef visualize(data_x,data_y,xtest,sample_index,mu,var,title):\n    plt.figure(figsize=(12, 5))\n    plt.title(title, fontsize=20)\n    \n    # 元の信号\n    plt.plot(data_x, data_y, 'x', color='green', label='correct signal')\n    # 部分的なサンプル点\n    plt.plot(data_x[sample_index], data_y[sample_index], 'o', color='red', label='sample dots')\n    \n    # 分散を標準偏差に変換\n    std = np.sqrt(var)\n    \n    # ガウス過程で求めた平均値を信号化\n    plt.plot(xtest, mu, color='blue', label='mean by Gaussian process')\n    # 分散\n    plt.fill_between(xtest, mu + 2*std, mu - 2*std, alpha=.2, color='blue', label= 'standard deviation by Gaussian process')\n    \n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0, fontsize=12)\n    plt.show()\n\n\ndef predict(xtrain,ytrain,xtest,theta_1,theta_2,theta_3):\n    \"\"\"\n    ガウス過程回帰\n    \"\"\"\n    ytrain=ytrain.copy()\n\n    N = len(xtrain)\n    M = len(xtest)\n    \n    # まとめてカーネル計算\n    xtrain=xtrain.reshape(N,-1)\n    xtest=xtest.reshape(M,-1)\n\n    # 図3.15 のとおり行列を分割\n    X=np.vstack([xtrain, xtest])\n    X = kgauss(X,theta_1,theta_2,theta_3)\n    \n    u, b=np.split(X,[N],0)\n    K, k_=np.split(u,[N],1)\n    k_t, k__=np.split(b,[N],1)\n    \n    assert k__.shape==(M,M)\n\n    inv_K=np.linalg.inv(K)\n    yy = np.dot(inv_K, ytrain)\n    \n    mu=np.dot(k_t,yy)\n    var=k__-np.dot(np.dot(k_t,inv_K),k_)\n    var=np.diag(var) # 対角成分の取得\n    return mu, var\n\n\nmu,var=predict(xtrain,ytrain,xtest,**study.best_params)\n\n\nvisualize(data_x,data_y,xtest,sample_index,mu,var,'推定したパラメータで回帰')\n\n\n\n\n\n\n\n\nテキストで使用されているパラメータを使って描画して比較してみる。\n\nmu,var=predict(xtrain,ytrain,xtest,1.0,0.4,0.1)\nvisualize(data_x,data_y,xtest,sample_index,mu,var,'テキストと同じパラメータの値で回帰')"
  },
  {
    "objectID": "posts/2024-08-14_gauss_sec3.html",
    "href": "posts/2024-08-14_gauss_sec3.html",
    "title": "ガウス過程と機械学習 3章 ガウス過程回帰",
    "section": "",
    "text": "『ガウス過程と機械学習』 (講談社) の3 章を読んだので、ガウス過程回帰の部分を実際にコードにしてみる。"
  },
  {
    "objectID": "posts/2024-08-14_gauss_sec3.html#図3.17-の基本アルゴリズムの実装",
    "href": "posts/2024-08-14_gauss_sec3.html#図3.17-の基本アルゴリズムの実装",
    "title": "ガウス過程と機械学習 3章 ガウス過程回帰",
    "section": "図3.17 の基本アルゴリズムの実装",
    "text": "図3.17 の基本アルゴリズムの実装\n以下の Qiita 記事の実装を借りて、一部変更した。\n『ガウス過程と機械学習』Pythonのnumpyだけで実装するガウス過程回帰 #機械学習プロフェッショナルシリーズ - Qiita\n注意)この記事のソースコードに「内積はドットで計算」とコメントされている部分があるが、内積ではなく行列積。\n\nimport numpy as np\nnp.random.seed(seed=9973)\n\n# 元データの作成\nn=100\ndata_x = np.linspace(0, 4*np.pi, n)\ndata_y = 2*np.sin(data_x) + 3*np.cos(2*data_x) + 5*np.sin(2/3*data_x) + np.random.randn(len(data_x))\n\n# 信号を欠損させて部分的なサンプル点を得る\nmissing_value_rate = 0.15\nsample_index = np.sort(np.random.choice(np.arange(n), int(n*missing_value_rate), replace=False))\n\n# データの定義\nxtrain = np.copy(data_x[sample_index])\nytrain = np.copy(data_y[sample_index])\n\nxtest = np.copy(data_x)\n\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize=(12, 5))\nplt.title('signal data', fontsize=20)\n\n# 元の信号\nplt.plot(data_x, data_y, 'x', color='green', label='correct signal')\n\n# 部分的なサンプル点\nplt.plot(data_x[sample_index], data_y[sample_index], 'o', color='red', label='sample dots')\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0, fontsize=12)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef RBF(x,x2,theta1,theta2,theta3,i_eq_j=False):\n    \"\"\"RBF kernel\"\"\"\n    if i_eq_j:\n        d=theta3\n    else:\n        d=0.\n    return theta1 * np.exp(-(x-x2)**2/theta2)+d\n    \n\n\ndef calc(xtrain,ytrain,xtest,kernel=RBF):\n    \"\"\"ガウス過程回帰の計算 図 3.17\"\"\"\n    # 平均\n    mu = []\n    # 分散\n    var = []\n\n    # 各パラメータ値\n    theta_1 = 16.\n    theta_2 = 2.\n    theta_3 = 0.34\n\n    N = len(xtrain)\n    K = np.zeros((N, N))\n    \n    for x in range(N):\n        for x_prime in range(N):\n            K[x, x_prime] = kernel(xtrain[x], xtrain[x_prime], theta_1, theta_2, theta_3,x==x_prime)\n            \n    inv_K=np.linalg.inv(K)\n    yy = np.dot(inv_K, ytrain)\n\n    M = len(xtest)\n    for test_i in range(M):\n        k = np.zeros((N,))\n        for x in range(N):\n            k[x] = kernel(xtrain[x], xtest[test_i], theta_1, theta_2, theta_3)\n\n        s = kernel(xtest[test_i], xtest[test_i], theta_1, theta_2, theta_3)\n        mu.append(np.dot(k, yy))\n        kK_ = np.dot(k, inv_K)\n        var.append(s - np.dot(kK_, k.T))\n    return mu,var\n\n\nmu,var=calc(xtrain,ytrain,xtest)\n\n\ndef visualize(data_x,data_y,xtest,sample_index,mu,var):\n    plt.figure(figsize=(12, 5))\n    plt.title('signal prediction by Gaussian process', fontsize=20)\n    \n    # 元の信号\n    plt.plot(data_x, data_y, 'x', color='green', label='correct signal')\n    # 部分的なサンプル点\n    plt.plot(data_x[sample_index], data_y[sample_index], 'o', color='red', label='sample dots')\n    \n    # 分散を標準偏差に変換\n    std = np.sqrt(var)\n    \n    # ガウス過程で求めた平均値を信号化\n    plt.plot(xtest, mu, color='blue', label='mean by Gaussian process')\n    # 分散\n    plt.fill_between(xtest, mu + 2*std, mu - 2*std, alpha=.2, color='blue', label= 'standard deviation by Gaussian process')\n    \n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0, fontsize=12)\n    plt.show()\n\n\nvisualize(data_x,data_y,xtest,sample_index,mu,var)"
  },
  {
    "objectID": "posts/2024-08-14_gauss_sec3.html#行列演算に変更",
    "href": "posts/2024-08-14_gauss_sec3.html#行列演算に変更",
    "title": "ガウス過程と機械学習 3章 ガウス過程回帰",
    "section": "行列演算に変更",
    "text": "行列演算に変更\nfor 文で処理している箇所を行列の演算に変更する。\nテキスト 図 3.18、公式 3.8 あたりを使う。\n\nimport numpy.matlib\ndef kgauss(X,theta_1,theta_2,theta_3):\n    \"\"\"\n    図 3.18 の実装\n    引数名は上の RBF 関数に合わせた\n    \"\"\"\n    N=len(X)\n    X=X.reshape(N,-1)\n    X=X.T # D * N 次元に\n    \n    z=(X**2).sum(axis=0).reshape(1,N)\n    K = numpy.matlib.repmat(z.T,1,N)+ numpy.matlib.repmat(z,N,1) -np.dot(X.T,X)*2.\n    return theta_1*np.exp(-K/theta_2)+theta_3*np.identity(N)\n\n\ndef calc2(xtrain,ytrain,xtest):\n    \"\"\"\n    図3.15 や公式 3.8 を使って for文を使わずに計算する\n    また、 train データの平均値が 0 でない場合に対応するため、 y の平均値を 0 に正規化してから計算している\n    \"\"\"\n    \n    # 平均0に正規化する。 y_mu は最後に mu に加算する\n    ytrain=ytrain.copy()\n    y_mu=ytrain.mean()\n    ytrain-=y_mu\n  \n    # 各パラメータ値\n    theta_1 = 16.\n    theta_2 = 2.\n    theta_3 = 0.34\n\n    N = len(xtrain)\n    M = len(xtest)\n    \n    # まとめてカーネル計算\n    xtrain=xtrain.reshape(N,-1)\n    xtest=xtest.reshape(M,-1)\n\n    # 図3.15 のとおり行列を分割\n    X=np.vstack([xtrain, xtest])\n    X = kgauss(X,theta_1,theta_2,theta_3)\n    \n    u, b=np.split(X,[N],0)\n    K, k_=np.split(u,[N],1)\n    k_t, k__=np.split(b,[N],1)\n    \n    assert k__.shape==(M,M)\n\n    inv_K=np.linalg.inv(K)\n    yy = np.dot(inv_K, ytrain)\n    \n    mu=np.dot(k_t,yy)\n    var=k__-np.dot(np.dot(k_t,inv_K),k_)\n    return mu+y_mu, var\n\n\nmu,var=calc2(xtrain,ytrain,xtest)\nvar=np.diag(var) # 対角成分の取得\nprint(mu[:5])\nprint(var[:5])\n\n[-0.55131748 -0.53446571 -0.48757389 -0.40223414 -0.26996228]\n[13.46226381 12.46070803 11.28034711  9.95007999  8.51800331]\n\n\n\nvisualize(data_x,data_y,xtest,sample_index,mu,var)\n\n\n\n\n\n\n\n\n上の結果とほぼ同じ結果が得られている。\n\ny の平均値が 0 でない場合の確認\nテキストに記載あるように、 y の平均値をあらかじめ 0 になるように正規化する処理を加えたのでその確認をする。\n\n# 元データの作成\nnp.random.seed(seed=9973)\nn=100\ndata_x = np.linspace(0, 4*np.pi, n)\n# 平均 +100 あたりになるように\ndata_y = 2*np.sin(data_x) + 3*np.cos(2*data_x) + 5*np.sin(2/3*data_x) + np.random.randn(len(data_x)) + 100\n\nmissing_value_rate = 0.15\nsample_index = np.sort(np.random.choice(np.arange(n), int(n*missing_value_rate), replace=False))\n\n# データの定義\nxtrain = np.copy(data_x[sample_index])\nytrain = np.copy(data_y[sample_index])\n\nxtest = np.copy(data_x)\n\n\nmu,var=calc2(xtrain,ytrain,xtest)\nvar=np.diag(var)\n\n\nvisualize(data_x,data_y,xtest,sample_index,mu,var)"
  },
  {
    "objectID": "posts/2024-08-14_gauss_sec3.html#サンプリングを確認",
    "href": "posts/2024-08-14_gauss_sec3.html#サンプリングを確認",
    "title": "ガウス過程と機械学習 3章 ガウス過程回帰",
    "section": "サンプリングを確認",
    "text": "サンプリングを確認\nテキスト 3.2.4\nテキストの順番と前後するが、ガウス過程回帰の結果を使ってサンプリング結果も確認しておく。\n求めた平均、分散からランダムにサンプリングして滑らかな関数となる様子を確認する。\nここで生成される曲線が「事後分布の雲」からランダムに出力された「関数 f」ということになる。\n\n\ndef get_cmap(name='tab10'):\n    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n    return plt.colormaps.get_cmap(name)\n\ndef visualize2(data_x,data_y,xtest,sample_index,mu,var,ys):\n    plt.figure(figsize=(12, 5))\n    plt.title('signal prediction by Gaussian process', fontsize=20)\n        \n    # ガウス過程で求めた平均値を信号化\n    cmap = get_cmap()\n    for i,y in enumerate(ys):\n        plt.plot(xtest, y, color=cmap(i/len(ys)), label=f'mean by Gaussian process {i}')   \n        \n    # 元の信号\n    plt.plot(data_x, data_y, 'x', color='green', label='correct signal')\n    plt.plot(data_x[sample_index], data_y[sample_index], 'o', color='red', label='sample dots')\n    std = np.sqrt(var) \n    plt.fill_between(xtest, mu + 2*std, mu - 2*std, alpha=.2, color='blue', label= 'standard deviation by Gaussian process')\n    \n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0, fontsize=12)\n    plt.show()\n\n\nrng = np.random.default_rng()\n# 5回のサンプリングを行った結果を表示\nys=[[rng.normal(a,np.sqrt(b)) for a,b in zip(mu,var)] for _ in range(5)]\nvisualize2(data_x,data_y,xtest,sample_index,mu,var,ys)"
  },
  {
    "objectID": "posts/2024-05-11-seed_everything.html",
    "href": "posts/2024-05-11-seed_everything.html",
    "title": "PyTorch 学習結果の再現性確保",
    "section": "",
    "text": "Pytorch Lightning で学習したとき、seed_everything で乱数固定すれば、同じ学習結果が得られると思っていたが実際にはそうでなかった。\n調べてみると、\nの設定も必要そうだった。Reproducibility — PyTorch 2.3 documentation\nこの 2 つを設定することで、内部で使用されるアルゴリズムを固定化できるとのこと。\n上記参考に以下の関数を呼ぶことで、同じ学習結果が得られるようになった。"
  },
  {
    "objectID": "posts/2024-05-11-seed_everything.html#その他参考",
    "href": "posts/2024-05-11-seed_everything.html#その他参考",
    "title": "PyTorch 学習結果の再現性確保",
    "section": "その他参考",
    "text": "その他参考\n\nhttps://qiita.com/north_redwing/items/1e153139125d37829d2d#cuda-convolution-benchmarking"
  },
  {
    "objectID": "posts/2024-08-16_kubo_3sho.html",
    "href": "posts/2024-08-16_kubo_3sho.html",
    "title": "データ解析のための統計モデリング入門 3章のポアソン回帰",
    "section": "",
    "text": "データ解析のための統計モデリング入門 3章のポアソン回帰をざっとやってみる"
  },
  {
    "objectID": "posts/2024-08-16_kubo_3sho.html#データ確認",
    "href": "posts/2024-08-16_kubo_3sho.html#データ確認",
    "title": "データ解析のための統計モデリング入門 3章のポアソン回帰",
    "section": "データ確認",
    "text": "データ確認\n\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nDATADIR=Path(\"/workdir/input/kubobook_2012/data/\")\nDATADIR\n\nPosixPath('/workdir/input/kubobook_2012/data')\n\n\n\ndf=pd.read_csv(DATADIR/\"data3a.csv\")\ndf.head()\n\n\n\n\n\n\n\n\ny\nx\nf\n\n\n\n\n0\n6\n8.31\nC\n\n\n1\n6\n9.44\nC\n\n\n2\n6\n9.50\nC\n\n\n3\n12\n9.07\nC\n\n\n4\n10\n10.16\nC\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\ny\nx\n\n\n\n\ncount\n100.000000\n100.000000\n\n\nmean\n7.830000\n10.089100\n\n\nstd\n2.624881\n1.008049\n\n\nmin\n2.000000\n7.190000\n\n\n25%\n6.000000\n9.427500\n\n\n50%\n8.000000\n10.155000\n\n\n75%\n10.000000\n10.685000\n\n\nmax\n15.000000\n12.400000\n\n\n\n\n\n\n\n\ncolor=['blue' if x=='C' else 'red' for x in df.f.to_list()]\nplt.scatter(df.x,df.y,c=color)"
  },
  {
    "objectID": "posts/2024-08-16_kubo_3sho.html#ポアソン回帰",
    "href": "posts/2024-08-16_kubo_3sho.html#ポアソン回帰",
    "title": "データ解析のための統計モデリング入門 3章のポアソン回帰",
    "section": "ポアソン回帰",
    "text": "ポアソン回帰\nsklearn のポアソン回帰を使って回帰してみる。\nポアソン回帰は \\(log (\\lambda) = \\beta X\\) の線形回帰になるらしい。(3章の回帰と同じで対数リンク関数が使われる)\nhttps://okumuralab.org/~okumura/stat/poisson_regression.html\n\nfrom sklearn import linear_model\nclf = linear_model.PoissonRegressor()\n\n\n# NOTE: 多変量の場合は、 df[[\"x1\",\"x2\"]] のようにする\nclf.fit(df[[\"x\"]],df[\"y\"])\n\nPoissonRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  PoissonRegressor?Documentation for PoissonRegressoriFittedPoissonRegressor() \n\n\n\nprint(\"beta_1:\",clf.intercept_) # bias \nprint(\"beta_2:\",clf.coef_) # 係数\n\nbeta_1: 1.378219625347999\nbeta_2: [0.06714908]\n\n\n\npreds=clf.predict(df[[\"x\"]])\npreds[:5]\n\narray([6.93255517, 7.47905849, 7.50925198, 7.29552944, 7.84953464])\n\n\n\n# lambda を手計算した結果と確認してみる\nnp.exp(df[:5][[\"x\"]]*clf.coef_[0]+clf.intercept_)\n\n\n\n\n\n\n\n\nx\n\n\n\n\n0\n6.932555\n\n\n1\n7.479058\n\n\n2\n7.509252\n\n\n3\n7.295529\n\n\n4\n7.849535\n\n\n\n\n\n\n\n\ndf[\"pred\"]=preds\n\n対数尤度計算してみる\n\n# 尤度計算してみる\nfrom scipy.stats import poisson\n\n\nloglike=df.apply(lambda x:np.log(poisson.pmf(x[\"y\"],x[\"pred\"])) ,axis=1).sum()\nprint(\"対数尤度:\",loglike)\n\n対数尤度: -235.41483299355457\n\n\nテキストの値とほぼ同じ値となっている。\nつぎに lambda の予測をプロットする。\n\ncolor=['blue' if x=='C' else 'red' for x in df.f.to_list()]\n\n# lambda の予測値を引く\nplt.scatter(df.x,df.y,c=color)\ndata_x = np.linspace(7,13,100).reshape(100,1)\nmu_=clf.predict(data_x)\nplt.plot(data_x,mu_)\n\nplt.show()\n\n/opt/conda/envs/foo/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but PoissonRegressor was fitted with feature names\n  warnings.warn("
  },
  {
    "objectID": "posts/2024-08-16_kubo_3sho.html#施肥効果の説明変数を追加-3.5",
    "href": "posts/2024-08-16_kubo_3sho.html#施肥効果の説明変数を追加-3.5",
    "title": "データ解析のための統計モデリング入門 3章のポアソン回帰",
    "section": "施肥効果の説明変数を追加 3.5〜",
    "text": "施肥効果の説明変数を追加 3.5〜\n\n# 施肥効果\ndf['d']=df.f.apply(lambda x:1 if x=='T' else 0)\n\n\nclf = linear_model.PoissonRegressor()\n# 多変量の場合は、 df[[\"x1\",\"x2\"]] のようにするだけで良い\nclf.fit(df[[\"x\",\"d\"]],df[\"y\"])\n\nPoissonRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  PoissonRegressor?Documentation for PoissonRegressoriFittedPoissonRegressor() \n\n\n\nprint(\"bias:\",clf.intercept_) # bias \nprint(\"beta:\",clf.coef_) # 係数\n\nbias: 1.3655409998858836\nbeta: [ 0.06925659 -0.01719852]\n\n\n対数尤度計算\n\npreds=clf.predict(df[[\"x\",\"d\"]])\nloglike=np.log(poisson.pmf(df[\"y\"],preds)).sum()\nprint(\"対数尤度:\",loglike)\n\n対数尤度: -235.34390910510987\n\n\n1変数だけのときの回帰よりも、尤度が少し増加していることが確認できた。"
  },
  {
    "objectID": "posts/make_grid_example.html",
    "href": "posts/make_grid_example.html",
    "title": "画像グリッド描画サンプル",
    "section": "",
    "text": "複数枚の画像をグリッド描画するサンプルコードです。\ntorchvision の make_grid を使って描画します。 make_grid\n\nimport matplotlib.pyplot as plt\nimport random\nimport numpy as np\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torchvision.utils import make_grid\nimport torch\n\n\nclass CFG:\n    H=32\n    W=64\n    row=3\n\n\ndef get_tranform():\n    \"\"\"画像をアスペクト比を変えずに CFG.H * CFG.W サイズに変換する\"\"\"\n    t = A.Compose([\n        A.LongestMaxSize(max_size=max(CFG.H,CFG.W)),\n        A.PadIfNeeded(min_height=CFG.H, min_width=CFG.W, border_mode=0, mask_value=0),\n        ToTensorV2(),\n    ])\n    return t\n\n\n画像の用意\n数枚ランダムに単色画像を作成する\n\ndef create_img(color,size):\n    \"\"\"単色画像の作成\"\"\"\n    return np.array(color,dtype=np.uint8)*np.ones((*size,3), dtype=np.uint8)\n\n\nimages= [create_img([random.randint(0,255) for _ in range(3)],(20,50)) for _ in range(20)]\n\n\n\ngrid の作成\n\nt=get_tranform()\nimages=[t(image=img)['image'] for img in images]\n\n\ntmp=torch.stack(images)\n\n\nres=make_grid(tmp,3,padding=2)\n\n\n# channel を一番最後に\ngrid_image=res.permute(1,2,0).numpy()\n\n\nplt.imshow(grid_image)\n\n\n\n\n\n\n\n\n\n\nsave\n\nfrom PIL import Image\n\n\nimage=Image.fromarray(grid_image)\n\n\nimage.save(\"test.jpg\")\n\n\n\n\n\nCopyrightCopyright tamuraup. 2024. All Rights Reserved"
  },
  {
    "objectID": "posts/2024-05-09_bcewithlogitsloss_check.html",
    "href": "posts/2024-05-09_bcewithlogitsloss_check.html",
    "title": "BCEWithLogitsLoss の確認",
    "section": "",
    "text": "BCEWithLogitsLoss はマルチラベル分類のロスに使えることを知った\nBCEWithLogitsLoss\n\nimport numpy as np\nimport torch\n\n\ntarget = torch.ones([1, 10], dtype=torch.float32)  # 10 classes, batch size = 1\n# A prediction (logit)\noutput = torch.cat([torch.full([1, 5], 1.5),torch.full([1,5],1.0)],dim=1)\ncriterion = torch.nn.BCEWithLogitsLoss(reduction='none')\ncriterion(output, target)  # -log(sigmoid(output value))\n\ntensor([[0.2014, 0.2014, 0.2014, 0.2014, 0.2014, 0.3133, 0.3133, 0.3133, 0.3133,\n         0.3133]])\n\n\n\nsigmoid=torch.sigmoid(torch.Tensor([1.0,1.5]))\ntorch.log(sigmoid)\n\ntensor([-0.3133, -0.2014])\n\n\npos_weight は 各クラスで Positive/Negative のデータ数が不均衡な場合に設定すると良いらしい\n\n\n\nCopyrightCopyright tamuraup. 2024. All Rights Reserved"
  },
  {
    "objectID": "posts/2025-07-01_soutui.html",
    "href": "posts/2025-07-01_soutui.html",
    "title": "主問題と双対問題の構造と相補性条件の理解",
    "section": "",
    "text": "Note\n\n\n\nこのレポートは OpenAI の ChatGPT によって自動生成されました。"
  },
  {
    "objectID": "posts/2025-07-01_soutui.html#双対問題の定義と意義",
    "href": "posts/2025-07-01_soutui.html#双対問題の定義と意義",
    "title": "主問題と双対問題の構造と相補性条件の理解",
    "section": "1. 双対問題の定義と意義",
    "text": "1. 双対問題の定義と意義\n線形計画法では、任意の主問題（primal problem）に対して、対応する双対問題（dual problem）を定式化できます。双対問題を考える意義は以下の通りです：\n\n主問題の最適値に対する上界/下界を提供\n問題の構造の別の視点からの理解を可能にする\n感度分析やラグランジュ緩和との接続点になる\n双対問題のほうが解きやすい場合もある"
  },
  {
    "objectID": "posts/2025-07-01_soutui.html#製造計画の例題",
    "href": "posts/2025-07-01_soutui.html#製造計画の例題",
    "title": "主問題と双対問題の構造と相補性条件の理解",
    "section": "2. 製造計画の例題",
    "text": "2. 製造計画の例題\n\n📘 主問題（製造者の視点）\n製品 A・B を製造する工場があり、それぞれに作業時間と材料が必要。目的は利益の最大化。\n\\[\n\\begin{align*}\n\\text{maximize} \\quad & 3x_1 + 2x_2 \\\\\n\\text{subject to} \\quad\n& x_1 + x_2 \\le 4 \\quad \\text{(作業時間：最大4時間)} \\\\\n& 2x_1 + x_2 \\le 5 \\quad \\text{(材料：最大5kg)} \\\\\n& x_1 \\ge 0,\\quad x_2 \\ge 0\n\\end{align*}\n\\]\n\n\\(x_1\\)：製品 A の生産数\n\\(x_2\\)：製品 B の生産数\n利益：製品 A は 3 万円、製品 B は 2 万円\n\n\n\n📗 双対問題（資源の価値評価）\n資源 1（作業時間）、資源 2（材料）に対して、それぞれ実際の「価値」（価格）を変数 $y_1$, $y_2$ として設定します。 この価値を用いることで、製造者が製品 1 単位を生産する際にかかる資源コストが、その製品の単位利益を下回らないように制約を課します。\n双対問題の目的は、こうして設定した資源の価格総額 $4y_1 + 5y_2$ を最小化することです。\n\\[\n\\begin{aligned}\n\\text{minimize} \\quad & 4y_1 + 5y_2 \\\\\\\\\n\\text{subject to} \\quad\n& y_1 + 2y_2 \\ge 3 \\quad \\text{(製品A の単位利益 3 万円を賄うコスト)} \\\\\\\\\n& y_1 + y_2 \\ge 2 \\quad \\text{(製品B の単位利益 2 万円を賄うコスト)} \\\\\\\\\n& y_1 \\ge 0, \\quad y_2 \\ge 0\n\\end{aligned}\n\\]\n\nこの問題では、資源に価格を与えることで「製品 1 単位を生産するために必要な資源コスト」が算出されます。 各製品の利益を下回らないようにコストを設定することで、資源の価値を適切に評価することができます。\n一方で、資源の価格総額はできるだけ小さく抑えることが目的となっています。 このように、製品の価値に見合うように資源価格を保ちつつ、その合計を最小にするという要請が両立するように、資源の価格は調整されます。"
  },
  {
    "objectID": "posts/2025-07-01_soutui.html#双対問題の複数の解釈",
    "href": "posts/2025-07-01_soutui.html#双対問題の複数の解釈",
    "title": "主問題と双対問題の構造と相補性条件の理解",
    "section": "3. 双対問題の複数の解釈",
    "text": "3. 双対問題の複数の解釈\n\n\n\n\n\n\n\n\n解釈のタイプ\n内容\n適した応用分野\n\n\n\n\n資源価格\n制約ごとに単位価値（影の価格）をつける\n経済学、経営学\n\n\n感度分析\n制約の右辺が変動したときの最適値の変化率\n実務最適化\n\n\nゲーム理論\n主問題と双対問題はプレイヤー間の駆け引き\n理論解析、ナッシュ均衡\n\n\n代替コスト\n制約を緩和するための代替手段の評価\n工学的設計、物流"
  },
  {
    "objectID": "posts/2025-07-01_soutui.html#相補性条件主問題と双対問題の接続",
    "href": "posts/2025-07-01_soutui.html#相補性条件主問題と双対問題の接続",
    "title": "主問題と双対問題の構造と相補性条件の理解",
    "section": "4. 相補性条件：主問題と双対問題の接続",
    "text": "4. 相補性条件：主問題と双対問題の接続\n主問題と双対問題の最適解がともに存在するとき、両者の間には相補性条件が成立します。\n\n等式による表現（この例における相補性）\n\\[\n\\begin{cases}\n(4 - x_1 - x_2) \\cdot y_1 = 0 \\\\\n(5 - 2x_1 - x_2) \\cdot y_2 = 0 \\\\\n(y_1 + 2y_2 - 3) \\cdot x_1 = 0 \\\\\n(y_1 + y_2 - 2) \\cdot x_2 = 0\n\\end{cases}\n\\]\n\n主問題の制約が余っていれば（スラック &gt; 0） → 対応する双対変数は 0\n主問題の制約がピッタリ張っていれば → 双対変数は正\n主問題の変数が正なら → 双対制約はギリギリ満たされている（＝号）\n主問題の変数がゼロなら → 双対制約は緩んでいる（＞）"
  },
  {
    "objectID": "posts/2025-07-01_soutui.html#直感的解釈の補足",
    "href": "posts/2025-07-01_soutui.html#直感的解釈の補足",
    "title": "主問題と双対問題の構造と相補性条件の理解",
    "section": "5. 直感的解釈の補足",
    "text": "5. 直感的解釈の補足\nたとえば、双対制約 \\(y_1 + 2y_2 \\ge 3\\) が厳しくて、実際に\n\\[\ny_1 + 2y_2 &gt; 3\n\\]\nだった場合、主問題側ではこう考える：\n\n「この資源価格で製品 A を作ると損になる。ならば作らない（\\(x_1 = 0\\)）！」\n\n逆に、もし \\(x_1 &gt; 0\\) で製品 A を実際に作っていたなら、\n\\[\ny_1 + 2y_2 = 3\n\\]\nでなければ相補性条件が破れる。"
  },
  {
    "objectID": "posts/2025-07-01_soutui.html#相補性条件のまとめ表",
    "href": "posts/2025-07-01_soutui.html#相補性条件のまとめ表",
    "title": "主問題と双対問題の構造と相補性条件の理解",
    "section": "6. 相補性条件のまとめ表",
    "text": "6. 相補性条件のまとめ表\n\n\n\n\n\n\n\n\n状況\n結果\n解釈\n\n\n\n\n双対制約が 緩い（&gt;）\n\\(x_i = 0\\)\nコスト高すぎ、作らない\n\n\n双対制約が 等号（=）\n\\(x_i &gt; 0\\)\nちょうど釣り合ってるから作る\n\n\n主問題の制約が 緩い（&lt;）\n\\(y_j = 0\\)\n余ってる資源は価値がない\n\n\n主問題の制約が 等号（=）\n\\(y_j &gt; 0\\)\nギリギリの資源は価値がある"
  },
  {
    "objectID": "posts/2025-07-01_soutui.html#おわりに",
    "href": "posts/2025-07-01_soutui.html#おわりに",
    "title": "主問題と双対問題の構造と相補性条件の理解",
    "section": "おわりに",
    "text": "おわりに\n双対問題と相補性条件は、線形計画における構造的理解のカギとなる概念です。特に、主問題の制約と双対変数、目的関数と制約の緊張関係を把握することで、数式の裏にある経済的・構造的意味を読み取る力が養われます。\n実務や理論の両面で活用されるこの知識は、最適化の応用やアルゴリズム設計にもつながります。さらなる理解のためには、他の問題設定での双対問題と相補性条件の演習も有効です。"
  },
  {
    "objectID": "archives/2024-05-20_compute_delta.html",
    "href": "archives/2024-05-20_compute_delta.html",
    "title": "音声信号の “delta” という特徴量",
    "section": "",
    "text": "音声信号に delta という特徴量があるらしい。 Practical Cryptography\nあまり良くわかっていませんが、各周波数帯での軌跡を特徴量にするイメージでしょうか？ 各周波数帯のデータと [1.0] * time をたたみ込み演算をしたもの特徴量とするようです。\nkaggle でも過去の上位解法に使用されているようです。[4-th place solution] Inference and Training tips\nまた torchaudio にも実装されています。torchaudio.functional.compute_deltas — Torchaudio 2.2.0.dev20240520 documentation\n実装例"
  },
  {
    "objectID": "archives/2024-05-20_compute_delta.html#一応演算結果確認しておく",
    "href": "archives/2024-05-20_compute_delta.html#一応演算結果確認しておく",
    "title": "音声信号の “delta” という特徴量",
    "section": "一応演算結果確認しておく",
    "text": "一応演算結果確認しておく\n\nimport numpy as np\nimport torch\n\n\n# kaggle の solution notebook より\ndef compute_deltas(\n        specgram: torch.Tensor, win_length: int = 5, mode: str = \"replicate\"\n) -&gt; torch.Tensor:\n    device = specgram.device\n    dtype = specgram.dtype\n\n    # pack batch\n    shape = specgram.size()\n    specgram = specgram.reshape(1, -1, shape[-1])\n\n    assert win_length &gt;= 3\n\n    n = (win_length - 1) // 2\n\n    # twice sum of integer squared\n    denom = n * (n + 1) * (2 * n + 1) / 3\n\n    specgram = torch.nn.functional.pad(specgram, (n, n), mode=mode)\n\n    kernel = torch.arange(-n, n + 1, 1, device=device, dtype=dtype).repeat(\n        specgram.shape[1], 1, 1\n    )\n    output = (\n            torch.nn.functional.conv1d(specgram, kernel, groups=specgram.shape[1]) / denom\n    )\n\n    # unpack batch\n    output = output.reshape(shape)\n\n    return output\n\n\nx=torch.rand([1, 10]) # dim (freq, time)\nprint(x.shape)\nprint(x)\n\ntorch.Size([1, 10])\ntensor([[0.6760, 0.4193, 0.8303, 0.1316, 0.1804, 0.4828, 0.7212, 0.0631, 0.1529,\n         0.5410]])\n\n\n\ndelta=compute_deltas(x)\nprint(delta.shape)\nprint(delta)\n\ntorch.Size([1, 10])\ntensor([[ 0.0052, -0.0934, -0.1279, -0.0523,  0.0133,  0.0404, -0.0475, -0.0452,\n          0.0117,  0.1344]])\n\n\n\nn=2\ntmp=torch.nn.functional.pad(x, (2, 2), mode='replicate')\ndenom = n * (n + 1) * (2 * n + 1) / 3\n\n\nfor p in range(n,n+10):\n    sm=0.0\n    for i in range(1,n+1):\n        sm+=(-tmp[0][p-i] +tmp[0][p+i])*i\n    print(sm/denom)\n\ntensor(0.0052)\ntensor(-0.0934)\ntensor(-0.1279)\ntensor(-0.0523)\ntensor(0.0133)\ntensor(0.0404)\ntensor(-0.0475)\ntensor(-0.0452)\ntensor(0.0117)\ntensor(0.1344)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "めもちょう",
    "section": "",
    "text": "統計入門28. フィッシャーの正確検定と超幾何分布の理解\n\n\n\n統計入門\n\nフィッシャーの正確検定\n\n\n\n\n\n\n\n\n\n2025 年 07 月 03 日\n\n\n\n\n\n\n\n\n\n\n\n\n主問題と双対問題の構造と相補性条件の理解\n\n\n\n線形計画法\n\n数理最適化\n\n\n\n\n\n\n\n\n\n2025 年 07 月 01 日\n\n\n\n\n\n\n\n\n\n\n\n\n統計入門27. 時系列データ分析-2\n\n\n\n統計入門\n\n時系列分析\n\n\n\n\n\n\n\n\n\n2025 年 06 月 30 日\n\n\n\n\n\n\n\n\n\n\n\n\n統計入門27. 時系列データ分析-1\n\n\n\n統計入門\n\n時系列分析\n\n\n\n\n\n\n\n\n\n2025 年 06 月 29 日\n\n\n\n\n\n\n\n\n\n\n\n\nAUCLoss に関する解説レポート\n\n\n\nML\n\n\n\n\n\n\n\n\n\n2025 年 06 月 15 日\n\n\n\n\n\n\n\n\n\n\n\n\nアップサンプリング + パディング処理の流れ\n\n\n\nML\n\n\n\n\n\n\n\n\n\n2025 年 06 月 15 日\n\n\n\n\n\n\n\n\n\n\n\n\nPython でスキャン画像の回転補正をやってみる\n\n\n\nPython\n\nimage\n\n\n\n\n\n\n\n\n\n2025 年 03 月 25 日\n\n\n\n\n\n\n\n\n\n\n\n\n時系列データの lag, step の作成操作\n\n\n\nml\n\n\n\n\n\n\n\n\n\n2025 年 02 月 05 日\n\n\n\n\n\n\n\n\n\n\n\n\nRust WebAssembly + React (Vite) で作成したサイトを GitLab Pages へデプロイ\n\n\n\nGitLab\n\nRust\n\nwasm\n\n\n\n\n\n\n\n\n\n2024 年 09 月 17 日\n\n\n\n\n\n\n\n\n\n\n\n\nガウス過程と機械学習 3章 ガウス過程回帰のパラメータ推定\n\n\n\nml\n\n\n\n\n\n\n\n\n\n2024 年 08 月 16 日\n\n\n\n\n\n\n\n\n\n\n\n\nデータ解析のための統計モデリング入門 3章のポアソン回帰\n\n\n\nml\n\n\n\n\n\n\n\n\n\n2024 年 08 月 16 日\n\n\n\n\n\n\n\n\n\n\n\n\nガウス過程と機械学習 3章 ガウス過程回帰\n\n\n\nml\n\nnotebook\n\n\n\n\n\n\n\n\n\n2024 年 08 月 14 日\n\n\n\n\n\n\n\n\n\n\n\n\nSQLコンテストの問題 4 を解く(第 1〜10 回)\n\n\n\nSQL\n\n\n\n\n\n\n\n\n\n2024 年 06 月 17 日\n\n\n\n\n\n\n\n\n\n\n\n\nAHC028 解説放送メモ\n\n\n\nahc\n\n\n\n\n\n\n\n\n\n2024 年 06 月 07 日\n\n\n\n\n\n\n\n\n\n\n\n\nnumpy, pandas.DataFrame で多数決をとる\n\n\n\nml\n\n\n\n\n\n\n\n\n\n2024 年 05 月 12 日\n\n\n\n\n\n\n\n\n\n\n\n\nPyTorch 学習結果の再現性確保\n\n\n\nml\n\n\n\n\n\n\n\n\n\n2024 年 05 月 11 日\n\n\n\n\n\n\n\n\n\n\n\n\nBCEWithLogitsLoss の確認\n\n\n\nml\n\n\n\n\n\n\n\n\n\n2024 年 05 月 09 日\n\n\n\n\n\n\n\n\n\n\n\n\n画像グリッド描画サンプル\n\n\n\nnotebook\n\nimage\n\n\n\n\n\n\n\n\n\n2024 年 05 月 04 日\n\n\n\n\n\n\n\n\n\n\n\n\nAHC032 解説放送メモ\n\n\n\nahc\n\n\n\n\n\n\n\n\n\n2024 年 05 月 03 日\n\n\n\n\n\n\n\n\n\n\n\n\ngpu check\n\n\n\nnotebook\n\n\n\n\n\n\n\n\n\n2024 年 05 月 01 日\n\n\n\n\n\nNo matching items\nCopyrightCopyright tamuraup. 2024. All Rights Reserved"
  }
]